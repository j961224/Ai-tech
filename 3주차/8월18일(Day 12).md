# 1. 8월 18일 배운 내용!

## 4. AutoGrad & Optimizer

* layer = Block

### torch.nn.Module

- nn.Module은 Input, Output, Forward, Backward(autograd로 자동 미분 -> 해당 weight 미분) 정의해야한다.

-> parameter로 정의한다.

![forward](https://user-images.githubusercontent.com/59636424/129818778-898ef142-f143-498b-8c82-b6f2c9cdfe6b.PNG)

### nn.Parameter

: Tensor 객체의 상속 객체이다.

-> **nn.moudle 내에 attribute가 될 때!, required_grad=True(gradient 계산할 수 있는 것으로 자동 지정)로 지정되어 autogradient의 대상이 된다.**

* nn.Parameter로 설정하면 학습의 대상이 되는 파라미터가 된다.

-> layer.parameters()로 하면 학습된 parameter들의 각각 weight값을 볼 수 있다.

**Tensor로 지정하면 paraemeter가 저장되지 않는다.**

### Backward

Forward의 결과값(모델의 output인 예측치)과 실제값간의 차이(loss)에 대한 미분을 수행한다!

* optimizer.zero_grad(): optimizer를 zero_grad로 바꿔주는데 gradient 값을 업데이트하는데 이전의 gradient 값이 지금 gradient 학습에 영향을 주지 않기 위해 초기화!

* loss.backward(): loss에 대해서 어떤 구하고자하는 모든 weigth값을 구해주는 것!

* optimizer.step(): weight값이 업데이터가 된다.

### Backward from the scratch

: 실제 backward는 직접 지정하지 않고 Auto grad를 사용한다.

-> 직접 지정 시, Module에서 backward와 optimizer 오버라이딩을 해야한다.

## 5. Dataset & Dataloader

![ffgfgfgfgfg](https://user-images.githubusercontent.com/59636424/129821998-12a45082-c7ab-4272-b512-7156db36d3a9.PNG)

* **__getitem__(): map style이라고 해서 하나의 데이터를 불러올 때 어떻게 반환을 할 지 정해준다.**

* **transforms: 이미지 데이터 전처리나 데이터 증강 등 데이터 변형 시, transform data 처리 -> tensor로 바꿔준다.**

    dataset을 전처리 해주는 부분과 tensor로 바꿔주는 역할이 구분이 된다.

* DataLoader: Dataset을 하나하나 처리할지 정했으면 그것을 묶어서 Model에 적용시킨다. => Batch만들어 줄 때, shuffle해서 data를 섞어준다.

### Dataset 클래스

데이터 입력 형태를 정의하는 클래스!! -> 입력하는 방식을 표준화시켜준다!

(Datasets_Dataloaders.ipynb 파일 참조!!)


### Dataset 클래스 생성시 유의점

: 데이터 형태에 따라 각 함수를 다르게 정의해야한다!

**모든 것을 데이터 생성 시점에 처리할 필요가 없다!** -> image의 Tensor 변화는 학습에 필요한 시점에 변환

### DataLodaer 클래스

: Data의 Batch를 생성해주는 class이다.

(Datasets_Dataloaders.ipynb 파일 참조!!)

-> DataLoader는 iterable한 객체이므로 iter를 넣으면 generator형태로 변환된다. => next를 넣어주면 다음 데이터를 출력한다.

* sampler과 batch_sampler는 data를 어떻게 뽑을지에 대한 기법, collate_fn: data와 label을 하나로 정렬을 해주고 variable length의 가변자를 할 때 사용 (text 처리 시)

