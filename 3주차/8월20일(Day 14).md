# 1. 8월 20일 배운 것 정리!

## 8. Multi-GPU 학습

### Model parallel

- 다중 GPU에 학습을 분산하는 두 가지 방법 (모델을 나누기 / 데이터를 나누기)

- Alexnet에서 모델을 나누는 생각은 써왔었다.

- 모델 병렬화는 고난이도 과제이다.

![zzzzzzzzzzzzzzzzzzzzzzzzzz](https://user-images.githubusercontent.com/59636424/130162212-a13f94e0-9af6-486f-8cab-88d1d030e276.PNG)

위의 그림에서 위의 경우처럼 하게 되면 병렬화 의미가 없다.

아래와 같이 pipeline 구조로 만드는 것이 중요하다!! (F0.0 -> F1.0, F0.1 -> F1.1 이렇게 배치 처리 잘하는 것이 중요하다.)

(병목현상을 잘 생각해야 한다.)

### Data parallel

데이터를 나눠 GPU에 할당 후 결과의 평균을 취하는 방법이다.

![forward](https://user-images.githubusercontent.com/59636424/130162806-4199b275-cab1-4b1d-9431-aef64e905e8c.PNG)

* Forward

~~~
1. 데이터가 오면 여러 GPU에게 데이터를 쪼갠다.

2. 모델들을 각각의 GPU에게 복사시킨다.

3. forward pass를 시행해서 연산 처리

4. 연산 결과를 1곳에 모은다. -> 1곳에 모으면 각각의 loss값을 1번에 보여줄 수 있다.
~~~

* Backward

~~~
1. 각 4개의 loss를 받아 gradient를 구한다.

2. gradient를 각 4개에 뿌린다.

3. 각각 GPU들이 backward 과정을 거친다.

4. weight 값의 새로운 파라미터가 나오게 된다.

5. gradient를 모아서 하나의 GPU에 모아서 그들의 평균을 내서 gradient 평균을 낸다.
~~~

* DataParallel은 단순히 데이터를 분배한 후 평균을 취하므로 **GPU 사용 불균형 문제가 발생한다.** => 그래서 Batch 사이즈 감소를 시킬 필요가 있다. (한 GPU가 병목이 일어난다.)

~~~
torch.nn.DataParallel을 사용!
~~~

* DistributedDataParallel은 앞서 말한 모으는 작업이 없고 각각 한 다음(gradient를 가지고 개별젹으로)에 평균치를 구한다.

-> CPU도 할당되므로 가능하다! -> 하나 개별적으로 연산의 평균을 수행한다.

~~~
sampler를 만들어 줘야한다!!

utils.data.distributed.DestributedSampler 사용!

-> shuffle = False, pin_memory = True(핀 메모리 사용하는 이유는 DRAM에 데이터를 바로바로 올릴 수 있도록 할 수 있다. 메모리에 바로 올린 다음에 GPU로 가서 연산을 빠르게 할 수 있도록 한다.)
~~~

### 9. Hyperparameter Tuning

- 모델 스스로 학습하지 않는 값으로 사람이 지정해야 한다.

* 가장 기본적인 방법으로 Grid Search와 random(베이지안 기반 기법)이 있다.

![gruid](https://user-images.githubusercontent.com/59636424/130164862-1069a631-f29e-43fd-b9f0-88c255966dd2.PNG)

#### 베이지안 최적화

: 베이지안 최적화는 확률 모델 P(score|configuration)에 초점을 맞추고 있다. 이 모델은 구성 c가 주어진 점수의 최대화를 목표로 하는 (score, configuration) 의 기록 H를 쿼리하는 반복 프로세스이다.

-> 베이지안 최적화는 **관측 데이터 기반 F(x) 추정**과 **함수 생성**역할을 한다.

~~~
관측 데이터 기반 F(x) 추정은 베이즈 정리 확용 및 가우시안 프로세스에 적용한다.

함수 생성은 확률 추정 결과 기반 입력값 후보 추천 함수를 생성한다.
~~~

-> **학습의 규모가 커질수록 탐색 시간 기반 베이지안이 가장 뛰어나지만, 생성 모델 수준을 고려하여 상황에 맞는 튜닝 방법 선택이 필요**

#### Ray

- ML/DL의 병렬 처리를 위해 개발된 모듈로 기본적으로 분산병렬이다.

* ASHAScheduler는 알고리즘이 실행되면서 중간중간 의미없다고 생각되는 metric은 잘라내는 알고리즘이다. (학습 스케줄링 알고리즘 지정)

![vvvv](https://user-images.githubusercontent.com/59636424/130165181-a8897f3a-2700-4227-9dd2-0ab7c9833da8.PNG)

: 안 쓰는 결과에 대해서는 굳이 안 쓰기 위해 단계마다 안 좋은 것은 미리 종료한다.

-> tune.run으로 병렬 처리 양식으로 학습을 시행한다.

~~~
get_best_trial #가장 높은 성능을 자랑하는 모델을 불러올 수 있다.
~~~

* **Ray**로 Hyperparameter tunning을 하면 쉽게 하이퍼파라미터를 찾을 수 있고 그 것을 return해서 쓸 수 있다.

