# 1. 8월 20일 배운 것 정리!

## 8. Multi-GPU 학습

### Model parallel

- 다중 GPU에 학습을 분산하는 두 가지 방법 (모델을 나누기 / 데이터를 나누기)

- Alexnet에서 모델을 나누는 생각은 써왔었다.

- 모델 병렬화는 고난이도 과제이다.

![zzzzzzzzzzzzzzzzzzzzzzzzzz](https://user-images.githubusercontent.com/59636424/130162212-a13f94e0-9af6-486f-8cab-88d1d030e276.PNG)

위의 그림에서 위의 경우처럼 하게 되면 병렬화 의미가 없다.

아래와 같이 pipeline 구조로 만드는 것이 중요하다!! (F0.0 -> F1.0, F0.1 -> F1.1 이렇게 배치 처리 잘하는 것이 중요하다.)

(병목현상을 잘 생각해야 한다.)

### Data parallel

데이터를 나눠 GPU에 할당 후 결과의 평균을 취하는 방법이다.

![forward](https://user-images.githubusercontent.com/59636424/130162806-4199b275-cab1-4b1d-9431-aef64e905e8c.PNG)

* Forward

~~~
1. 데이터가 오면 여러 GPU에게 데이터를 쪼갠다.

2. 모델들을 각각의 GPU에게 복사시킨다.

3. forward pass를 시행해서 연산 처리

4. 연산 결과를 1곳에 모은다. -> 1곳에 모으면 각각의 loss값을 1번에 보여줄 수 있다.
~~~

* Backward

~~~
1. 각 4개의 loss를 받아 gradient를 구한다.

2. gradient를 각 4개에 뿌린다.

3. 각각 GPU들이 backward 과정을 거친다.

4. weight 값의 새로운 파라미터가 나오게 된다.

5. gradient를 모아서 하나의 GPU에 모아서 그들의 평균을 내서 gradient 평균을 낸다.
~~~

* DataParallel은 단순히 데이터를 분배한 후 평균을 취하므로 **GPU 사용 불균형 문제가 발생한다.** => 그래서 Batch 사이즈 감소를 시킬 필요가 있다. (한 GPU가 병목이 일어난다.)

~~~
torch.nn.DataParallel을 사용!
~~~

* DistributedDataParallel은 앞서 말한 모으는 작업이 없고 각각 한 다음(gradient를 가지고 개별젹으로)에 평균치를 구한다.

-> CPU도 할당되므로 가능하다! -> 하나 개별적으로 연산의 평균을 수행한다.

~~~
sampler를 만들어 줘야한다!!

utils.data.distributed.DestributedSampler 사용!

-> shuffle = False, pin_memory = True(핀 메모리 사용하는 이유는 DRAM에 데이터를 바로바로 올릴 수 있도록 할 수 있다. 메모리에 바로 올린 다음에 GPU로 가서 연산을 빠르게 할 수 있도록 한다.)
~~~



