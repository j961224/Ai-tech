# 1. 8월 20일 배운 것 정리!

## 8. Multi-GPU 학습

### Model parallel

- 다중 GPU에 학습을 분산하는 두 가지 방법 (모델을 나누기 / 데이터를 나누기)

- Alexnet에서 모델을 나누는 생각은 써왔었다.

- 모델 병렬화는 고난이도 과제이다.

![zzzzzzzzzzzzzzzzzzzzzzzzzz](https://user-images.githubusercontent.com/59636424/130162212-a13f94e0-9af6-486f-8cab-88d1d030e276.PNG)

위의 그림에서 위의 경우처럼 하게 되면 병렬화 의미가 없다.

아래와 같이 pipeline 구조로 만드는 것이 중요하다!! (F0.0 -> F1.0, F0.1 -> F1.1 이렇게 배치 처리 잘하는 것이 중요하다.)

(병목현상을 잘 생각해야 한다.)

### Data parallel

데이터를 나눠 GPU에 할당 후 결과의 평균을 취하는 방법이다.

![forward](https://user-images.githubusercontent.com/59636424/130162806-4199b275-cab1-4b1d-9431-aef64e905e8c.PNG)

* Forward

~~~
1. 데이터가 오면 여러 GPU에게 데이터를 쪼갠다.

2. 모델들을 각각의 GPU에게 복사시킨다.

3. forward pass를 시행해서 연산 처리

4. 연산 결과를 1곳에 모은다. -> 1곳에 모으면 각각의 loss값을 1번에 보여줄 수 있다.
~~~

* Backward

~~~
1. 각 4개의 loss를 받아 gradient를 구한다.

2. gradient를 각 4개에 뿌린다.

3. 각각 GPU들이 backward 과정을 거친다.

4. weight 값의 새로운 파라미터가 나오게 된다.

5. gradient를 모아서 하나의 GPU에 모아서 그들의 평균을 내서 gradient 평균을 낸다.
~~~

* DataParallel은 단순히 데이터를 분배한 후 평균을 취하므로 **GPU 사용 불균형 문제가 발생한다.** => 그래서 Batch 사이즈 감소를 시킬 필요가 있다. (한 GPU가 병목이 일어난다.)

~~~
torch.nn.DataParallel을 사용!
~~~

* DistributedDataParallel은 앞서 말한 모으는 작업이 없고 각각 한 다음(gradient를 가지고 개별젹으로)에 평균치를 구한다.

-> CPU도 할당되므로 가능하다! -> 하나 개별적으로 연산의 평균을 수행한다.

~~~
sampler를 만들어 줘야한다!!

utils.data.distributed.DestributedSampler 사용!

-> shuffle = False, pin_memory = True(핀 메모리 사용하는 이유는 DRAM에 데이터를 바로바로 올릴 수 있도록 할 수 있다. 메모리에 바로 올린 다음에 GPU로 가서 연산을 빠르게 할 수 있도록 한다.)
~~~

### 9. Hyperparameter Tuning

- 모델 스스로 학습하지 않는 값으로 사람이 지정해야 한다.

* 가장 기본적인 방법으로 Grid Search와 random(베이지안 기반 기법)이 있다.

![gruid](https://user-images.githubusercontent.com/59636424/130164862-1069a631-f29e-43fd-b9f0-88c255966dd2.PNG)

#### 베이지안 최적화

: 베이지안 최적화는 확률 모델 P(score|configuration)에 초점을 맞추고 있다. 이 모델은 구성 c가 주어진 점수의 최대화를 목표로 하는 (score, configuration) 의 기록 H를 쿼리하는 반복 프로세스이다.

-> 베이지안 최적화는 **관측 데이터 기반 F(x) 추정**과 **함수 생성**역할을 한다.

~~~
관측 데이터 기반 F(x) 추정은 베이즈 정리 확용 및 가우시안 프로세스에 적용한다.

함수 생성은 확률 추정 결과 기반 입력값 후보 추천 함수를 생성한다.
~~~

-> **학습의 규모가 커질수록 탐색 시간 기반 베이지안이 가장 뛰어나지만, 생성 모델 수준을 고려하여 상황에 맞는 튜닝 방법 선택이 필요**

#### Ray

- ML/DL의 병렬 처리를 위해 개발된 모듈로 기본적으로 분산병렬이다.

* ASHAScheduler는 알고리즘이 실행되면서 중간중간 의미없다고 생각되는 metric은 잘라내는 알고리즘이다. (학습 스케줄링 알고리즘 지정)

![vvvv](https://user-images.githubusercontent.com/59636424/130165181-a8897f3a-2700-4227-9dd2-0ab7c9833da8.PNG)

: 안 쓰는 결과에 대해서는 굳이 안 쓰기 위해 단계마다 안 좋은 것은 미리 종료한다.

-> tune.run으로 병렬 처리 양식으로 학습을 시행한다.

~~~
get_best_trial #가장 높은 성능을 자랑하는 모델을 불러올 수 있다.
~~~

* **Ray**로 Hyperparameter tunning을 하면 쉽게 하이퍼파라미터를 찾을 수 있고 그 것을 return해서 쓸 수 있다.

## 10. PyTorch Troubleshooting

### Out Of Memory(OOM)이 해결하기 어려운 이유는?

- 왜 발생했는지 알기 어려움

- 어디서 발생했는지 알기 어려움

- Error backtracking이 이상한데로 감

- 메모리의 이전상황의 파악이 어려움

=> 보통 BAtch size를 줄이고 GPU clean을 시켜 Run을 한다!

### GPUUtil 사용하기

: 이것으로 GPU 상태를 볼 수 있다.

-> iter마다 메모리가 늘어나는지 확인이 가능하다

~~~
import GPUtil
GPUtil.showUtilization() # GPU 형태를 볼 수 있다.
~~~

### torch.cuda.empty_cache()

- 사용되지 않은 GPU상 cache를 정리한다. (backward 시 특히, 메모리 버퍼안에 데이터가 계속 저장하니 생각보다 메모리를 차지할 경우가 있다.)

- 가용 메모리를 확보한다.

### trainning loop에 tensor로 축적되는 변수는 확인할 것

- tensor로 처리된 변수는 GPU 상에 메모리를 사용한다.

- 해당변수 loop 안에 연산에 있을 때, GPU에 연산된 graph를 생성한다.

![fbfbfbf](https://user-images.githubusercontent.com/59636424/130168199-15d808eb-ff5c-453f-b1d7-33f8ebfe7d47.PNG)

~~~
보통 backward 시, 축적된 loss값을 1번만 쓸 것인데 연산이 축적되어 쓸모 없이 메모리를 먹는 경우가 있다.

-> 이 경우에는 1-d tensor의 경우(1번만 사용하는 경우)에 python 기본 객체로 변환하여 처리한다.

(필요 없는 데이터 저장을 방지하기 위해 iter_loss에 .item(나 float(iterloss))을 붙이면 python 기본 객체로 반환된다.
~~~

### del 명령어 적절히 사용하기

- 필요가 없어진 변수는 적절히 삭제가 필요함

- Python 메모리 특성 상, loop가 끝나도 메모리를 차지한다.

### 가능한 batch 사이즈를 실험해보기

- 학습시 OOM이 발생했다면, batch 사이즈를 1로 해서 실험해보기

### torch.no_grad() 사용하기

- inference 시점에서 사용하면 메모리 버퍼 현상이 일어나지 않는다.

- no_grad를 사용 시, backward가 일어나도 메모리를 추가적으로 만들어내지 않는다.

### 예상치 못한 에러 메세지

- OOM 말고도 유사한 에러들이 발생한다.

- CUDNN_STATUS_NOT_INIT(CPU를 잘 못 설치했을 경우 대부분 발생) or device-side-assert(다양한 경우가 있다.) 등

### 그 외

- colab에서는 너무 큰 사이즈를 사용하지 말자!

- CNN의 경우는 크기가 안 맞아서 생기는 에러가 많다. (torch.summary를 사용해서 사이즈를 맞추자!)

## 3-2. 차트의 요소 - Color 사용하기

### 1-1. 색이 중요한 이유

:위치와 색은 가장 효과적인 채널 구분이 가능하다.

### 1-2. 화려함이 시각화의 전부는 아니다!!

: 인사이트가 어떤 방식으로 어떤 오용을 막는지가 중요하다!

### 2. Color paratte 종류

#### 2-1. 범주형

: 독립된 색상으로 구성되어 범주형 변수에 사용한다. (색의 차이로 구분한다.)

#### 2-2. 연속형

: 연속적인 색상을 이용하는데 어두운 배경에서는 밝은 색이, 밝은 배경에서는 어두운 색이 큰 값을 표현한다.

=> 단일 색조로 표현하는 것이 좋고 균일한 색상 변화가 중요하다.

#### 2-3. 발산형

: 연속형과 유사하지만 **중앙을 기준으로 양 끝이 발산형태이다.**

-> 서로 다른 2개(지지율 등)를 표현하는데 적합하다.

#### 3-1. 강조, 그리고 색상 대비

Highlighting 기능을 이용한다.

> 색상 대비를 사용한다
> > 명도 대비: 밝은 색과 어두운 색을 배치하면 밝은 색은 더 밝게, 더운 색은 더 어둡게 보인다.
> > 색상 대비: 가까운 색은 차이가 더 크게 보인다!
> > 채도 대비: 채도의 차이, 채도가 더 높아보인다. -> 이걸로 많이 쓰인다.
> > 보색 대비: 정반대 색상을 사용해 더 선명해 보인다.

#### 3-2. 색각 이상

색상을 감지 못하면 **색맹**, 부분적 인지 이상이 있다면 **색약**으로 색 인지가 중요한 분야에는 이에 대한 고려가 필수이다.

![tttttttttttttttttttttttttt](https://user-images.githubusercontent.com/59636424/130180217-4174894a-b3fe-41aa-a9bc-8b55f6d23f22.PNG)

~~~
위의 그래프는 수학 성적과 국어 성적 top 5를 뽑은 색으로는 'Set2'를 사용했다.

일반적으로 tab10과 Set2가 가장 많이 사용되고 더 많은 색은 위에서 언급한 R colormap을 사용하면 좋다.

color bar는 각 그래프의 오른쪽에 붙어있는 것으로 일종의 subplot이라고 할 수 있다.
~~~

## 3. 차트의 요소 - Facet

: Facet은 분할의 의미를 두고 화면 상에 View를 분할 및 추가하여 다양한 관점을 전달한다.

* 가장 쉬운 방법 3가지 방법

> plt.subplot()
> ptl.figure() + fig.add_subplot()
> plt.subplots()

* 쉽게 조정할 수 있는 요소

> figuresize
> dpi
> sharex, sharey
> squeeze
> aspect

### 2.3 Grid Spec 활용

* 서브 플롯을 표현하기 위해서 2가지 방법

    1. Slicing 사용

    2. x, y, dx, dy를 사용
 
![gggggggggggggggg](https://user-images.githubusercontent.com/59636424/130182961-5caf5678-568b-41f9-9a57-df144063d00b.PNG)

위의 그림을 Slicing을 생각하면, 파란색은 axes[0,:3], 노란색은 axes[1:,:3], 보라색은 axes[3,:]으로 slicing을 진행할 수 있다.

* 내부에 그리기

![qqqqqqqqqqqqqqqqqqqqqqq](https://userimages.githubusercontent.com/59636424/130183174-f602362c-f35a-44ef-a648-66ce4eeb0057.PNG)

    Ax 내부에 서브플롯을 추가하는 방법
    
    ax.inset_axes()

![ttttttttttttttttttt](https://user-images.githubusercontent.com/59636424/130183296-c2bc2b63-ff41-49d8-b43a-3c4ce12ef732.PNG)

    그리드를 사용하지 않고 사이드에 추가
    
    make_axes_locatable(ax)

~~~
# 개별 ax에 대해서나 `subplots` 함수를 사용할 때는 `sharex`, `sharey`를 사용하여 축을 공유
ax2 = fig.add_subplot(122, sharey=ax1)
~~~

## 4. More Tips

### 1-1. Default Grid

기본적인 Grid는 축과 평행한 선을 사용하여 거리 및 값 정보를 보조적으로 제공한다.

