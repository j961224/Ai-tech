# 1. 8월 20일 배운 것 정리!

## 8. Multi-GPU 학습

### Model parallel

- 다중 GPU에 학습을 분산하는 두 가지 방법 (모델을 나누기 / 데이터를 나누기)

- Alexnet에서 모델을 나누는 생각은 써왔었다.

- 모델 병렬화는 고난이도 과제이다.

![zzzzzzzzzzzzzzzzzzzzzzzzzz](https://user-images.githubusercontent.com/59636424/130162212-a13f94e0-9af6-486f-8cab-88d1d030e276.PNG)

위의 그림에서 위의 경우처럼 하게 되면 병렬화 의미가 없다.

아래와 같이 pipeline 구조로 만드는 것이 중요하다!! (F0.0 -> F1.0, F0.1 -> F1.1 이렇게 배치 처리 잘하는 것이 중요하다.)

(병목현상을 잘 생각해야 한다.)

### Data parallel

데이터를 나눠 GPU에 할당 후 결과의 평균을 취하는 방법이다.

![forward](https://user-images.githubusercontent.com/59636424/130162806-4199b275-cab1-4b1d-9431-aef64e905e8c.PNG)

* Forward

~~~
1. 데이터가 오면 여러 GPU에게 데이터를 쪼갠다.

2. 모델들을 각각의 GPU에게 복사시킨다.

3. forward pass를 시행해서 연산 처리

4. 연산 결과를 1곳에 모은다. -> 1곳에 모으면 각각의 loss값을 1번에 보여줄 수 있다.
~~~

* Backward

~~~
1. 각 4개의 loss를 받아 gradient를 구한다.

2. gradient를 각 4개에 뿌린다.

3. 각각 GPU들이 backward 과정을 거친다.

4. weight 값의 새로운 파라미터가 나오게 된다.

5. gradient를 모아서 하나의 GPU에 모아서 그들의 평균을 내서 gradient 평균을 낸다.
~~~

* DataParallel은 단순히 데이터를 분배한 후 평균을 취하므로 **GPU 사용 불균형 문제가 발생한다.** => 그래서 Batch 사이즈 감소를 시킬 필요가 있다. (한 GPU가 병목이 일어난다.)

~~~
torch.nn.DataParallel을 사용!
~~~

* DistributedDataParallel은 앞서 말한 모으는 작업이 없고 각각 한 다음(gradient를 가지고 개별젹으로)에 평균치를 구한다.

-> CPU도 할당되므로 가능하다! -> 하나 개별적으로 연산의 평균을 수행한다.

~~~
sampler를 만들어 줘야한다!!

utils.data.distributed.DestributedSampler 사용!

-> shuffle = False, pin_memory = True(핀 메모리 사용하는 이유는 DRAM에 데이터를 바로바로 올릴 수 있도록 할 수 있다. 메모리에 바로 올린 다음에 GPU로 가서 연산을 빠르게 할 수 있도록 한다.)
~~~

### 9. Hyperparameter Tuning

- 모델 스스로 학습하지 않는 값으로 사람이 지정해야 한다.

* 가장 기본적인 방법으로 Grid Search와 random(베이지안 기반 기법)이 있다.

![gruid](https://user-images.githubusercontent.com/59636424/130164862-1069a631-f29e-43fd-b9f0-88c255966dd2.PNG)

#### 베이지안 최적화

: 베이지안 최적화는 확률 모델 P(score|configuration)에 초점을 맞추고 있다. 이 모델은 구성 c가 주어진 점수의 최대화를 목표로 하는 (score, configuration) 의 기록 H를 쿼리하는 반복 프로세스이다.

-> 베이지안 최적화는 **관측 데이터 기반 F(x) 추정**과 **함수 생성**역할을 한다.

~~~
관측 데이터 기반 F(x) 추정은 베이즈 정리 확용 및 가우시안 프로세스에 적용한다.

함수 생성은 확률 추정 결과 기반 입력값 후보 추천 함수를 생성한다.
~~~

-> **학습의 규모가 커질수록 탐색 시간 기반 베이지안이 가장 뛰어나지만, 생성 모델 수준을 고려하여 상황에 맞는 튜닝 방법 선택이 필요**

#### Ray

- ML/DL의 병렬 처리를 위해 개발된 모듈로 기본적으로 분산병렬이다.

* ASHAScheduler는 알고리즘이 실행되면서 중간중간 의미없다고 생각되는 metric은 잘라내는 알고리즘이다. (학습 스케줄링 알고리즘 지정)

![vvvv](https://user-images.githubusercontent.com/59636424/130165181-a8897f3a-2700-4227-9dd2-0ab7c9833da8.PNG)

: 안 쓰는 결과에 대해서는 굳이 안 쓰기 위해 단계마다 안 좋은 것은 미리 종료한다.

-> tune.run으로 병렬 처리 양식으로 학습을 시행한다.

~~~
get_best_trial #가장 높은 성능을 자랑하는 모델을 불러올 수 있다.
~~~

* **Ray**로 Hyperparameter tunning을 하면 쉽게 하이퍼파라미터를 찾을 수 있고 그 것을 return해서 쓸 수 있다.

## 10. PyTorch Troubleshooting

### Out Of Memory(OOM)이 해결하기 어려운 이유는?

- 왜 발생했는지 알기 어려움

- 어디서 발생했는지 알기 어려움

- Error backtracking이 이상한데로 감

- 메모리의 이전상황의 파악이 어려움

=> 보통 BAtch size를 줄이고 GPU clean을 시켜 Run을 한다!

### GPUUtil 사용하기

: 이것으로 GPU 상태를 볼 수 있다.

-> iter마다 메모리가 늘어나는지 확인이 가능하다

~~~
import GPUtil
GPUtil.showUtilization() # GPU 형태를 볼 수 있다.
~~~

### torch.cuda.empty_cache()

- 사용되지 않은 GPU상 cache를 정리한다. (backward 시 특히, 메모리 버퍼안에 데이터가 계속 저장하니 생각보다 메모리를 차지할 경우가 있다.)

- 가용 메모리를 확보한다.

### trainning loop에 tensor로 축적되는 변수는 확인할 것

- tensor로 처리된 변수는 GPU 상에 메모리를 사용한다.

- 해당변수 loop 안에 연산에 있을 때, GPU에 연산된 graph를 생성한다.

![fbfbfbf](https://user-images.githubusercontent.com/59636424/130168199-15d808eb-ff5c-453f-b1d7-33f8ebfe7d47.PNG)

~~~
보통 backward 시, 축적된 loss값을 1번만 쓸 것인데 연산이 축적되어 쓸모 없이 메모리를 먹는 경우가 있다.

-> 이 경우에는 1-d tensor의 경우(1번만 사용하는 경우)에 python 기본 객체로 변환하여 처리한다.

(필요 없는 데이터 저장을 방지하기 위해 iter_loss에 .item(나 float(iterloss))을 붙이면 python 기본 객체로 반환된다.
~~~

### del 명령어 적절히 사용하기

- 필요가 없어진 변수는 적절히 삭제가 필요함

- Python 메모리 특성 상, loop가 끝나도 메모리를 차지한다.

### 가능한 batch 사이즈를 실험해보기

- 학습시 OOM이 발생했다면, batch 사이즈를 1로 해서 실험해보기

### torch.no_grad() 사용하기

- inference 시점에서 사용하면 메모리 버퍼 현상이 일어나지 않는다.

- no_grad를 사용 시, backward가 일어나도 메모리를 추가적으로 만들어내지 않는다.

### 예상치 못한 에러 메세지

- OOM 말고도 유사한 에러들이 발생한다.

- CUDNN_STATUS_NOT_INIT(CPU를 잘 못 설치했을 경우 대부분 발생) or device-side-assert(다양한 경우가 있다.) 등

### 그 외

- colab에서는 너무 큰 사이즈를 사용하지 말자!

- CNN의 경우는 크기가 안 맞아서 생기는 에러가 많다. (torch.summary를 사용해서 사이즈를 맞추자!)

