# 1. 8월 19일 배운 것!

## 6. 모델 불러오기

### model.save()

-> 학습의 결과를 저장하기 위한 함수

-> 모델 형태와 파라미터를 저장

~~~
for param_tensor in model.state_dict(): #모델의 파라미터를 표시
~~~

~~~
torch.save(model.state_dict(), 
           os.path.join(MODEL_PATH, "model.pt")) #모델의 파라미터를 저장
~~~

~~~
new_model = TheModelClass() #같은 모델의 형태에서 파라미터만 load (동일한 모델이어야 한다)
~~~

~~~
from torchsummary import summary
summary(vgg, (3, 224, 224)) # 이 명령으로 keras 형태로 layer가 어떤 것이 있는지 볼 수 있다.
~~~

### checkpoints

: **학습의 중간결과를 저장하여 최선의 결과를 선택**

-> earlystopping 기법 사용 시 이전 학습의 결과물을 저장한다.

-> 일반적으로 epoch, loss, metric을 함께 저장하여 확인한다!

~~~
criterion = nn.BCEWithLogitsLoss() #binary classification cross entropy with loss -> sigmoid 함수를 달아준다.
~~~

### pretrained model transfer learning

**남이 만들 모델을 쓰고 싶다.**

-> 다른 데이터셋으로 만든 모델을 현재 데이터에 적용한다.

-> 적은 데이터를 가진 모델에 적용시키고 싶다면 사용한다!

-> backbone architecture가 잘 학습된 모델에서 일부부만 변경하여 학습을 수행한다.

**보통 TorchVision으로 다양한 기본 모델을 사용한다**

**자연어 처리는 HuggingFace가 사실상 표준이다!**

### Freezing

: 사전 학습 모델 활용 시 모델의 일부분을 frozen 시킨다. => 그대로 가져와서 쓰되, 일부분만 파라미터 값을 쓰고 나머지는 새로 학습을 시킨다.

![vfvfvfvfvfvf](https://user-images.githubusercontent.com/59636424/130022290-29b4d609-80a7-4910-b56d-ad435303b207.PNG)

-> 특정 위치까지 멈춘 다음에 일정 부분만 파라미터 값을 안 바뀌게 하는 것을 Frozen이라고 한다!

~~~
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
vgg = models.vgg16(pretrained=True).to(device) # 사전학습 모델인 vgg모델을 불러와서 쓰는 경우이다.
#pretrained는 True를 사용해야 한다!
~~~

-> 여기서 보통 모델 마지막에 Linear Layer를 추가하는 등의 경우도 있다. (그렇게 되면 마지막 레이어를 제외하고 frozen을 시켜야 한다.)

## 7. Monitoring tools for PyTorch

* Tensorboard와 weight&biases를 사용한다.

### Tensorboard

: TensorFlow의 프로젝트로 만들어진 시각화 도구이다.

> scalar: metric(accuracy, loss 등) 등 상수 값의 연속(epoch)을 표시
> graph: 모델의 computational graph 표시
> histogram: weight 등 값의 분포를 표현(weight 값이 정규 분포의 형태를 띄면 좋다!)
> image: 예측 값과 실제 값을 비교 표시
> mesh: 3d 형태의 데이터를 표현하는 도구

~~~
logs_base_dir = "logs" #log 데이터 저장
os.makedirs(logs_base_dir, exist_ok=True)
~~~

~~~
writer = SummaryWriter(exp) #이걸로 기록할 위치만 정해주고 writer.type(아래 add_scalar같은 것)와 flush()로 값을 기록한다.
~~~

~~~
#add_scalar 함수: scalar값을 기록
# Loss/train: loss category에 train 값
# n_iter: x 축의 값(하나씩 증가한다.)
writer.add_scalar('Loss/train', np.random.random(), n_iter)
writer.add_scalar('Loss/test', np.random.random(), n_iter)

writer.flush() # 값 기록(disk에 쓰기)
~~~

~~~
%tensorboard --logdir "logs" #파일 위치 지정(logs_base_dir)같은 명령으로 콘솔에서도 사용 가능 -> 자동으로 port에 생성
~~~
