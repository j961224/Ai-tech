# 1. 8월 19일 배운 것!

## 6. 모델 불러오기

### model.save()

-> 학습의 결과를 저장하기 위한 함수

-> 모델 형태와 파라미터를 저장

~~~
for param_tensor in model.state_dict(): #모델의 파라미터를 표시
~~~

~~~
torch.save(model.state_dict(), 
           os.path.join(MODEL_PATH, "model.pt")) #모델의 파라미터를 저장
~~~

~~~
new_model = TheModelClass() #같은 모델의 형태에서 파라미터만 load (동일한 모델이어야 한다)
~~~

~~~
from torchsummary import summary
summary(vgg, (3, 224, 224)) # 이 명령으로 keras 형태로 layer가 어떤 것이 있는지 볼 수 있다.
~~~

### checkpoints

: **학습의 중간결과를 저장하여 최선의 결과를 선택**

-> earlystopping 기법 사용 시 이전 학습의 결과물을 저장한다.

-> 일반적으로 epoch, loss, metric을 함께 저장하여 확인한다!

~~~
criterion = nn.BCEWithLogitsLoss() #binary classification cross entropy with loss -> sigmoid 함수를 달아준다.
~~~

### pretrained model transfer learning

**남이 만들 모델을 쓰고 싶다.**

-> 다른 데이터셋으로 만든 모델을 현재 데이터에 적용한다.

-> 적은 데이터를 가진 모델에 적용시키고 싶다면 사용한다!

-> backbone architecture가 잘 학습된 모델에서 일부부만 변경하여 학습을 수행한다.

**보통 TorchVision으로 다양한 기본 모델을 사용한다**

**자연어 처리는 HuggingFace가 사실상 표준이다!**

### Freezing

: 사전 학습 모델 활용 시 모델의 일부분을 frozen 시킨다. => 그대로 가져와서 쓰되, 일부분만 파라미터 값을 쓰고 나머지는 새로 학습을 시킨다.

![vfvfvfvfvfvf](https://user-images.githubusercontent.com/59636424/130022290-29b4d609-80a7-4910-b56d-ad435303b207.PNG)

-> 특정 위치까지 멈춘 다음에 일정 부분만 파라미터 값을 안 바뀌게 하는 것을 Frozen이라고 한다!

~~~
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
vgg = models.vgg16(pretrained=True).to(device) # 사전학습 모델인 vgg모델을 불러와서 쓰는 경우이다.
#pretrained는 True를 사용해야 한다!
~~~

-> 여기서 보통 모델 마지막에 Linear Layer를 추가하는 등의 경우도 있다. (그렇게 되면 마지막 레이어를 제외하고 frozen을 시켜야 한다.)

## 7. Monitoring tools for PyTorch

* Tensorboard와 weight&biases를 사용한다.

### Tensorboard

: TensorFlow의 프로젝트로 만들어진 시각화 도구이다.

> scalar: metric(accuracy, loss 등) 등 상수 값의 연속(epoch)을 표시
> graph: 모델의 computational graph 표시
> histogram: weight 등 값의 분포를 표현(weight 값이 정규 분포의 형태를 띄면 좋다!)
> image: 예측 값과 실제 값을 비교 표시
> mesh: 3d 형태의 데이터를 표현하는 도구

~~~
logs_base_dir = "logs" #log 데이터 저장
os.makedirs(logs_base_dir, exist_ok=True)
~~~

~~~
writer = SummaryWriter(exp) #이걸로 기록할 위치만 정해주고 writer.type(아래 add_scalar같은 것)와 flush()로 값을 기록한다.
~~~

~~~
#add_scalar 함수: scalar값을 기록
# Loss/train: loss category에 train 값
# n_iter: x 축의 값(하나씩 증가한다.)
writer.add_scalar('Loss/train', np.random.random(), n_iter)
writer.add_scalar('Loss/test', np.random.random(), n_iter)

writer.flush() # 값 기록(disk에 쓰기)
~~~

~~~
%tensorboard --logdir "logs" #파일 위치 지정(logs_base_dir)같은 명령으로 콘솔에서도 사용 가능 -> 자동으로 port에 생성
~~~

* 위의 코드 실행 결과

![fbfbfbfbfbf](https://user-images.githubusercontent.com/59636424/130042588-6f1354fc-be4f-4a05-8248-5e656689a1c9.PNG)

~~~
writer.add_histogram('distribution centers', x + i, i) #분포를 보여준다.
~~~

* 위의 코드 실행 결과

![histo](https://user-images.githubusercontent.com/59636424/130045289-32e19c68-4927-43ff-bd46-22e4dd7e7236.PNG)

~~~
writer = SummaryWriter(logs_base_dir)
writer.add_images('my_image_batch', img_batch, 0)
~~~

* 위의 코드 실행 결과

![bfbfbfbfbfbfbf](https://user-images.githubusercontent.com/59636424/130045646-a984f909-cf80-4e04-b30d-1fe7a54cda74.PNG)

~~~
 w.add_hparams({'lr': 0.1*i, 'bsize': i}, #hyperparameter를 동시에 기록할 수 있다.
                      {'hparam/accuracy': 10*i, 'hparam/loss': 10*i})
~~~

-> 이 코드로 tensorboard를 확인하면, h param이 생겨 학습될 때마다 어떻게 학습되었는지 나온다!


~~~
img_grid = torchvision.utils.make_grid(images) #알아서 이미지 grid를 만든다!
~~~

~~~
# write to tensorboard
writer.add_image('four_fashion_mnist_images', img_grid) #데이터를 tensorboard에 기록!
~~~

* 위의 결과

![dlalwlwll](https://user-images.githubusercontent.com/59636424/130046922-39a58101-f0ca-4172-b2bb-955efaf5a023.PNG)

~~~
writer.add_embedding(features, #2차원, 3차원공간에 표현하기 위해 데이터 압축해주는 임베딩
                    metadata=class_labels,
                    label_img=images.unsqueeze(1))
~~~

~~~
writer.add_figure('predictions vs. actuals', #예측한 것 이미지를 텐서보드에 넣기
                            plot_classes_preds(net, inputs, labels),
                            global_step=epoch * len(trainloader) + i)
~~~

* 위의 결과

![bbbbbbbbbbbbbbbbb](https://user-images.githubusercontent.com/59636424/130048072-41621ceb-7e1f-41fa-ba85-53c05152fb7a.PNG)

### Weight & biases

- 머신러닝 실험을 원활히 지원하기 위한 상용도구!

- MLOps의 대표적인 툴로 저변 확대 중이다

~~~
!pip install wandb -p # 이 코드로 설치!

config = {"epochs": ~ }
wandb.init(project="wandb에 만든 project명",config=config)
#config 설정
#wandb.config.batch_size=
#wandb.config.learning_rate=

for e in range(1,EPOCHS+1):
   --
   wandb.log({'accuracy':~,'loss':~}) # 기록 add_~~~ 함수와 동일한 역할을 한다.
~~~


~~~
wandb.init(project="my-test-proejct", entity='j961224') #wandb의 project 불러오기
~~~

-> 실험 시, 중간에 멈추고 바꿔서 다시 실행하면 run history를 보여준다!

![zzzzzzzzzzzzzzzzzzz](https://user-images.githubusercontent.com/59636424/130052974-1eb679ef-c743-49ef-8fb4-a28700e1e747.PNG)

-> 실험들을 한 번에 관리하고 보여줄 때 weight & biases가 좋다!! (코드와 결과 공유 할 시 좋다!)

# 2. 피어세션 정리

- 코드 리뷰

  - 커스텀 모델, 커스텀 데이터셋 관련
  - GAN 모델 구현
  - ViT모델 구현

- 논문, 서적 및 세미나 정보 공유
