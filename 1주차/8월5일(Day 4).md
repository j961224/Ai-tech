# 1. 8월 5일 배운 내용

## 9. CNN

![CNN-1](https://user-images.githubusercontent.com/59636424/128276311-b07e2cb1-716b-4718-98a8-4fe084a88349.PNG)

-> kernel이라는 고정된 가중치 사용!(K는 커널 사이즈)

=> kernel size만큼 입력벡터 x를 사용!

=> **kernel size만큼 움직이면서 계산하는 것이 CNN 특징!** -> parameter size를 줄일 수 있다!


![cnn-2](https://user-images.githubusercontent.com/59636424/128276636-7eaad300-55a7-4d19-ba92-00cc4028da78.PNG)

-> z를 움직이면서 연산을 수행한다!(첫번째 수식!)(수식에 그냥 z를 이용한 것이 커널!)

-> 신호를 커널을 이용해 국소적으로 증폭 또는 감소시켜 정보를 추출한다!

-> +가 cross-correlation이라고 부른다!

-> **kernel이 위치에 따라 바뀌지 않는다!**

### 2차원 Convolution 연산

![cnn-3](https://user-images.githubusercontent.com/59636424/128277188-d83d7b4c-886f-49f2-8516-ecad24783043.PNG)

-> 2차원 Convolution 연산 이미지이다!

-> 입력행렬에 해당하는 데이터에서 커널을 x방향, y방향으로 1칸씩 움직이며 적용한다!

-> **채널이 여러개라면 채널 개수만큼 적용한다!** -> 2차원 입력에서 채널의 개수만큼 커널의 개수도 존재해야한다!!


### Convolution 연산의 역전파

: 커널이 모든 입력데이터에 공통으로 적용!  -> 역전파 계산 시에도 convolution 연산이 적용된다!

![cccc](https://user-images.githubusercontent.com/59636424/128278947-fa13c78d-2c86-442b-8e13-6dd0d301cc0c.PNG)

: 각각의 gradient를 보여준다.(커널에는 델타에 입력값 x3에 대해 곱해서 전달하게 된다.)

![c1c1](https://user-images.githubusercontent.com/59636424/128279188-79761ba7-90a7-4597-9cba-16b6bd55a00f.PNG)

: 각각의 gradient를 보여준 사진에서와 같은 방식으로 커널에 해당하는 연산이 위의 사진과 같다!!


## 10. RNN

### 시퀀스 데이터

: 소리, 문자열 등의 순차적으로 들어오는 시퀀스 데이터

![rnn-1](https://user-images.githubusercontent.com/59636424/128280281-b519547f-49a7-4466-ab2e-fa66c0bb4d62.PNG)

: 시퀀스 정보는 앞으로 발생할 데이터의 확률분포를 다루기 위해 조건부확률을 사용한다!!

-> 베이즈 정리를 사용한다.

=> **하지만 과거의 모든 정보를 사용하지만 모든 과거 정보들이 필요한 것이 아니다!**


![zzzz](https://user-images.githubusercontent.com/59636424/128281525-d9b4486f-87b5-46c4-b67b-f8ae6518a85d.PNG)

: 고정된 길이만큼 사용!(고정된 길이 타우를 어떻게 결정할지가 문제!)


![dlwjs](https://user-images.githubusercontent.com/59636424/128281713-893dcbc7-2e7b-4746-9f8e-79332d8ccbf7.PNG)

: 바로 직전 정보랑 훨씬 이전 정보를 모아서 H_t라고 해서 잠재변수를 인코딩한다.

=> 하지만 어떻게 인코딩할지가 문제?!


![zzzzzzzzzzzzzzzzzzzzzzzz](https://user-images.githubusercontent.com/59636424/128282032-906b374d-ae83-4c04-a3a1-ca6fb5fc45d1.PNG)

: 앞에서의 인코딩문제를 해결할 바로 이전 정보와 이전 잠재변수로 예측하는 RNN이 등장!

### RNN 이해하기!

: RNN모형은 MLP와 유사하지만 과거 정보를 다루기 힘들다!

![rnnn](https://user-images.githubusercontent.com/59636424/128282420-023ce8c3-aca1-4f77-af5e-725e96b75766.PNG)

: 과거정보를 다루기 위해서 입력으로부터 전달하게되는 W_X와 이전 잠재변수로부터 정보를 전달받는 W_H를 사용한다.

-> t번째 잠재변수인 H_t는 현재 들어온 입력벡터 X_t와 이전 시점에 잠재변수인 H_(t-1)을 받아서 만든다!

-> 이러한 H_t를 이용해서 현재 시점의 출력은 O_t를 만든다.

-> **3개의 W는 t에 따라 변하지 않는 가중치임을 알아야한다!!** -> t에 따라 변하는 것은 잠재변수와 입력변수이다.

### BPTT 알아보자!

![역전파](https://user-images.githubusercontent.com/59636424/128283686-aa042d56-df92-40de-926f-b779d1b19ec1.PNG)

: 잠재변수에 따라 순차적으로 계산된다. => 맨 마지막 시점의 gradient가 과거까지 gradient까지 흐르는 것이 BPTT(빨간색 선)

![크](https://user-images.githubusercontent.com/59636424/128284317-b9546952-c248-413d-875b-44d3d1d6b7eb.PNG)

: BPTT를 통해 각 가중치행렬 미분을 계산했을 때, 마지막 최종 product는 i+1부터 t시점까지 모든 히든 변수에 대한 미분 term이 곱해진 것이다.

==> **시퀀스 길이가 길어지면 계산이 불안정해진다.**

### Truncated BPTT

![truncked](https://user-images.githubusercontent.com/59636424/128284897-33fae8cd-f07f-47c5-abca-a7b257f3e52c.PNG)

: gradient를 전달할 때, 모든 t시점에서 전달하지 않고 특정 block에서 끊고 gradient를 나눠서 전달하는 방식

=> 다른 해결책으로 GRU와 LSTM을 사용한다!





