# 1. 8월 5일 배운 내용

## 9. CNN

![CNN-1](https://user-images.githubusercontent.com/59636424/128276311-b07e2cb1-716b-4718-98a8-4fe084a88349.PNG)

-> kernel이라는 고정된 가중치 사용!(K는 커널 사이즈)

=> kernel size만큼 입력벡터 x를 사용!

=> **kernel size만큼 움직이면서 계산하는 것이 CNN 특징!** -> parameter size를 줄일 수 있다!


![cnn-2](https://user-images.githubusercontent.com/59636424/128276636-7eaad300-55a7-4d19-ba92-00cc4028da78.PNG)

-> z를 움직이면서 연산을 수행한다!(첫번째 수식!)(수식에 그냥 z를 이용한 것이 커널!)

-> 신호를 커널을 이용해 국소적으로 증폭 또는 감소시켜 정보를 추출한다!

-> +가 cross-correlation이라고 부른다!

-> **kernel이 위치에 따라 바뀌지 않는다!**

### 2차원 Convolution 연산

![cnn-3](https://user-images.githubusercontent.com/59636424/128277188-d83d7b4c-886f-49f2-8516-ecad24783043.PNG)

-> 2차원 Convolution 연산 이미지이다!

-> 입력행렬에 해당하는 데이터에서 커널을 x방향, y방향으로 1칸씩 움직이며 적용한다!

-> **채널이 여러개라면 채널 개수만큼 적용한다!** -> 2차원 입력에서 채널의 개수만큼 커널의 개수도 존재해야한다!!


### Convolution 연산의 역전파

: 커널이 모든 입력데이터에 공통으로 적용!  -> 역전파 계산 시에도 convolution 연산이 적용된다!

![cccc](https://user-images.githubusercontent.com/59636424/128278947-fa13c78d-2c86-442b-8e13-6dd0d301cc0c.PNG)

: 각각의 gradient를 보여준다.(커널에는 델타에 입력값 x3에 대해 곱해서 전달하게 된다.)

![c1c1](https://user-images.githubusercontent.com/59636424/128279188-79761ba7-90a7-4597-9cba-16b6bd55a00f.PNG)

: 각각의 gradient를 보여준 사진에서와 같은 방식으로 커널에 해당하는 연산이 위의 사진과 같다!!


## 10. RNN

### 시퀀스 데이터

: 소리, 문자열 등의 순차적으로 들어오는 시퀀스 데이터

![rnn-1](https://user-images.githubusercontent.com/59636424/128280281-b519547f-49a7-4466-ab2e-fa66c0bb4d62.PNG)

: 시퀀스 정보는 앞으로 발생할 데이터의 확률분포를 다루기 위해 조건부확률을 사용한다!!

-> 베이즈 정리를 사용한다.

=> **하지만 과거의 모든 정보를 사용하지만 모든 과거 정보들이 필요한 것이 아니다!**


![zzzz](https://user-images.githubusercontent.com/59636424/128281525-d9b4486f-87b5-46c4-b67b-f8ae6518a85d.PNG)

: 고정된 길이만큼 사용!(고정된 길이 타우를 어떻게 결정할지가 문제!)


![dlwjs](https://user-images.githubusercontent.com/59636424/128281713-893dcbc7-2e7b-4746-9f8e-79332d8ccbf7.PNG)

: 바로 직전 정보랑 훨씬 이전 정보를 모아서 H_t라고 해서 잠재변수를 인코딩한다.

=> 하지만 어떻게 인코딩할지가 문제?!


![zzzzzzzzzzzzzzzzzzzzzzzz](https://user-images.githubusercontent.com/59636424/128282032-906b374d-ae83-4c04-a3a1-ca6fb5fc45d1.PNG)

: 앞에서의 인코딩문제를 해결할 바로 이전 정보와 이전 잠재변수로 예측하는 RNN이 등장!

### RNN 이해하기!

: RNN모형은 MLP와 유사하지만 과거 정보를 다루기 힘들다!

![rnnn](https://user-images.githubusercontent.com/59636424/128282420-023ce8c3-aca1-4f77-af5e-725e96b75766.PNG)

: 과거정보를 다루기 위해서 입력으로부터 전달하게되는 W_X와 이전 잠재변수로부터 정보를 전달받는 W_H를 사용한다.

-> t번째 잠재변수인 H_t는 현재 들어온 입력벡터 X_t와 이전 시점에 잠재변수인 H_(t-1)을 받아서 만든다!

-> 이러한 H_t를 이용해서 현재 시점의 출력은 O_t를 만든다.

-> **3개의 W는 t에 따라 변하지 않는 가중치임을 알아야한다!!** -> t에 따라 변하는 것은 잠재변수와 입력변수이다.

### BPTT 알아보자!

![역전파](https://user-images.githubusercontent.com/59636424/128283686-aa042d56-df92-40de-926f-b779d1b19ec1.PNG)

: 잠재변수에 따라 순차적으로 계산된다. => 맨 마지막 시점의 gradient가 과거까지 gradient까지 흐르는 것이 BPTT(빨간색 선)

![크](https://user-images.githubusercontent.com/59636424/128284317-b9546952-c248-413d-875b-44d3d1d6b7eb.PNG)

: BPTT를 통해 각 가중치행렬 미분을 계산했을 때, 마지막 최종 product는 i+1부터 t시점까지 모든 히든 변수에 대한 미분 term이 곱해진 것이다.

==> **시퀀스 길이가 길어지면 계산이 불안정해진다.**

### Truncated BPTT

![truncked](https://user-images.githubusercontent.com/59636424/128284897-33fae8cd-f07f-47c5-abca-a7b257f3e52c.PNG)

: gradient를 전달할 때, 모든 t시점에서 전달하지 않고 특정 block에서 끊고 gradient를 나눠서 전달하는 방식

=> 다른 해결책으로 GRU와 LSTM을 사용한다!


## 5-1. File/Exception/Log Handling

### Exception

#### 1) 예상 가능한 예외

: 사전에 인지할 수 있는 예외로 개발자가 반드시 명시적으로 정의

#### 2) 예상 불가능한 예외

: 인터프리터 과정에서 발생하는 예외

#### 예외처리

- 예상이 불가능한 예외는 Exception Handling이 필요하다!

#### Exception Handling

~~~
# try ~ except 문법

try:
  예외 발생 가능 코드
except <Exception type>:
  예외 발생 시 대응하는 코드
~~~

~~~
for i in range(10):
  try:
    print(10/i)
  except ZeroDivisionError: #Exception에 맞는 type을 넣어야한다!
    print("Not divided by 0")

Not divided by 0
10.0
5.0
3.3333333333333335
2.5
2.0
1.6666666666666667
1.4285714285714286
1.25
1.1111111111111112
~~~

|Exception 이름|내용|
|--------|--------|
|IndexError|List의 Index 범위를 넘어갈 때 사용|
|ValueError|변환할 수 없는 문자와 숫자를 변환할 때 사용|
|FileNotFoundError|존재하지 않는 파일을 호출할 때 사용|
|ZeroDivisionError|0으로 숫자를 나눌 때 사용|
|NameError|존재하지 않은 변수를 호출할 때 사용|

* assert 구문: 특정 조건에 만족하지 않을 경우 예외 발생!

~~~
def binary_number(n:int):
  assert isinstance(n,int) # True나 False가 발생하는데 False면 코드 멈춤!
  return bin(n)
print(binary_number(10.0))

#AssertionError
~~~

### File Handling

#### 파일의 종류

|Text 파일|Binary 파일|
|-------|--------|
|문자열 형식으로 저장된 파일|이진형식으로 저장된 파일|
|메모장 파일, HTML 파일|엑셀파일. 위드 파일 등|

- f = open("<파일이름>", "접근 모드") 형식

~~~
f = open('dream.txt","r") # a를 쓰면 데이터 추가!
content = f.read()
f.close()


with open("dream.txt","r",encoding="utf8") as my_file: #한글같은 것은 인코딩으로 utf8, utf16,cp949 사용!(utf8 권장)
   contents = my_file.read() #별도의 close없어도 잘 닫힌다.
   #my_file.readline() # 1줄씩 읽어오기!
~~~

~~~
import shutil

source = "dream.txt"
dest = os.path.join("abc","yuseok.txt")
shutil.copy(source,dest) # shutil.copy: 파일 복사 함수

#abc폴더에 파일 옮길 수 있다.
~~~

#### pickle

: 데이터 object 등을 실행 중에 정보를 저장하고 불러와서 쓴다!

~~~
import pickle #python에 특화된 binary 파일

f = open("list.pickle","wb")
test = [1,2,3,4,5]
pickle.dump(test,f)
f.close()

f = open("list.pickle","rb")
test_pickle = pickle.load(f) #하나의 객체를 파일로 저장해서 쓰는 것을 영속화라고 한다.
test_pickle
f.close()
~~~

### logging

: 프로그램이 실행되는 동안 일어나는 정보를 기록을 남기기

|Log관리 모듈|설명|
|----|-----|
|debug|디버깅|
|info|정보 주는 것|
|warning|잘못 된 것이 있으니 주의|
|error|잘못 입력한 것 error|
|critical|프로그램 완전히 종료되었을 때 남기는 것|

* logging level

: 프로그램 진행 상황에 따라 다른 level의 log를 출력함

### configparser

: 프로그램의 실행 설정을 파일에 저장! => dict type

### argparser

: 콘솔창에서 프로그램 실행 시 setting 정보 저장

### logging formmater

: 결과값을 출력할 때, format을 지정해줄 수 있다. (log 결과값 출력)

