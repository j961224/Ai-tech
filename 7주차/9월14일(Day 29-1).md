# 1. 9월 14일 배운 것2!

## 2. Advanced Self-supervised Pre-training Models

## GPT-2

GPT-1과 비교했을 때, 모델 구조는 다를 것이 없다.

* transformer모델에 layer를 더 쌓은 구조! (Language Model task인 다음 단어 예측하는 task로 학습을 진행)

* Training data는 훨씬 더 증가된 data 사용!

    dataset을 대규모로 쓰이는 과정에서 되도록 데이터 질이 높다.
    
    -> 잘 쓰여진 글로부터 효과적으로 다양한 지식을 배울 수 있도록 한다.
    
* down-stream tasks가 언어 생성 모델에서 zero-shot setting으로 다룰 수 있다.

    구체적으로 풀고 싶은 문제를 downstream task라고 한다.
    
    -> pre-train 방식을 이용해 학습을 진행하고, 그 후에 원하고자 하는 태스크를 fine-tuning 방식을 통해 모델을 업데이트 하는 방식을 사용한다.
    
    -> 이 때, task를 down-stream task라고 한다.

![ttttt](https://user-images.githubusercontent.com/59636424/133253144-61a189ef-109c-4fb9-872c-3c697ed106f3.PNG)

**주어진 첫 문단을 가지고 이것을 이어 받아서 단어를 순차적으로 예측!**


### GPT-2: Motivation (decaNLP)

QA로서 Multitask Learning에서 착안!

-> **모든 종류 자연어 처리 task가 질의응답으로 다 바뀔 수 있다. (자연어 생성 형태)**

-> 감정 분류 시, 긍정이니 부정이니로 물어볼 수 있다.

### GPT-2: Datasets

* Reddit 글 중에, 질의응답에서 외부 링크를 포함하는 글이 있을 수 있다. => 이게 많은 사람에게 추천받을 수 있다.

=> **3개 이상의 좋아요를 받은 글 중에, 외부링크를 포함하는 경우, 그 문서에 가서 문서들을 수집해서 학습한 것!!**

---

* **Preprocess**

    subword의 word embedding 사용! (Byte pair encoding)

---

* Modification

    layer normalization은 layer가 특정 위치에서 하나 더 위로 가거나 아래로 가거나 세부적인 차이점이 있다.
    
    각 layer들을 random initialization할 때, layer가 위로 가면 갈수록, layer index에 비례해서 initalization 값을 더 작은 값으로 만듦!(1/루트 n)
    
    -> 따라서 layer가 위로 가면 갈수록, 여러 선형 변환해당하는 값들이 0에 가까워지도록 한다!
    
    -> 위쪽의 layer의 역할이 점점 더 줄어들 수 있다.
    
### GPT-2: Question Answering

zero-shot setting에서 task에 대한 예측을 맞추는데 있어서 질문글을 주고 다음에 나올 답을 예측을 하는 방법
    
-> **학습데이터를 쓰지 않고 task에 맞바로 예측 수행 시, 성능 test**

    * 55 F1-score 도출! -> Fine-tuned BERT는 89 F1-score다.
    
    하지만! fine-tuning을 하지 않고 성능을 55를 달성했으므로 가능성 보임!

### GPT-2: Summarization

fine-tuning 과정 없이, 바로 zero-shot setting으로 inference 수행 가능!!

-> **article의 마지막에 "TL;DR"이라는 단어를 준다면, 그러면 앞쪽의 글을 1줄 요약한다!**

### GPT-2: Translation

![ttttt](https://user-images.githubusercontent.com/59636424/133263955-90509c21-0755-465b-b9cf-137d682aae2e.PNG)

**주어진 문장에, 문장 다음에는 번역을 하고픈 언어를 문장 마지막에 "in French"와 같은 단어를 붙여주면 앞서 나온 문장을 불어로 번역**

## GPT-3
    
GPT-2에서 개선했지만 모델 구조는 유사하고 **GPT-2 모델 size(파라미터 숫자)보다 훨씬 더 많이 늘렸다.**

-> 큰 Batch size를 통해 학습했더니 더 좋은 결과가 나왔다.(3.2M)

* Zero-shot

![tttttttttt](https://user-images.githubusercontent.com/59636424/133265760-e9564975-bb87-4b64-81ca-535c6d83ee38.PNG)

text를 주고 바로 cheese를 번역하라고 한다.

=> **학습 데이터를 전혀 사용 X**

* One-shot

![rrr](https://user-images.githubusercontent.com/59636424/133266423-ef41e0b4-c591-4dab-8940-c9eab984ee73.PNG)

하고자 하는 task 먼저 주기 -> 불어로 번역된 예시를 준다. -> cheese를 주고 번역하라고 한다.(자연어 생성 task)

=> **결국 학습 데이터로서 한 쌍의 데이터를 줬다.** -> zero-shot보다는 더 성능이 좋아진다!

* Few-shot

![ww](https://user-images.githubusercontent.com/59636424/133266714-4b942991-f0aa-48be-adc3-149415ac7fb3.PNG)

One-shot에서 학습 데이터를 여러 개 사용한다.

=> **GPT-3 모델에서 별도 fine-tuning 없이, 예시를 보여주고 그걸로 학습해서 task 수행**

## ALBERT: A Lite BERT(경량화된 BERT)

* 더 큰 모델이 될수록 단점

    더 많은 GPU 메모리 필요!
    
    더 많은 학습 시간이 필요!
    
 => **BERT모델이 비대함을 ALBERT가 성능이 좋아지면서 모델 사이즈를 줄이고 학습 시간도 빠르게 만든다!**
 
    
* 해결책
    
    Factorized Embedding Parameterization
    
    Cross-layer Parameter Sharing
    
    Sentence Order Prediction -> 변형된 형태의 문장 level의 self supervised pretrain task를 제안!
    
### ALBERT - Factorized Embedding Parameterization

* BERT, GPT 모델을 보면 self-attention block을 계속 쌓아나가면서 전체적인 모델은 **residual connection때문에 입출력 차원이 동일하게 진행**한다!

=> **차원이 작으면 정보를 담을 공간이 부족하지만, 크면 모델 사이즈와 연산량이 증가한다!**

각 embedding layer의 word가 가지는 정보는 전체 sequence를 고려해서 단어 encoding해서 정보를 저장하는 **hidden state vector에 비해 상대적으로 훨씬 적은 정보를 사용!**

---

![ttttt](https://user-images.githubusercontent.com/59636424/133279119-ba0228f9-d83d-4a85-9542-9e9b4c4b98fc.PNG)

* 그냥 BERT에서는!

어쩔 수 없이 고정해서 써야하는 것이 4차원이라면? -> embedding layer에서는 각 word들이 4 x 4를 사용

**word embedding 입력을 주기 전에 -> 차원을 줄여서 필요로 하는 어떤 파라미터 계산량을 줄이기 위해서 제시**

* 그래서 ALBERT에서는!

![xxxx](https://user-images.githubusercontent.com/59636424/133281188-8b398420-4f20-4cdd-8236-db20d3e50205.PNG)

self-attention의 hidden state dimension이 4여야한다. -> 첫번째 self-attention block에서도 4차원 입력을 받아야한다.

-> **각 word별로 2차원 벡터만을 가진 word embedding을 구성한다면?**

-> 입력이 2차원 벡터로 주어지므로 4차원을 입력으로 받는 **residual connection이 성립하는 첫번째 self-attention block의 입력벡터를 만들어줘야한다!**

-> **추가적으로 layer 하나를 두고 2차원 embedding vector를 4차원으로 차원을 늘려주는 하나의 layer 추가!**

**원래 4차원의 word embedding vector를 2차원의 벡터 / 2차원의 word embedding에 적용하는 선형 변환 matrix 1개로 전체 파라미터 수 줄이기!**

    이것이 low rank matrix factorization!!

### ALBERT - Cross-layer Paramter Sharing

* 원래는!

head 수가 8개면 선형 행렬 변환 8 set가 있다!

=> 각 Query, Key, Value, Output linear transformation matrix가 학습에 필요로 한다!

=> self-attention block이 계속 쌓아지면 block별로 행렬들이 존재한다! => 별개의 parameter set을 가짐!

* ALBERT에서는!

**self-attention에 존재하는 linear transformation matrix들을 공통적인 shared된 parameter로 구성**한다.

실험에서는, Shared-FFN(feed-forward network parameter만 공유), Shared-attention(attention의 linear transformation matrix parameter만 공유), 둘 다 공유

결론은, original보다는 낮지만 크게 성능이 차이 나지 않는다!

### ALBERT - Sentence Order Prediction(SOP)

기존 BERT에서 NSP(Next Sentence Prediction)로 학습했는데 실효성이 없어 ALBERT는 **2개의 문장을 원래 순서대로 concat을 해서 연속성 유무를 확인했는데 만약 원래 순서가 정순서라면, 여기서 순서를 바꿔서 concat해서 역순서라고 판단하도록 추가했다.**

기존에는 Negative samples(next sentence에 해당하지 않는 부분)를 서로 다른 문서에서 문장을 추출해 concat을 해서 next sentence가 아니라고 학습! 

-> 이러한 방법은 내용이 아예 겹치지 않을 경우가 많다! (그래서 겹치는 단어로 simple하게 판단할 가능성이 높다!)

### ALBERT: GLUE(BenchMark dataset으로 포함하는) Results

![zzzzzz](https://user-images.githubusercontent.com/59636424/133287876-f71b9dc6-61c0-40bd-bf10-fb2716f4435d.PNG)

ALBERT가 전체적으로 좋은 성능을 냄을 알 수 있다.

## ELECTRA

![rtrtrtr](https://user-images.githubusercontent.com/59636424/133289496-7a8a3b91-e198-4ae7-862c-fd07f80ee8d7.PNG)

기존 BERT, GPT-2와는 다른 형태의 pretrained model이다!

**단어 복원 모델(Generator - 전형적인 BERT모델의 MLM), 예측된 단어인지나 원래 있던 단어인지 판별하는 Discriminator(구분자)를 둔 것이 큰 특징이다!**

* Discriminator는 original, replace 이렇게 이진분류하는 모델이다!

![eee](https://user-images.githubusercontent.com/59636424/133292898-ca36fb24-ccad-420c-ae4c-9c28dc3a768d.PNG)

* 학습 계산량이 많이하게 되면 GLUE benchmark에서 보면, electra의 경우 bert보다 더 좋은 성능을 보여준다.

## 모델 경량화

### DistillBERT

teacher model이 내는 output과 pattern을 student model이 잘 학습할 수 있도록 한다.

-> teacher model에 비해 parameter 수가 작고 경량화된 모델인 student model!

### TinyBERT

teacher model이 있지만 DistillBERT와 달리, embedding layer와 각 linear transformation matrix와 hidden state vector도 student model이 유사해지도록 한다!

-> MSE Loss를 통해 동시 학습 진행(teacher와 student) => teacher hidden state 차원보다 더 작을 수는 있다!!

**예측값과 중간 결과물까지도 닮도록 학습했다.**

## Fusing Knowledge Graph into Language Model

최신 논문 경향으로 기존 pretrained 모델과 지식 그래프인 외부적인 정보를 잘 결합하는 형태의 논문 방향으로 흘러가고 있다.

BERT는 없는 추가적인 정보를 효율적으로 쓰는 모습을 보이지 못 했다. -> 연결해주는 역할이 필요하다!(지식 그래프가 그 역할!)

### ERNIE

### KagNET

