# 9월 16일 한 것 정리!

## 1. 선택 과제 정리!

### 1. huggingFace Finetuning 선택 과제

#### Dataset EDA

우선, 이번 과제에서는 **imdb 영화 리뷰 데이터**를 사용할 예정이다!

* 데이터 불러오기

~~~
train_texts, train_labels = read_imdb_split('aclImdb/train')
test_texts, test_labels = read_imdb_split('aclImdb/test')
~~~

* 데이터 label 수와 분포 확인

~~~
#긍부정이 거의 갯수가 일치
print(train_labels.count(0))
print(train_labels.count(1))
print(test_labels.count(0))
print(test_labels.count(1))

12500
12500
12500
12500
~~~

-> 데이터 수는 정확하게 같음을 알 수 있다.

* train & test text word 갯수

~~~
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
tt = pd.DataFrame(columns=['texts_len'])
count_words_f = lambda x: len(str(x).split())

tt['texts_len'] = list(map(count_words_f, train_texts))

def plot_doc_lengths(dataframe):
    max_seq_len = np.round(dataframe.texts_len.mean() + dataframe.texts_len.std()).astype(int)
    sns.distplot(tuple(dataframe.texts_len), hist=True, kde=True, label='Document lengths')
    plt.axvline(x=max_seq_len, color='k', linestyle='--', label=f'Sequence length mean:{max_seq_len}')
    plt.title('Train Document lengths')
    plt.legend()
    plt.show()
    print(f" 가장 긴 문장은 {tt['texts_len'].max()} 개의 단어를, 평균 문장은 {tt['texts_len'].mean()} 개의 단어를, 가장 짧은 문장은 {tt['texts_len'].min()} 개의 단어를 가지고 있습니다.")
plot_doc_lengths(tt)
~~~

![ㄿㄿㄿㄿ](https://user-images.githubusercontent.com/59636424/133604093-9599a4eb-a6c7-41ac-a383-c2ffe4225d3d.PNG)

~~~
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
tt = pd.DataFrame(columns=['texts_len'])
count_words_f = lambda x: len(str(x).split())

tt['texts_len'] = list(map(count_words_f, test_texts))

def plot_doc_lengths(dataframe):
    max_seq_len = np.round(dataframe.texts_len.mean() + dataframe.texts_len.std()).astype(int)
    sns.distplot(tuple(dataframe.texts_len), hist=True, kde=True, label='Document lengths')
    plt.axvline(x=max_seq_len, color='k', linestyle='--', label=f'Sequence length mean:{max_seq_len}')
    plt.title('Test Document lengths')
    plt.legend()
    plt.show()
    print(f" 가장 긴 문장은 {tt['texts_len'].max()} 개의 단어를, 평균 문장은 {tt['texts_len'].mean()} 개의 단어를, 가장 짧은 문장은 {tt['texts_len'].min()} 개의 단어를 가지고 있습니다.")
plot_doc_lengths(tt)
~~~

![ㅎㅎㅎㅎㅎ](https://user-images.githubusercontent.com/59636424/133604213-a3062c54-c643-4300-87c2-b94cc9b192a9.PNG)

### Tokenizer & Train

~~~
train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

config = DistilBertConfig.from_pretrained('distilbert-base-uncased',vocab_size=30522, \
                                          max_position_embeddings=512, sinusoidal_pos_embds=False, \
                                          n_layers=6, n_heads=12, dim=768, hidden_dim=3072, \
                                          dropout=0.1, attention_dropout=0.1, activation='gelu')
training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=5,              # total number of training epochs
    per_device_train_batch_size=32,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=100,
    learning_rate=5e-5,
    adam_epsilon=1e-08 # 0으로 나누는 것을 방지하기 위한 epsilon 값
)

model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased",config=config)
#과적 합을 방지하기위한 효율적인 정규화 도구
# multi-sample-dropout이 더 낮은 오류율과 손실을 달성하는데 도움을 준다. -> loss가 1번씩 뛰는 것을 보고 사용!
#model.classifier = MyModel()


trainer = Trainer(
    model=model,                         # the instantiated 🤗 Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=val_dataset             # evaluation dataset
)

trainer.train()
~~~

-> **실험 중, learning rate 5e-5 & epoch 5 & adam_epsilon=1e-08 & batch_Size=32 일 때,  0.9318로 성능이 제일 좋았다!**

* Multi sample dropout

~~~
import torch.nn as nn
def multi_sample_dropout(in_feature, out_feature, p=0.1, bias=True):
    return nn.Sequential(
        nn.Dropout(p),
        nn.Linear(in_feature, out_feature, bias)
    )

def multi_sample_dropout_forward(x, dropout_layer, hidden_size=2):
    return torch.mean(torch.stack([
        dropout_layer(x) for _ in range(hidden_size)], dim=0), dim=0)

class MyModel(nn.Module):
    def __init__(self, num_classes=2):
        super().__init__()
        self.your_layer = nn.Linear(768, 100)
        self.multilabel_dropout_layers = multi_sample_dropout(100, num_classes, 0.25)
    def forward(self, x):
        x = self.your_layer(x)
        # 각 드롭 아웃 샘플에 대해 계산되며 최종 손실 값은 모든 드롭 아웃 샘플에 대한 손실 값을 평균화
        
        # 우선 4개의 dropout을 사용했는데 이게 Self-Ensemble 효과가 있다고 한다.
        return multi_sample_dropout_forward(x, self.multilabel_dropout_layers, 4)
~~~

-> Multi sample dropout을 사용했지만, 데이터 label이 균일해서 그런지 사용했을 때, 효과는 거의 없었다.


### 3. Byte Pair Encoding 선택 과제

**짧게 얘기하면 out of vocabulary문제를 해결한다!**

* 학습데이터에서 등장하지 않는 단어가 나오는 경우, Unknown token으로 처리해주어 모델의 입력으로 넣게 되면서 전체적으로 모델의 성능이 저하

* 그렇다고, 모든 단어의 embedding을 만들기에는 필요한 embedding parameter의 수가 지나치게 많다!

**그래서 데이터 압축 알고리즘 중 하나인 byte pair encoding 기법을 적용한 sub-word tokenizaiton이라는 개념 등장!**

---

* Byte Pair Encoding이란??

**하나의 단어를 여러 서브워드로 분리해서 단어를 인코딩 및 임베딩하는 방법이다!**

-> 좀 더 자세히는 우선 훈련 데이터에 있는 단어들을 모든 글자(chracters) 또는 유니코드(unicode) 단위로 단어 집합(vocabulary)를 만들고, 가장 많이 등장하는 유니그램을 하나의 유니그램으로 통합한다!

* Byte Pair Encoding 구현 코드

~~~
from typing import List, Dict, Set
from itertools import chain
import re
from collections import defaultdict, Counter

# https://wikidocs.net/22592 -> wikidocs 참고!

def build_bpe(
        corpus: List[str],
        max_vocab_size: int
) -> List[int]:

    # Special tokens
    PAD = BytePairEncoding.PAD_token  # Index of <PAD> must be 0
    UNK = BytePairEncoding.UNK_token  # Index of <UNK> must be 1
    CLS = BytePairEncoding.CLS_token  # Index of <CLS> must be 2
    SEP = BytePairEncoding.SEP_token  # Index of <SEP> must be 3
    MSK = BytePairEncoding.MSK_token  # Index of <MSK> must be 4
    SPECIAL = [PAD, UNK, CLS, SEP, MSK]

    WORD_END = BytePairEncoding.WORD_END  # Use this token as the end of a word
    # YOUR CODE HERE
    
    # special token 추가
    idx2word = SPECIAL
    
    # corpus 단어마다 특수기호 '_'를 끝에 넣은 뒤, 한글자 단위로 초기화하여 dictionary에 저장
    """ 
    1. 처음에 단어를 모두 character로 잘라 idx2word(vocablary)에 넣기
    
    1-1. dict_vocab(dictionary)에는 단어마다 character 단위로 분리해서 마지막에 _ 붙이기
    
    if corpus가 ["lower", "newest"]라면,
    dict_vocab = {"l o w e r _":1, "n e w e s t _":1}로 만들어줌!
    
    idx2word+=["l"]+["o"]+["w"]+["e"]+["r"]+["n"]+["s"]+["t"] 
    
    (단, 단어 1개만 들어오면 실행 X)
    """
    dict_vocab=dict()
    count_of_character=0
    for s in corpus:
        tmp = list(s)
        tmpstring = ""
        for i in range(len(tmp)):
            # if 부분은 초기 character 단위로 자른 것을 list에 추가
            if tmp[i] not in idx2word:
                idx2word+=[tmp[i]]
            tmpstring+=tmp[i]+" "
        tmpstring+="_"
        if tmpstring not in dict_vocab:
            dict_vocab[tmpstring]=1
        else:
            dict_vocab[tmpstring]+=1
    
    
    # 두 개의 character를 쌍으로 묶어 하나의 unit으로 만들어 빈도수 체크!
    """
    2. dict_vocab(dictionary)의 단어마다 유니그램의 쌍을 하나의 유니그램으로 만들고 빈도수 체크!
    
    if dict_vocab = {"l o w e r _":1, "n e w e s t _":1, 'w i d e s t _':1} 라면, 
    pairs = { (e,s):2, (o,w):1 .....} 라는 charcter 2개 쌍의 묶음과 빈도수 체크!
    """ 
    def get_stats(dict_vocab):
        pairs = defaultdict(int)
        for word, freq in dict_vocab.items():
            symbols = word.split()
            for i in range(len(symbols)-1):
                pairs[symbols[i],symbols[i+1]] += freq
        return pairs
    
    
    # 가장 빈도수 높은 pair로 포함하는 단어를 체크하여 같은 부분은 붙여준다!
    """ 
    3. 가장 빈도수가 높은 유니그램의 쌍을 하나의 유니그램으로 통합
    
    3-1. 통합한 유니그램으로 dict_vocab(dictionary) 업데이트!
    
    if v_in = {"l o w e r _":1, "n e w e s t _":1, 'w i d e s t _':1} 이고 pair = (e,s)라면,
    v_out = {"l o w e r _":1, "n e w es t _":1, 'w i d es t _':1}로 만들어줌!
    """ 
    def merge_vocab(pair, v_in):
        v_out = {}
        #escape로 띄어쓰기 기준으로 '\' 적용(한 글자 사이사이에 '\' 넣기!)
        #이 코드가 ne -> n\ e로 만들어 'n e'로 들어가는 부분이 있는 부분 확인
        bigram = re.escape(' '.join(pair))
        
        # (n,e) 쌍을 ne로 변경
        x = ''.join(pair)
        
        for word in v_in:
            # word에서 n e(bigram 'n\ e' pattern)가 들어가는 부분을 ne(변수 x)로 붙여줌!
            w_out = re.sub(bigram, x, word)
            
            # dictionary의 변경된 단어에 빈도수 저장!
            v_out[w_out] = v_in[word]
        return v_out
    
    
    # byte pair encoding 최대 vocabulary크기-1만큼 돌리기!
    for i in range(max_vocab_size-1):
        pairs = get_stats(dict_vocab)
        
        #이제 만들 짝이 없다면 중지!
        if not pairs:
            break
        
        #character 쌍 중에, 가장 빈도수 높은 쌍 추출
        best = max(pairs, key=pairs.get)
        dict_vocab = merge_vocab(best, dict_vocab)
        
        # 빈도수 가장 높은 pair를 idx2word에 넣어주기!
        """ 
        4. 가장 빈도수가 높은 유니그램의 쌍을 하나의 유니그램으로 통합
        
        4-1. idx2word(vocabulary)에 통합된 유니그램을 추가!
        
        if best가 (e,s) 라면, 
        idx2word에 "es"를 추가해준다! 
        
        제약사항1. 최대 vocab size를 넘지 않도록!
        제약사항2. corpus로 받은 단어가 2개 이상인 경우 적용! 
        
        (제약사항 2번의 경우, Test case 2번을 통과하기 위해서 시행함)
   
        """ 
        if ''.join(best) not in idx2word and len(idx2word)<max_vocab_size-1:
            idx2word+=[''.join(best)]
        
        """ 
        최대 vocabulary 사이즈 넘으면 멈춤!(max_vocab_size)
   
        """ 
        if len(idx2word) == max_vocab_size-1:
            break
            
    
    idx2word+=[WORD_END]
    idx2word = sorted(idx2word, key=len, reverse=True)
    
    return idx2word
~~~

-> 특이하게 [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf) 논문에 코드가 잘 설명되어있다!!

과정은 아래와 같다!

        1. 처음에 단어를 모두 character로 잘라 idx2word(vocablary)에 넣기
        
        1-1. dict_vocab(dictionary)에는 단어마다 character 단위로 분리해서 마지막에 _ 붙이기
        
        2. dict_vocab(dictionary)의 단어마다 유니그램의 쌍을 하나의 유니그램으로 만들고 빈도수 체크!
        
        3. 가장 빈도수가 높은 유니그램의 쌍을 하나의 유니그램으로 통합
        
        3-1. 통합한 유니그램으로 dict_vocab(dictionary) 업데이트!
        
        4. 가장 빈도수가 높은 유니그램의 쌍을 하나의 유니그램으로 통합
        
        4-1. idx2word(vocabulary)에 통합된 유니그램을 추가!
        
        5. 2 ~ 4-1까지의 과정을 반복한다!

---

* Wordpiece Model이란??

WordPiece Model은 BPE의 변형 알고리즘이다!

**BPE와 달리, 병합되었을 때 코퍼스의 우도(Likelihood)를 가장 높이는 쌍을 병합한다!!**
