# 9월 16일 한 것 정리!

## 1. 선택 과제 정리!

### 1. huggingFace Finetuning

#### Dataset EDA

우선, 이번 과제에서는 **imdb 영화 리뷰 데이터**를 사용할 예정이다!

* 데이터 불러오기

~~~
train_texts, train_labels = read_imdb_split('aclImdb/train')
test_texts, test_labels = read_imdb_split('aclImdb/test')
~~~

* 데이터 label 수와 분포 확인

~~~
#긍부정이 거의 갯수가 일치
print(train_labels.count(0))
print(train_labels.count(1))
print(test_labels.count(0))
print(test_labels.count(1))

12500
12500
12500
12500
~~~

-> 데이터 수는 정확하게 같음을 알 수 있다.

* train & test text word 갯수

~~~
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
tt = pd.DataFrame(columns=['texts_len'])
count_words_f = lambda x: len(str(x).split())

tt['texts_len'] = list(map(count_words_f, train_texts))

def plot_doc_lengths(dataframe):
    max_seq_len = np.round(dataframe.texts_len.mean() + dataframe.texts_len.std()).astype(int)
    sns.distplot(tuple(dataframe.texts_len), hist=True, kde=True, label='Document lengths')
    plt.axvline(x=max_seq_len, color='k', linestyle='--', label=f'Sequence length mean:{max_seq_len}')
    plt.title('Train Document lengths')
    plt.legend()
    plt.show()
    print(f" 가장 긴 문장은 {tt['texts_len'].max()} 개의 단어를, 평균 문장은 {tt['texts_len'].mean()} 개의 단어를, 가장 짧은 문장은 {tt['texts_len'].min()} 개의 단어를 가지고 있습니다.")
plot_doc_lengths(tt)
~~~

![ㄿㄿㄿㄿ](https://user-images.githubusercontent.com/59636424/133604093-9599a4eb-a6c7-41ac-a383-c2ffe4225d3d.PNG)

~~~
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
tt = pd.DataFrame(columns=['texts_len'])
count_words_f = lambda x: len(str(x).split())

tt['texts_len'] = list(map(count_words_f, test_texts))

def plot_doc_lengths(dataframe):
    max_seq_len = np.round(dataframe.texts_len.mean() + dataframe.texts_len.std()).astype(int)
    sns.distplot(tuple(dataframe.texts_len), hist=True, kde=True, label='Document lengths')
    plt.axvline(x=max_seq_len, color='k', linestyle='--', label=f'Sequence length mean:{max_seq_len}')
    plt.title('Test Document lengths')
    plt.legend()
    plt.show()
    print(f" 가장 긴 문장은 {tt['texts_len'].max()} 개의 단어를, 평균 문장은 {tt['texts_len'].mean()} 개의 단어를, 가장 짧은 문장은 {tt['texts_len'].min()} 개의 단어를 가지고 있습니다.")
plot_doc_lengths(tt)
~~~

![ㅎㅎㅎㅎㅎ](https://user-images.githubusercontent.com/59636424/133604213-a3062c54-c643-4300-87c2-b94cc9b192a9.PNG)

### Tokenizer & Train

~~~
train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

config = DistilBertConfig.from_pretrained('distilbert-base-uncased',vocab_size=30522, \
                                          max_position_embeddings=512, sinusoidal_pos_embds=False, \
                                          n_layers=6, n_heads=12, dim=768, hidden_dim=3072, \
                                          dropout=0.1, attention_dropout=0.1, activation='gelu')
training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=5,              # total number of training epochs
    per_device_train_batch_size=32,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=100,
    learning_rate=5e-5,
    adam_epsilon=1e-08 # 0으로 나누는 것을 방지하기 위한 epsilon 값
)

model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased",config=config)
#과적 합을 방지하기위한 효율적인 정규화 도구
# multi-sample-dropout이 더 낮은 오류율과 손실을 달성하는데 도움을 준다. -> loss가 1번씩 뛰는 것을 보고 사용!
#model.classifier = MyModel()


trainer = Trainer(
    model=model,                         # the instantiated 🤗 Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=val_dataset             # evaluation dataset
)

trainer.train()
~~~

-> **실험 중, learning rate 5e-5 & epoch 5 & adam_epsilon=1e-08 & batch_Size=32 일 때,  0.9318로 성능이 제일 좋았다!**
