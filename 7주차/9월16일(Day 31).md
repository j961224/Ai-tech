# 9ì›” 16ì¼ í•œ ê²ƒ ì •ë¦¬!

## 1. ì„ íƒ ê³¼ì œ ì •ë¦¬!

### 1. huggingFace Finetuning ì„ íƒ ê³¼ì œ

#### Dataset EDA

ìš°ì„ , ì´ë²ˆ ê³¼ì œì—ì„œëŠ” **imdb ì˜í™” ë¦¬ë·° ë°ì´í„°**ë¥¼ ì‚¬ìš©í•  ì˜ˆì •ì´ë‹¤!

* ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°

~~~
train_texts, train_labels = read_imdb_split('aclImdb/train')
test_texts, test_labels = read_imdb_split('aclImdb/test')
~~~

* ë°ì´í„° label ìˆ˜ì™€ ë¶„í¬ í™•ì¸

~~~
#ê¸ë¶€ì •ì´ ê±°ì˜ ê°¯ìˆ˜ê°€ ì¼ì¹˜
print(train_labels.count(0))
print(train_labels.count(1))
print(test_labels.count(0))
print(test_labels.count(1))

12500
12500
12500
12500
~~~

-> ë°ì´í„° ìˆ˜ëŠ” ì •í™•í•˜ê²Œ ê°™ìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.

* train & test text word ê°¯ìˆ˜

~~~
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
tt = pd.DataFrame(columns=['texts_len'])
count_words_f = lambda x: len(str(x).split())

tt['texts_len'] = list(map(count_words_f, train_texts))

def plot_doc_lengths(dataframe):
    max_seq_len = np.round(dataframe.texts_len.mean() + dataframe.texts_len.std()).astype(int)
    sns.distplot(tuple(dataframe.texts_len), hist=True, kde=True, label='Document lengths')
    plt.axvline(x=max_seq_len, color='k', linestyle='--', label=f'Sequence length mean:{max_seq_len}')
    plt.title('Train Document lengths')
    plt.legend()
    plt.show()
    print(f" ê°€ì¥ ê¸´ ë¬¸ì¥ì€ {tt['texts_len'].max()} ê°œì˜ ë‹¨ì–´ë¥¼, í‰ê·  ë¬¸ì¥ì€ {tt['texts_len'].mean()} ê°œì˜ ë‹¨ì–´ë¥¼, ê°€ì¥ ì§§ì€ ë¬¸ì¥ì€ {tt['texts_len'].min()} ê°œì˜ ë‹¨ì–´ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.")
plot_doc_lengths(tt)
~~~

![ã„¿ã„¿ã„¿ã„¿](https://user-images.githubusercontent.com/59636424/133604093-9599a4eb-a6c7-41ac-a383-c2ffe4225d3d.PNG)

~~~
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
tt = pd.DataFrame(columns=['texts_len'])
count_words_f = lambda x: len(str(x).split())

tt['texts_len'] = list(map(count_words_f, test_texts))

def plot_doc_lengths(dataframe):
    max_seq_len = np.round(dataframe.texts_len.mean() + dataframe.texts_len.std()).astype(int)
    sns.distplot(tuple(dataframe.texts_len), hist=True, kde=True, label='Document lengths')
    plt.axvline(x=max_seq_len, color='k', linestyle='--', label=f'Sequence length mean:{max_seq_len}')
    plt.title('Test Document lengths')
    plt.legend()
    plt.show()
    print(f" ê°€ì¥ ê¸´ ë¬¸ì¥ì€ {tt['texts_len'].max()} ê°œì˜ ë‹¨ì–´ë¥¼, í‰ê·  ë¬¸ì¥ì€ {tt['texts_len'].mean()} ê°œì˜ ë‹¨ì–´ë¥¼, ê°€ì¥ ì§§ì€ ë¬¸ì¥ì€ {tt['texts_len'].min()} ê°œì˜ ë‹¨ì–´ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.")
plot_doc_lengths(tt)
~~~

![ã…ã…ã…ã…ã…](https://user-images.githubusercontent.com/59636424/133604213-a3062c54-c643-4300-87c2-b94cc9b192a9.PNG)

### Tokenizer & Train

~~~
train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

config = DistilBertConfig.from_pretrained('distilbert-base-uncased',vocab_size=30522, \
                                          max_position_embeddings=512, sinusoidal_pos_embds=False, \
                                          n_layers=6, n_heads=12, dim=768, hidden_dim=3072, \
                                          dropout=0.1, attention_dropout=0.1, activation='gelu')
training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=5,              # total number of training epochs
    per_device_train_batch_size=32,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=100,
    learning_rate=5e-5,
    adam_epsilon=1e-08 # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•œ epsilon ê°’
)

model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased",config=config)
#ê³¼ì  í•©ì„ ë°©ì§€í•˜ê¸°ìœ„í•œ íš¨ìœ¨ì ì¸ ì •ê·œí™” ë„êµ¬
# multi-sample-dropoutì´ ë” ë‚®ì€ ì˜¤ë¥˜ìœ¨ê³¼ ì†ì‹¤ì„ ë‹¬ì„±í•˜ëŠ”ë° ë„ì›€ì„ ì¤€ë‹¤. -> lossê°€ 1ë²ˆì”© ë›°ëŠ” ê²ƒì„ ë³´ê³  ì‚¬ìš©!
#model.classifier = MyModel()


trainer = Trainer(
    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=val_dataset             # evaluation dataset
)

trainer.train()
~~~

-> **ì‹¤í—˜ ì¤‘, learning rate 5e-5 & epoch 5 & adam_epsilon=1e-08 & batch_Size=32 ì¼ ë•Œ,  0.9318ë¡œ ì„±ëŠ¥ì´ ì œì¼ ì¢‹ì•˜ë‹¤!**

* Multi sample dropout

~~~
import torch.nn as nn
def multi_sample_dropout(in_feature, out_feature, p=0.1, bias=True):
    return nn.Sequential(
        nn.Dropout(p),
        nn.Linear(in_feature, out_feature, bias)
    )

def multi_sample_dropout_forward(x, dropout_layer, hidden_size=2):
    return torch.mean(torch.stack([
        dropout_layer(x) for _ in range(hidden_size)], dim=0), dim=0)

class MyModel(nn.Module):
    def __init__(self, num_classes=2):
        super().__init__()
        self.your_layer = nn.Linear(768, 100)
        self.multilabel_dropout_layers = multi_sample_dropout(100, num_classes, 0.25)
    def forward(self, x):
        x = self.your_layer(x)
        # ê° ë“œë¡­ ì•„ì›ƒ ìƒ˜í”Œì— ëŒ€í•´ ê³„ì‚°ë˜ë©° ìµœì¢… ì†ì‹¤ ê°’ì€ ëª¨ë“  ë“œë¡­ ì•„ì›ƒ ìƒ˜í”Œì— ëŒ€í•œ ì†ì‹¤ ê°’ì„ í‰ê· í™”
        
        # ìš°ì„  4ê°œì˜ dropoutì„ ì‚¬ìš©í–ˆëŠ”ë° ì´ê²Œ Self-Ensemble íš¨ê³¼ê°€ ìˆë‹¤ê³  í•œë‹¤.
        return multi_sample_dropout_forward(x, self.multilabel_dropout_layers, 4)
~~~

-> Multi sample dropoutì„ ì‚¬ìš©í–ˆì§€ë§Œ, ë°ì´í„° labelì´ ê· ì¼í•´ì„œ ê·¸ëŸ°ì§€ ì‚¬ìš©í–ˆì„ ë•Œ, íš¨ê³¼ëŠ” ê±°ì˜ ì—†ì—ˆë‹¤.


### 3. Byte Pair Encoding ì„ íƒ ê³¼ì œ

**ì§§ê²Œ ì–˜ê¸°í•˜ë©´ out of vocabularyë¬¸ì œë¥¼ í•´ê²°í•œë‹¤!**

* í•™ìŠµë°ì´í„°ì—ì„œ ë“±ì¥í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ê°€ ë‚˜ì˜¤ëŠ” ê²½ìš°, Unknown tokenìœ¼ë¡œ ì²˜ë¦¬í•´ì£¼ì–´ ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ë„£ê²Œ ë˜ë©´ì„œ ì „ì²´ì ìœ¼ë¡œ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì €í•˜

* ê·¸ë ‡ë‹¤ê³ , ëª¨ë“  ë‹¨ì–´ì˜ embeddingì„ ë§Œë“¤ê¸°ì—ëŠ” í•„ìš”í•œ embedding parameterì˜ ìˆ˜ê°€ ì§€ë‚˜ì¹˜ê²Œ ë§ë‹¤!

**ê·¸ë˜ì„œ ë°ì´í„° ì••ì¶• ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜ì¸ byte pair encoding ê¸°ë²•ì„ ì ìš©í•œ sub-word tokenizaitonì´ë¼ëŠ” ê°œë… ë“±ì¥!**

---

* Byte Pair Encodingì´ë€??

**í•˜ë‚˜ì˜ ë‹¨ì–´ë¥¼ ì—¬ëŸ¬ ì„œë¸Œì›Œë“œë¡œ ë¶„ë¦¬í•´ì„œ ë‹¨ì–´ë¥¼ ì¸ì½”ë”© ë° ì„ë² ë”©í•˜ëŠ” ë°©ë²•ì´ë‹¤!**

-> ì¢€ ë” ìì„¸íˆëŠ” ìš°ì„  í›ˆë ¨ ë°ì´í„°ì— ìˆëŠ” ë‹¨ì–´ë“¤ì„ ëª¨ë“  ê¸€ì(chracters) ë˜ëŠ” ìœ ë‹ˆì½”ë“œ(unicode) ë‹¨ìœ„ë¡œ ë‹¨ì–´ ì§‘í•©(vocabulary)ë¥¼ ë§Œë“¤ê³ , ê°€ì¥ ë§ì´ ë“±ì¥í•˜ëŠ” ìœ ë‹ˆê·¸ë¨ì„ í•˜ë‚˜ì˜ ìœ ë‹ˆê·¸ë¨ìœ¼ë¡œ í†µí•©í•œë‹¤!

* Byte Pair Encoding êµ¬í˜„ ì½”ë“œ

~~~
from typing import List, Dict, Set
from itertools import chain
import re
from collections import defaultdict, Counter

# https://wikidocs.net/22592 -> wikidocs ì°¸ê³ !

def build_bpe(
        corpus: List[str],
        max_vocab_size: int
) -> List[int]:

    # Special tokens
    PAD = BytePairEncoding.PAD_token  # Index of <PAD> must be 0
    UNK = BytePairEncoding.UNK_token  # Index of <UNK> must be 1
    CLS = BytePairEncoding.CLS_token  # Index of <CLS> must be 2
    SEP = BytePairEncoding.SEP_token  # Index of <SEP> must be 3
    MSK = BytePairEncoding.MSK_token  # Index of <MSK> must be 4
    SPECIAL = [PAD, UNK, CLS, SEP, MSK]

    WORD_END = BytePairEncoding.WORD_END  # Use this token as the end of a word
    # YOUR CODE HERE
    
    # special token ì¶”ê°€
    idx2word = SPECIAL
    
    # corpus ë‹¨ì–´ë§ˆë‹¤ íŠ¹ìˆ˜ê¸°í˜¸ '_'ë¥¼ ëì— ë„£ì€ ë’¤, í•œê¸€ì ë‹¨ìœ„ë¡œ ì´ˆê¸°í™”í•˜ì—¬ dictionaryì— ì €ì¥
    """ 
    1. ì²˜ìŒì— ë‹¨ì–´ë¥¼ ëª¨ë‘ characterë¡œ ì˜ë¼ idx2word(vocablary)ì— ë„£ê¸°
    
    1-1. dict_vocab(dictionary)ì—ëŠ” ë‹¨ì–´ë§ˆë‹¤ character ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•´ì„œ ë§ˆì§€ë§‰ì— _ ë¶™ì´ê¸°
    
    if corpusê°€ ["lower", "newest"]ë¼ë©´,
    dict_vocab = {"l o w e r _":1, "n e w e s t _":1}ë¡œ ë§Œë“¤ì–´ì¤Œ!
    
    idx2word+=["l"]+["o"]+["w"]+["e"]+["r"]+["n"]+["s"]+["t"] 
    
    (ë‹¨, ë‹¨ì–´ 1ê°œë§Œ ë“¤ì–´ì˜¤ë©´ ì‹¤í–‰ X)
    """
    dict_vocab=dict()
    count_of_character=0
    for s in corpus:
        tmp = list(s)
        tmpstring = ""
        for i in range(len(tmp)):
            # if ë¶€ë¶„ì€ ì´ˆê¸° character ë‹¨ìœ„ë¡œ ìë¥¸ ê²ƒì„ listì— ì¶”ê°€
            if tmp[i] not in idx2word:
                idx2word+=[tmp[i]]
            tmpstring+=tmp[i]+" "
        tmpstring+="_"
        if tmpstring not in dict_vocab:
            dict_vocab[tmpstring]=1
        else:
            dict_vocab[tmpstring]+=1
    
    
    # ë‘ ê°œì˜ characterë¥¼ ìŒìœ¼ë¡œ ë¬¶ì–´ í•˜ë‚˜ì˜ unitìœ¼ë¡œ ë§Œë“¤ì–´ ë¹ˆë„ìˆ˜ ì²´í¬!
    """
    2. dict_vocab(dictionary)ì˜ ë‹¨ì–´ë§ˆë‹¤ ìœ ë‹ˆê·¸ë¨ì˜ ìŒì„ í•˜ë‚˜ì˜ ìœ ë‹ˆê·¸ë¨ìœ¼ë¡œ ë§Œë“¤ê³  ë¹ˆë„ìˆ˜ ì²´í¬!
    
    if dict_vocab = {"l o w e r _":1, "n e w e s t _":1, 'w i d e s t _':1} ë¼ë©´, 
    pairs = { (e,s):2, (o,w):1 .....} ë¼ëŠ” charcter 2ê°œ ìŒì˜ ë¬¶ìŒê³¼ ë¹ˆë„ìˆ˜ ì²´í¬!
    """ 
    def get_stats(dict_vocab):
        pairs = defaultdict(int)
        for word, freq in dict_vocab.items():
            symbols = word.split()
            for i in range(len(symbols)-1):
                pairs[symbols[i],symbols[i+1]] += freq
        return pairs
    
    
    # ê°€ì¥ ë¹ˆë„ìˆ˜ ë†’ì€ pairë¡œ í¬í•¨í•˜ëŠ” ë‹¨ì–´ë¥¼ ì²´í¬í•˜ì—¬ ê°™ì€ ë¶€ë¶„ì€ ë¶™ì—¬ì¤€ë‹¤!
    """ 
    3. ê°€ì¥ ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ìœ ë‹ˆê·¸ë¨ì˜ ìŒì„ í•˜ë‚˜ì˜ ìœ ë‹ˆê·¸ë¨ìœ¼ë¡œ í†µí•©
    
    3-1. í†µí•©í•œ ìœ ë‹ˆê·¸ë¨ìœ¼ë¡œ dict_vocab(dictionary) ì—…ë°ì´íŠ¸!
    
    if v_in = {"l o w e r _":1, "n e w e s t _":1, 'w i d e s t _':1} ì´ê³  pair = (e,s)ë¼ë©´,
    v_out = {"l o w e r _":1, "n e w es t _":1, 'w i d es t _':1}ë¡œ ë§Œë“¤ì–´ì¤Œ!
    """ 
    def merge_vocab(pair, v_in):
        v_out = {}
        #escapeë¡œ ë„ì–´ì“°ê¸° ê¸°ì¤€ìœ¼ë¡œ '\' ì ìš©(í•œ ê¸€ì ì‚¬ì´ì‚¬ì´ì— '\' ë„£ê¸°!)
        #ì´ ì½”ë“œê°€ ne -> n\ eë¡œ ë§Œë“¤ì–´ 'n e'ë¡œ ë“¤ì–´ê°€ëŠ” ë¶€ë¶„ì´ ìˆëŠ” ë¶€ë¶„ í™•ì¸
        bigram = re.escape(' '.join(pair))
        
        # (n,e) ìŒì„ neë¡œ ë³€ê²½
        x = ''.join(pair)
        
        for word in v_in:
            # wordì—ì„œ n e(bigram 'n\ e' pattern)ê°€ ë“¤ì–´ê°€ëŠ” ë¶€ë¶„ì„ ne(ë³€ìˆ˜ x)ë¡œ ë¶™ì—¬ì¤Œ!
            w_out = re.sub(bigram, x, word)
            
            # dictionaryì˜ ë³€ê²½ëœ ë‹¨ì–´ì— ë¹ˆë„ìˆ˜ ì €ì¥!
            v_out[w_out] = v_in[word]
        return v_out
    
    
    # byte pair encoding ìµœëŒ€ vocabularyí¬ê¸°-1ë§Œí¼ ëŒë¦¬ê¸°!
    for i in range(max_vocab_size-1):
        pairs = get_stats(dict_vocab)
        
        #ì´ì œ ë§Œë“¤ ì§ì´ ì—†ë‹¤ë©´ ì¤‘ì§€!
        if not pairs:
            break
        
        #character ìŒ ì¤‘ì—, ê°€ì¥ ë¹ˆë„ìˆ˜ ë†’ì€ ìŒ ì¶”ì¶œ
        best = max(pairs, key=pairs.get)
        dict_vocab = merge_vocab(best, dict_vocab)
        
        # ë¹ˆë„ìˆ˜ ê°€ì¥ ë†’ì€ pairë¥¼ idx2wordì— ë„£ì–´ì£¼ê¸°!
        """ 
        4. ê°€ì¥ ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ìœ ë‹ˆê·¸ë¨ì˜ ìŒì„ í•˜ë‚˜ì˜ ìœ ë‹ˆê·¸ë¨ìœ¼ë¡œ í†µí•©
        
        4-1. idx2word(vocabulary)ì— í†µí•©ëœ ìœ ë‹ˆê·¸ë¨ì„ ì¶”ê°€!
        
        if bestê°€ (e,s) ë¼ë©´, 
        idx2wordì— "es"ë¥¼ ì¶”ê°€í•´ì¤€ë‹¤! 
        
        ì œì•½ì‚¬í•­1. ìµœëŒ€ vocab sizeë¥¼ ë„˜ì§€ ì•Šë„ë¡!
        ì œì•½ì‚¬í•­2. corpusë¡œ ë°›ì€ ë‹¨ì–´ê°€ 2ê°œ ì´ìƒì¸ ê²½ìš° ì ìš©! 
        
        (ì œì•½ì‚¬í•­ 2ë²ˆì˜ ê²½ìš°, Test case 2ë²ˆì„ í†µê³¼í•˜ê¸° ìœ„í•´ì„œ ì‹œí–‰í•¨)
   
        """ 
        if ''.join(best) not in idx2word and len(idx2word)<max_vocab_size-1:
            idx2word+=[''.join(best)]
        
        """ 
        ìµœëŒ€ vocabulary ì‚¬ì´ì¦ˆ ë„˜ìœ¼ë©´ ë©ˆì¶¤!(max_vocab_size)
   
        """ 
        if len(idx2word) == max_vocab_size-1:
            break
            
    
    idx2word+=[WORD_END]
    idx2word = sorted(idx2word, key=len, reverse=True)
    
    return idx2word
~~~

-> íŠ¹ì´í•˜ê²Œ [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf) ë…¼ë¬¸ì— ì½”ë“œê°€ ì˜ ì„¤ëª…ë˜ì–´ìˆë‹¤!!

ê³¼ì •ì€ ì•„ë˜ì™€ ê°™ë‹¤!

        1. ì²˜ìŒì— ë‹¨ì–´ë¥¼ ëª¨ë‘ characterë¡œ ì˜ë¼ idx2word(vocablary)ì— ë„£ê¸°
        
        1-1. dict_vocab(dictionary)ì—ëŠ” ë‹¨ì–´ë§ˆë‹¤ character ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•´ì„œ ë§ˆì§€ë§‰ì— _ ë¶™ì´ê¸°
        
        2. dict_vocab(dictionary)ì˜ ë‹¨ì–´ë§ˆë‹¤ ìœ ë‹ˆê·¸ë¨ì˜ ìŒì„ í•˜ë‚˜ì˜ ìœ ë‹ˆê·¸ë¨ìœ¼ë¡œ ë§Œë“¤ê³  ë¹ˆë„ìˆ˜ ì²´í¬!
        
        3. ê°€ì¥ ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ìœ ë‹ˆê·¸ë¨ì˜ ìŒì„ í•˜ë‚˜ì˜ ìœ ë‹ˆê·¸ë¨ìœ¼ë¡œ í†µí•©
        
        3-1. í†µí•©í•œ ìœ ë‹ˆê·¸ë¨ìœ¼ë¡œ dict_vocab(dictionary) ì—…ë°ì´íŠ¸!
        
        4. ê°€ì¥ ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ìœ ë‹ˆê·¸ë¨ì˜ ìŒì„ í•˜ë‚˜ì˜ ìœ ë‹ˆê·¸ë¨ìœ¼ë¡œ í†µí•©
        
        4-1. idx2word(vocabulary)ì— í†µí•©ëœ ìœ ë‹ˆê·¸ë¨ì„ ì¶”ê°€!
        
        5. 2 ~ 4-1ê¹Œì§€ì˜ ê³¼ì •ì„ ë°˜ë³µí•œë‹¤!

---

* Wordpiece Modelì´ë€??

WordPiece Modelì€ BPEì˜ ë³€í˜• ì•Œê³ ë¦¬ì¦˜ì´ë‹¤!

**BPEì™€ ë‹¬ë¦¬, ë³‘í•©ë˜ì—ˆì„ ë•Œ ì½”í¼ìŠ¤ì˜ ìš°ë„(Likelihood)ë¥¼ ê°€ì¥ ë†’ì´ëŠ” ìŒì„ ë³‘í•©í•œë‹¤!!**
