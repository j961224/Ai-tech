# 9ì›” 16ì¼ í•œ ê²ƒ ì •ë¦¬!

## 1. ì„ íƒ ê³¼ì œ ì •ë¦¬!

### 1. huggingFace Finetuning

#### Dataset EDA

ìš°ì„ , ì´ë²ˆ ê³¼ì œì—ì„œëŠ” **imdb ì˜í™” ë¦¬ë·° ë°ì´í„°**ë¥¼ ì‚¬ìš©í•  ì˜ˆì •ì´ë‹¤!

* ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°

~~~
train_texts, train_labels = read_imdb_split('aclImdb/train')
test_texts, test_labels = read_imdb_split('aclImdb/test')
~~~

* ë°ì´í„° label ìˆ˜ì™€ ë¶„í¬ í™•ì¸

~~~
#ê¸ë¶€ì •ì´ ê±°ì˜ ê°¯ìˆ˜ê°€ ì¼ì¹˜
print(train_labels.count(0))
print(train_labels.count(1))
print(test_labels.count(0))
print(test_labels.count(1))

12500
12500
12500
12500
~~~

-> ë°ì´í„° ìˆ˜ëŠ” ì •í™•í•˜ê²Œ ê°™ìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.

* train & test text word ê°¯ìˆ˜

~~~
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
tt = pd.DataFrame(columns=['texts_len'])
count_words_f = lambda x: len(str(x).split())

tt['texts_len'] = list(map(count_words_f, train_texts))

def plot_doc_lengths(dataframe):
    max_seq_len = np.round(dataframe.texts_len.mean() + dataframe.texts_len.std()).astype(int)
    sns.distplot(tuple(dataframe.texts_len), hist=True, kde=True, label='Document lengths')
    plt.axvline(x=max_seq_len, color='k', linestyle='--', label=f'Sequence length mean:{max_seq_len}')
    plt.title('Train Document lengths')
    plt.legend()
    plt.show()
    print(f" ê°€ì¥ ê¸´ ë¬¸ì¥ì€ {tt['texts_len'].max()} ê°œì˜ ë‹¨ì–´ë¥¼, í‰ê·  ë¬¸ì¥ì€ {tt['texts_len'].mean()} ê°œì˜ ë‹¨ì–´ë¥¼, ê°€ì¥ ì§§ì€ ë¬¸ì¥ì€ {tt['texts_len'].min()} ê°œì˜ ë‹¨ì–´ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.")
plot_doc_lengths(tt)
~~~

![ã„¿ã„¿ã„¿ã„¿](https://user-images.githubusercontent.com/59636424/133604093-9599a4eb-a6c7-41ac-a383-c2ffe4225d3d.PNG)

~~~
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
tt = pd.DataFrame(columns=['texts_len'])
count_words_f = lambda x: len(str(x).split())

tt['texts_len'] = list(map(count_words_f, test_texts))

def plot_doc_lengths(dataframe):
    max_seq_len = np.round(dataframe.texts_len.mean() + dataframe.texts_len.std()).astype(int)
    sns.distplot(tuple(dataframe.texts_len), hist=True, kde=True, label='Document lengths')
    plt.axvline(x=max_seq_len, color='k', linestyle='--', label=f'Sequence length mean:{max_seq_len}')
    plt.title('Test Document lengths')
    plt.legend()
    plt.show()
    print(f" ê°€ì¥ ê¸´ ë¬¸ì¥ì€ {tt['texts_len'].max()} ê°œì˜ ë‹¨ì–´ë¥¼, í‰ê·  ë¬¸ì¥ì€ {tt['texts_len'].mean()} ê°œì˜ ë‹¨ì–´ë¥¼, ê°€ì¥ ì§§ì€ ë¬¸ì¥ì€ {tt['texts_len'].min()} ê°œì˜ ë‹¨ì–´ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.")
plot_doc_lengths(tt)
~~~

![ã…ã…ã…ã…ã…](https://user-images.githubusercontent.com/59636424/133604213-a3062c54-c643-4300-87c2-b94cc9b192a9.PNG)

### Tokenizer & Train

~~~
train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

config = DistilBertConfig.from_pretrained('distilbert-base-uncased',vocab_size=30522, \
                                          max_position_embeddings=512, sinusoidal_pos_embds=False, \
                                          n_layers=6, n_heads=12, dim=768, hidden_dim=3072, \
                                          dropout=0.1, attention_dropout=0.1, activation='gelu')
training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=5,              # total number of training epochs
    per_device_train_batch_size=32,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=100,
    learning_rate=5e-5,
    adam_epsilon=1e-08 # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•œ epsilon ê°’
)

model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased",config=config)
#ê³¼ì  í•©ì„ ë°©ì§€í•˜ê¸°ìœ„í•œ íš¨ìœ¨ì ì¸ ì •ê·œí™” ë„êµ¬
# multi-sample-dropoutì´ ë” ë‚®ì€ ì˜¤ë¥˜ìœ¨ê³¼ ì†ì‹¤ì„ ë‹¬ì„±í•˜ëŠ”ë° ë„ì›€ì„ ì¤€ë‹¤. -> lossê°€ 1ë²ˆì”© ë›°ëŠ” ê²ƒì„ ë³´ê³  ì‚¬ìš©!
#model.classifier = MyModel()


trainer = Trainer(
    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=val_dataset             # evaluation dataset
)

trainer.train()
~~~

-> **ì‹¤í—˜ ì¤‘, learning rate 5e-5 & epoch 5 & adam_epsilon=1e-08 & batch_Size=32 ì¼ ë•Œ,  0.9318ë¡œ ì„±ëŠ¥ì´ ì œì¼ ì¢‹ì•˜ë‹¤!**
