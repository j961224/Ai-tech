# 1. 9월 14일 공부한 것!

## 1. Self-Spervised Pre-Training Models

### Recent Trends

* Transformer는 self-attention과 sequence encoder와 decoder는 다양한 분야에 쓰이고 있다.

* 최근에 self-attention block을 더 쌓아서 사용하고 있다. -> 대규모 학습 데이터로 학습하는 self-supervised learning framework를 사용!

* 자연어 생성 task에서 처음부터 시작해서 하나씩 하나씩 단어를 생성하는 greedy decoding framework에서 벗어나지 못 하고 있다.

### GPT-1

![xxxxx](https://user-images.githubusercontent.com/59636424/133192115-6e3d006c-8bfa-4d29-8413-b4521a3fb852.PNG)

다양한 special token을 제안해서 많은 task를 동시에 커버 가능하다!

-> **text prediction은 첫 단어부터 다음 단어를 순차적으로 예측하는 언어 모델** task를 수행!!

-> 입출력 sequence가 별도로 있는 것이 아니라, **I go home 문장이라면 SOS-> I, 생성된 I -> go 이렇게 순차적으로 다음 단어 예측하는 언어 모델로 12개의 layer로 학습**!

---

* if label된 데이터를 사용한다면?! (Classification)

![xxxx](https://user-images.githubusercontent.com/59636424/133192877-3dd76487-d8cd-4eca-93fc-43beb66d81b7.PNG)

이 때, **Task classifier를 사용**한다!

text 문장을 넣고 앞에 **start token**과 뒤에 end of sentence라는 좀 더 특별한 **extract token**을 넣는다. 

-> 최종적으로 나온 **extract token에 해당하는 encoding vector를 최종 output layer의 input으로 주어 긍/부정 task로 학습**한다!

### BERT Summary

* self-attention block을 그대로 사용하되 2가지 버전을 제안

      1. BERT BASE: self attention block(L) 12, attention head 숫자(A) 12, 각 self-attention의 encoding vector 차원 수(H) 768

      2. BERT LARGE: self attention block(L) 24, attention head 숫자(A) 16, 각 self-attention의 encoding vector 차원 수(H) 1024

* Input 표현

      * WordPiece(단어를 좀 더 잘게 쪼갬)로 표현해서 subword embedding을 한다.
      
      * Learned positional embedding 
      
      -> transformer의 sin, cos 주기들도 사전에 결정된 값들을 사용했다면 matrix 조차 random initalization의해서 전체 모델 학습 과정을 통해 embedding vector도 학습에 의해 최적화시킨다!
      
      * [CLS]와 [SEP] 추가! -> CLS는 classification embedding, SEP은 Packed sentence embedding
      
      * Segment Embedding

      -> 문장의 독립된 정보로 보면, 2번째 문장에서 첫 단어가 'he'라면 2번째 문장이라는 정보를 넣기 위해 segment embeddings을 추가!
      
      -> 문장 level의 position 반영!!
      
      


* Entailment task

![xzxzxx](https://user-images.githubusercontent.com/59636424/133193404-f8646fc6-76fd-4e19-9f64-001acfb840a3.PNG)

Premise(어제 존이 결혼을 했다.), Hypothesis(어제 최소한 1명은 결혼했다.)가 주어지면, Premise가 참이면 Hypothesis가 참이다! 

=> **논리적 내포 관계 (다수의 문장을 입력해서 예측을 수행해야 한다!)**

Premise와 Hypothesis를 하나의 sequence로 만들되, **문장 사이에 Delim token을 추가**한다!

-> 위와 마찬가지로 **extract token을 최종적인 output layer에 통과하여 논리적으로 내포 관계인지, 모순 관계인지 분류!**

-> **extract token이 Query로 사용되어서 task에 필요로 하는 정보를 적절히 추출!!**

* Similarity와 Multiple Choice가 있다!!

---

**주제 분류나 document의 장르 분류**하기 위해서는 extract token는 때어내고 그 전 단계에서 나오는 word별 encoding vector를 사용

-> transformer encoder를 **추가적인 layer를 붙여 주제 분류를 하면서 학습**을 한다! (Task Classifier)

### Improving Language Understanding by Generative Pre-trainging

* 각 task별로 존재하던 모델보다 더 높은 성능을 보인다!

## Bert

GPT와 마찬가지로 언어 모델링이라는 task로 문장의 일부 단어를 맞추는 task이다!

* 이러한 transformer 이전에 Bi-LSTM 기반 pretrained한 접근법도 존재했음!(ELMo method)

![xcxcxc](https://user-images.githubusercontent.com/59636424/133194764-971bc1da-ecf3-4b69-ab23-3a96dcfecbe5.PNG)

-> LSTM encoder를 다 대체를 한 모델들이 단어를 맞추는 pretrained task에서도 더 많은 데이터를 학습되도록 모델 고도화됨!

### Masked Language Model

이때까지, 앞의 정보만을 보고 단어를 맞추는 pretrained가 되어 왔다.(앞 쪽 문맥만!)

=> 그래서 앞, 뒤 문맥을 사용하고자 한다!

### Pre-training Tasks in BERT

* Masked Language Model(MLM)

**각 문장 단어에 대해 일정한 확률로 'MASK'라는 단어로 치환**해준다.

-> 그렇게 해서 **MASK라는 단어에 어떤 단어인지 맞추는 형태로 학습**을 한다!

=> **몇 퍼센트의 단어를 MASK 단어로 치환해 맞출지를 사전에 잘 결정**해야한다!

* if 15%라고 한다면?

-> 15% 넘어버리면 **단어를 맞추기에 충분한 정보가 없다!**

-> 15% 아래로 설정하면 transformer 모델이 전체 정보를 encoding 과정이 많은 계산을 필요로 하는데 **적은 MASK를 예측하면 효율이 떨어지고 학습이 느리다!**

**15%가 제일 적절하다!**

---

* 15%의 단어를 맞추도록 했을 때, 해당 단어를 100% 다 MASK로 치환하면 부작용이 생긴다!

    * 가령 문서 주제 분류 같은 것을 수행하면 MASK token이 등장하지 않는다!

    => pretrained 당시 주어지는 입력 데이터 양상이나 패턴이 실제 task를 수행하는데 주어지는 입력과 다른 특성을 보일 수 있음! -> 이것이 학습 방해할 수 있다.
    
    1. 위에 100단어 중에 15개를 masking한다면, 이 중, 80%인 12개를 실제로 MASK token으로 치환해서 맞추도록 한다!

    2. 10%인 1.5개 단어는 랜덤 단어로 치환한다. -> 해당 단어는 잘못된 단어라고 하더라도 원래 있어야 하는 단어로 잘 복원해줄 수 있도록 문제 난이도를 높임!

    3. 10%인 1.5개 단어는 전혀 바꾸지 않고 원래 단어로 둔다! -> 해당 단어는 다른 단어로 바뀌어야 하는지를 예측했을 때, 원래 있었던 단어라고 소신있게 예측할 수 있는 학습을 유도!


### Next Sentence Prediction

문장 level task에 대응하기 위해 pretrained 기법!

#### example

![ccc](https://user-images.githubusercontent.com/59636424/133198339-5d8cf1b7-91f3-4bda-9c4a-6ba80b84f77f.PNG)

주어진 하나의 글에서 2개의 문장을 뽑아 중간중간에와 끝에 **[SEP]** 을 넣는다. -> 다수 문장 level의 예측 task의 역할로 **[CLS]** token을 문장 앞에 추가한다!

-> 2**문장이 연속적으로 나올 수 있는 문장인지 아닌지를 예측하는 Next Sentence를 예측**하는 binary classification을 추가했다.

* 과정

    1. 전체 sequence를 transformer로 인코딩한다.

    2. MASK 자리에 있어야 하는 단어를 해당하는 encoding vector로 예측한다.

    3. CLS token은 해당하는 encoding vector로 output layer를 두고 binary classifier 수행하도록 한다!

    -> 실제 2문장이 인접한 문장인지 아닌지를 판별한다.
    
    -> Backpropagation을 통해서 CLS token을 통해 전체 network이 학습 진행!
    
### Bert Summary

* Model Architecture

      Bert Base: self-attention 갯수(L) 12, encoding vector 차원(H) 768, head 갯수(A) 12
      
      Bert LARGE: self-attention 갯수(L) 24, encoding vector 차원(H) 1024, head 갯수(A) 16
 
* Input 표현

      WordPiece embeddings (단어를 더 잘게 쪼개서 subword embedding 시도)
      
      Learned positional embedding (positional embedding도 학습한다.)
      
      -> positional embedding은 sin, cos 주기도 사전에 결정된 값을 써서 position마다의 embedding vector를 사용했는데 random initalization으로 전체적인 모델 학습으로 최적화된 값으로 도출한다.
      
      CLS와 SEP -> CLS는 classification embedding이고 SEP은 packed sentence embedding이다.
      
      Segment Embedding
      
      -> Positional embedding 시, 각 문장 level을 독립적으로 보고자 할 때, 2번째 문장에는 2번째 문장의 위치임을 알리기 위해서 Segment Embedding을 추가로 더함


