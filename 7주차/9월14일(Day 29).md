# 1. 9월 14일 공부한 것!

## 1. Self-Spervised Pre-Training Models

### Recent Trends

* Transformer는 self-attention과 sequence encoder와 decoder는 다양한 분야에 쓰이고 있다.

* 최근에 self-attention block을 더 쌓아서 사용하고 있다. -> 대규모 학습 데이터로 학습하는 self-supervised learning framework를 사용!

* 자연어 생성 task에서 처음부터 시작해서 하나씩 하나씩 단어를 생성하는 greedy decoding framework에서 벗어나지 못 하고 있다.

### GPT-1

![xxxxx](https://user-images.githubusercontent.com/59636424/133192115-6e3d006c-8bfa-4d29-8413-b4521a3fb852.PNG)

다양한 special token을 제안해서 많은 task를 동시에 커버 가능하다!

-> **text prediction은 첫 단어부터 다음 단어를 순차적으로 예측하는 언어 모델** task를 수행!!

-> 입출력 sequence가 별도로 있는 것이 아니라, **I go home 문장이라면 SOS-> I, 생성된 I -> go 이렇게 순차적으로 다음 단어 예측하는 언어 모델로 12개의 layer로 학습**!

---

* if label된 데이터를 사용한다면?! (Classification)

![xxxx](https://user-images.githubusercontent.com/59636424/133192877-3dd76487-d8cd-4eca-93fc-43beb66d81b7.PNG)

이 때, **Task classifier를 사용**한다!

text 문장을 넣고 앞에 **start token**과 뒤에 end of sentence라는 좀 더 특별한 **extract token**을 넣는다. 

-> 최종적으로 나온 **extract token에 해당하는 encoding vector를 최종 output layer의 input으로 주어 긍/부정 task로 학습**한다!

### BERT Summary

* self-attention block을 그대로 사용하되 2가지 버전을 제안

      1. BERT BASE: self attention block(L) 12, attention head 숫자(A) 12, 각 self-attention의 encoding vector 차원 수(H) 768

      2. BERT LARGE: self attention block(L) 24, attention head 숫자(A) 16, 각 self-attention의 encoding vector 차원 수(H) 1024

* Input 표현

      * WordPiece(단어를 좀 더 잘게 쪼갬)로 표현해서 subword embedding을 한다.
      
      * Learned positional embedding 
      
      -> transformer의 sin, cos 주기들도 사전에 결정된 값들을 사용했다면 matrix 조차 random initalization의해서 전체 모델 학습 과정을 통해 embedding vector도 학습에 의해 최적화시킨다!
      
      * [CLS]와 [SEP] 추가! -> CLS는 classification embedding, SEP은 Packed sentence embedding
      
      * Segment Embedding

      -> 문장의 독립된 정보로 보면, 2번째 문장에서 첫 단어가 'he'라면 2번째 문장이라는 정보를 넣기 위해 segment embeddings을 추가!
      
      -> 문장 level의 position 반영!!
      
      


* Entailment task

![xzxzxx](https://user-images.githubusercontent.com/59636424/133193404-f8646fc6-76fd-4e19-9f64-001acfb840a3.PNG)

Premise(어제 존이 결혼을 했다.), Hypothesis(어제 최소한 1명은 결혼했다.)가 주어지면, Premise가 참이면 Hypothesis가 참이다! 

=> **논리적 내포 관계 (다수의 문장을 입력해서 예측을 수행해야 한다!)**

Premise와 Hypothesis를 하나의 sequence로 만들되, **문장 사이에 Delim token을 추가**한다!

-> 위와 마찬가지로 **extract token을 최종적인 output layer에 통과하여 논리적으로 내포 관계인지, 모순 관계인지 분류!**

-> **extract token이 Query로 사용되어서 task에 필요로 하는 정보를 적절히 추출!!**

* Similarity와 Multiple Choice가 있다!!

---

**주제 분류나 document의 장르 분류**하기 위해서는 extract token는 때어내고 그 전 단계에서 나오는 word별 encoding vector를 사용

-> transformer encoder를 **추가적인 layer를 붙여 주제 분류를 하면서 학습**을 한다! (Task Classifier)

### Improving Language Understanding by Generative Pre-trainging

* 각 task별로 존재하던 모델보다 더 높은 성능을 보인다!

## Bert

GPT와 마찬가지로 언어 모델링이라는 task로 문장의 일부 단어를 맞추는 task이다!

* 이러한 transformer 이전에 Bi-LSTM 기반 pretrained한 접근법도 존재했음!(ELMo method)

![xcxcxc](https://user-images.githubusercontent.com/59636424/133194764-971bc1da-ecf3-4b69-ab23-3a96dcfecbe5.PNG)

-> LSTM encoder를 다 대체를 한 모델들이 단어를 맞추는 pretrained task에서도 더 많은 데이터를 학습되도록 모델 고도화됨!

### Masked Language Model

이때까지, 앞의 정보만을 보고 단어를 맞추는 pretrained가 되어 왔다.(앞 쪽 문맥만!)

=> 그래서 앞, 뒤 문맥을 사용하고자 한다!

### Pre-training Tasks in BERT

* Masked Language Model(MLM)

**각 문장 단어에 대해 일정한 확률로 'MASK'라는 단어로 치환**해준다.

-> 그렇게 해서 **MASK라는 단어에 어떤 단어인지 맞추는 형태로 학습**을 한다!

=> **몇 퍼센트의 단어를 MASK 단어로 치환해 맞출지를 사전에 잘 결정**해야한다!

* if 15%라고 한다면?

-> 15% 넘어버리면 **단어를 맞추기에 충분한 정보가 없다!**

-> 15% 아래로 설정하면 transformer 모델이 전체 정보를 encoding 과정이 많은 계산을 필요로 하는데 **적은 MASK를 예측하면 효율이 떨어지고 학습이 느리다!**

**15%가 제일 적절하다!**

---

* 15%의 단어를 맞추도록 했을 때, 해당 단어를 100% 다 MASK로 치환하면 부작용이 생긴다!

    * 가령 문서 주제 분류 같은 것을 수행하면 MASK token이 등장하지 않는다!

    => pretrained 당시 주어지는 입력 데이터 양상이나 패턴이 실제 task를 수행하는데 주어지는 입력과 다른 특성을 보일 수 있음! -> 이것이 학습 방해할 수 있다.
    
    1. 위에 100단어 중에 15개를 masking한다면, 이 중, 80%인 12개를 실제로 MASK token으로 치환해서 맞추도록 한다!

    2. 10%인 1.5개 단어는 랜덤 단어로 치환한다. -> 해당 단어는 잘못된 단어라고 하더라도 원래 있어야 하는 단어로 잘 복원해줄 수 있도록 문제 난이도를 높임!

    3. 10%인 1.5개 단어는 전혀 바꾸지 않고 원래 단어로 둔다! -> 해당 단어는 다른 단어로 바뀌어야 하는지를 예측했을 때, 원래 있었던 단어라고 소신있게 예측할 수 있는 학습을 유도!


### Next Sentence Prediction

문장 level task에 대응하기 위해 pretrained 기법!

#### example

![ccc](https://user-images.githubusercontent.com/59636424/133198339-5d8cf1b7-91f3-4bda-9c4a-6ba80b84f77f.PNG)

주어진 하나의 글에서 2개의 문장을 뽑아 중간중간에와 끝에 **[SEP]** 을 넣는다. -> 다수 문장 level의 예측 task의 역할로 **[CLS]** token을 문장 앞에 추가한다!

-> 2**문장이 연속적으로 나올 수 있는 문장인지 아닌지를 예측하는 Next Sentence를 예측**하는 binary classification을 추가했다.

* 과정

    1. 전체 sequence를 transformer로 인코딩한다.

    2. MASK 자리에 있어야 하는 단어를 해당하는 encoding vector로 예측한다.

    3. CLS token은 해당하는 encoding vector로 output layer를 두고 binary classifier 수행하도록 한다!

    -> 실제 2문장이 인접한 문장인지 아닌지를 판별한다.
    
    -> Backpropagation을 통해서 CLS token을 통해 전체 network이 학습 진행!
    
### Bert Summary

* Model Architecture

      Bert Base: self-attention 갯수(L) 12, encoding vector 차원(H) 768, head 갯수(A) 12
      
      Bert LARGE: self-attention 갯수(L) 24, encoding vector 차원(H) 1024, head 갯수(A) 16
 
* Input 표현

      * WordPiece embeddings (단어를 더 잘게 쪼개서 subword embedding 시도)
      
      * Learned positional embedding (positional embedding도 학습한다.)
      
      -> positional embedding은 sin, cos 주기도 사전에 결정된 값을 써서 position마다의 embedding vector를 사용했는데 random initalization으로 전체적인 모델 학습으로 최적화된 값으로 도출한다.
      
      * CLS와 SEP -> CLS는 classification embedding이고 SEP은 packed sentence embedding이다.
      
      * Segment Embedding
      
      -> Positional embedding 시, 각 문장 level을 독립적으로 보고자 할 때, 2번째 문장에는 2번째 문장의 위치임을 알리기 위해서 Segment Embedding을 추가로 더함


### BERT와 GPT-2 간의 차이점

![whatthe](https://user-images.githubusercontent.com/59636424/133199046-40631e0d-ca55-46bb-af8c-42805c5737bc.PNG)

* GPT-2의 경우, 주어진 sequence encoding 시, 바로 다음 단어를 예측하므로 특정 timestep에서 다음에 나타나는 단어 접근 허용 불가!!! (왼쪽 정보만 access)

      -> Masked multi self attention을 사용한다!

* BERT의 경우, MASK로 치환된 token을 예측하니 전체 모든 단어들이 접근이 가능하다!

### BERT: Fine-tuning Process

* Sentence pair에 대해서 classification하는 경우(논리적 내포관계 판별)

![aaaa](https://user-images.githubusercontent.com/59636424/133199651-4f88d91c-b47b-4e68-93f5-f9573a5903d8.PNG)

      1. SEP token으로 문장을 구분한다.
      
      2. BERT를 통해 Encoding 한다. -> 각 Word들에 대한 encoding vector를 구한다.
      
      3. CLS token에 해당하는 encoding vector를 output layer의 입력으로 준다. -> 다수 문장에 대한 예측 task 수행

* 단일 문장의 classification인 경우

![wwww](https://user-images.githubusercontent.com/59636424/133199885-799076bc-6eb2-4991-9e6b-5f78eb6cc39f.PNG)

      1. 1문장을 주고 CLS token에 해당하는 encoding vector을 output layer에 입력을 주오 task 수행
 
* Question Answering의 경우

![qqqqq](https://user-images.githubusercontent.com/59636424/133200076-eaff31c0-7e2d-4183-908b-4c0ac68123ee.PNG)

* 한 문장에 단어별로 classification해야하는 경우(PoS tagging과 같은 경우)

![ttyyyy](https://user-images.githubusercontent.com/59636424/133200279-fe45cf84-421e-4dc1-bdfc-84ec9d639bc0.PNG)

      1. CLS token을 포함해서 encoding vector를 얻어낸다.

      2. 이 vector들을 동일한 output layer에 통과시켜 각 word 별 classification 수행!

### BERT vs GPT-1

* Training data 사이즈

      * GPT는 800M word들을 사용

      * BERT는 2500M word들을 사용

* special tokens training

      * BERT는 SEP, CLS 사용

      * BERT는 여러 문장에서 각 문장 index를 표현하는 segment embedding을 사용

* Batch size(size가 크면 모델 학습 안정화와 성능이 좋다.)

      * BERT는 128000 단어

      * GPT는 32000 단어

* Task specific fine-tuning

      * GPT는 learning rate을 동일한 5e-5를 사용

      * BERT는 task별로 learning rate를 선택

### BERT: GLUE Benchmark Results

![ff](https://user-images.githubusercontent.com/59636424/133201078-562f578b-d76e-4e0a-a645-7b6b5997ccbd.PNG)

fine-tuning 시, 일관적으로 BERT가 좋은 성능을 보여준다!!

GLUE data는 각 task의 다앙한 데이터가 존재!

### Machine Reading Comprehension(MRC), Question Answering

* MRC: BERT를 통해 fine-tuning으로 더 좋은 성능을 얻을 수 있는 대표적 task이다!

![bbb](https://user-images.githubusercontent.com/59636424/133201318-c72c75c7-9e54-4b33-89e0-47d73953bd47.PNG)

-> 기본적으로 질의 응답형태!

-> 기계 독해에 기반한 질문에 대한 정보를 잘 추출해서 예측하는 방법

### BERT: SQuAD 1.1

SQuAD 데이터는 많은 사람들로부터 task를 수행도록해서 수집된 데이터! (지문을 읽고 해당 독해를 통한 풀 수 있는 문제와 답이 구축되어 있다.)

![cvvv](https://user-images.githubusercontent.com/59636424/133201967-ee3dd6ec-e4fd-49a4-bda4-ffdbfc1597c9.PNG)

* Bert로 질의응답을 수행한다면?

      1. 지문과 질문을 2개 서로 다른 문장인 것처럼 SEP을 통해 concat을 하여 하나의 Sequence로 만들어서 BERT를 통해 인코딩 진행

      2. 지문 상의 단어별로 word encoding이 나온다!
      
      -> 정답에 해당할법한 위치(지문 상의 특정 문구)를 예측하도록 한다.
      
      2-1. 지문에서 답에 해당하는 문구가 시작하는 위치를 예측해야한다.
      
      -> 예측하기 위해, 여러 단어 중 fully connected layer -> softmax를 통과를 한 것에서 정답의 첫 번째 위치 logit값이 100%에 가깝도록 학습
      
      2-2. 모델이 정답이 끝나는 시점도 예측해야한다!
      
      -> 다른 fully connected layer를 만들어 softmax도 통과시켜서 정답의 끝 위치 logit값이 100%에 가깝도록 학습시킨다!
      
### BERT: SQuAD 2.0
 
* SQuAD 2.0: 주어진 지문에 대해서 항상 정답이 있을 법한 질문을 뽑았고 가령, 질문에 답을 못 찾는 경우의 데이터셋도 원래 데이터에 같이 포함한 버전

**그래서 지문에 답이 있는지 없는지부터 파악! -> 답이 있다면 SQuAD 1.0에서 하듯이 답을 찾는다.**

* 지문에 답이 있는지 없는지 판단하는 task는 문단과 질문을 종합해서 보고 판단해야하는 task이다!

      CLS token을 활용할 수 있다! -> 이것으로 하나의 sequence로 만듦
      
      주어진 질문과 지문 쌍에서 답이 실제로 없는 ground truth에서 "no answer"라고 표시된 경우 
      
      -> CLS token을 binary classification을 하는 output layer로 결과값이 "no answer"로 적절히 예측되도록 하는 Cross Entropy loss로 학습!
      
### BERT: On SWAG

다수 문장을 다루는 task에 BERT를 사용한 사례

![fff](https://user-images.githubusercontent.com/59636424/133203632-0b2f7a72-c85d-4318-9ea6-88a80c43bb1e.PNG)

* 주어진 문장으로 다음에 나타날법한 적절한 문장을 고르는 task 과정

      1. CLS token을 사용! 
      
      -> 예시의 4문장을 앞의 문장과 concat해서 BERT로 encoding -> 동일한 Fully connected layer를 통과해서 각 문장의 scalar 값을 구할 수 있다.
      
      2. 4개의 scalar 값을 softmax에 입력으로 넣어 ground truth에 해당하는 logits값이 100%로 나올 수 있게 학습!



