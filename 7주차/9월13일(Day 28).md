# 1. 9월 13일 배운 것!

## 1. Transformer

sequence 데이터를 attention만을 사용해서 sequence data를 예측한다.

### RNN: Long-term dependency

![ㄱㄱㄱ](https://user-images.githubusercontent.com/59636424/133071138-a279f11d-cd1b-4c50-8ce2-ff010248acb0.PNG)

멀리 있는 timestep의 정보를 손실 및 분실로 인해 일어날 수 있다.

### Bi-Directional RNNs

![ㅈㅈㅈㅈㅈ](https://user-images.githubusercontent.com/59636424/133071976-107d4f28-6fcb-45ff-8498-f6e8084eb0c2.PNG)

Backward RNN에서 h_3^d에서 go나 home과 같은 단어의 정보를 담을 수 있다.

