# 1. 9월 13일 배운 것!

## 1. Transformer

sequence 데이터를 attention만을 사용해서 sequence data를 예측한다.

### RNN: Long-term dependency

![ㄱㄱㄱ](https://user-images.githubusercontent.com/59636424/133071138-a279f11d-cd1b-4c50-8ce2-ff010248acb0.PNG)

멀리 있는 timestep의 정보를 손실 및 분실로 인해 일어날 수 있다.

### Bi-Directional RNNs

![ㅈㅈㅈㅈㅈ](https://user-images.githubusercontent.com/59636424/133071976-107d4f28-6fcb-45ff-8498-f6e8084eb0c2.PNG)

Backward RNN에서 h_3^d에서 go나 home과 같은 단어의 정보를 담을 수 있다.

-> 각 Forward RNN의 hidden state vector와 Backward RNN의 hidden state vector를 가져와서 concat시켜준다.(특정 timestep에)

-> concat이 되었으므로 2배의 차원의 vector를 go에 대한 encoding vector라고 할 수 있다.

### Transformer: Long-Term Dependency

![ㄷㄷ](https://user-images.githubusercontent.com/59636424/133091090-598199cf-5497-42c6-b460-87bfb327127d.PNG)

decoder의 hidden state vector라고 생각하면(I를!), encoder hidden state vector set는 I, go, home vector가 동일하게 사용!

    -> I(decoder hidden state vector)는 자신을 포함한 encoder hidden state vector와 내적한 후, softmax를 취한다.

    -> 이렇게 구한 것을 가중 평균을 구해서 I에 대한 encoding vector로 구할 수 있다!

**동일한 벡터 set 내에서 적용할 수 있다는 것에 self-attention**이라고 한다!

![ㅂㅈ](https://user-images.githubusercontent.com/59636424/133095289-02481443-5abe-4341-8774-51a2c4c1175b.PNG)

    -> 앞에서 말한 I가 Query vector라고 한다!
    
    -> Query와 내적이 되는 것들을 Key vector라고 한다!
    
    -> 가중 평균이 구해지는 vector들이 Values vector라고 한다!
    
**Key와 Value의 갯수는 같아야한다!**

![ㄱㄱ](https://user-images.githubusercontent.com/59636424/133097605-61acefea-722c-49a1-b5f3-e53b8c7ae520.PNG)

=> **이렇게 멀리 있던 정보를 손쉽게 가져와서 사용했으므로 long term dependency를 극복한다!**

---

Query와 Key의 내적으로 각 value 가중치를 구한다!

**Query와 Key는 같은 차원의 벡터여야 하는데 Value는 다른 차원이어도 된다!**

![ㄷㄷ](https://user-images.githubusercontent.com/59636424/133100219-1dde2000-cb4f-48a1-977f-53fea8b40588.PNG)

    Query와 Key가 3차원이고 Value가 5차원이 된다면 
    
    softmax를 거친 것으로 상수배를 해서 가중 평균을 내므로 Query와 Key는 Value의 벡터와 같지 않아도 된다!

![attt](https://user-images.githubusercontent.com/59636424/133100885-3e81a339-3921-4eb3-8399-12b7c22b6cac.PNG)

    분홍색 boundary는 softmax를 거친 확률값으로 i번째의 유사도값이다!
    
    => i번째 value vector i번째 확률값들과 곱해서 더하면 가중평균 값이다!
    
![xx](https://user-images.githubusercontent.com/59636424/133104066-81844a7c-0a14-450e-9e33-f8678de407df.PNG)

    Q는 Query 갯수, d_k는 하나의 Query의 차원
    
    K는 Key 갯수, d_k는 하나의 Key의 차원
    
    V는 Value 갯수, d_v는 하나의 Value의 차원
    
    1번째 query vector에 대한 attention module의 output vector로 오른쪽 행렬의 위에 빨간색 boundary이다!
    
### Transformer: Scaled Dot-Product Attention

![image](https://user-images.githubusercontent.com/59636424/133104722-2b289449-b1f3-4593-b71f-30ee8f5e97dd.png)

![softmax](https://user-images.githubusercontent.com/59636424/133107337-77b0f388-023d-40fb-bb48-60c7b1444770.jpg)

-> softmax로부터 큰 값이 몰릴 수 있다.(표준편차가 클수록)

-> 그래서 **루트 d_k로 나누므로 scaling을 해준다.** 
