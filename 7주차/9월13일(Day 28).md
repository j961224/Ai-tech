# 1. 9월 13일 배운 것!

## 1. Transformer

sequence 데이터를 attention만을 사용해서 sequence data를 예측한다.

### RNN: Long-term dependency

![ㄱㄱㄱ](https://user-images.githubusercontent.com/59636424/133071138-a279f11d-cd1b-4c50-8ce2-ff010248acb0.PNG)

멀리 있는 timestep의 정보를 손실 및 분실로 인해 일어날 수 있다.

### Bi-Directional RNNs

![ㅈㅈㅈㅈㅈ](https://user-images.githubusercontent.com/59636424/133071976-107d4f28-6fcb-45ff-8498-f6e8084eb0c2.PNG)

Backward RNN에서 h_3^d에서 go나 home과 같은 단어의 정보를 담을 수 있다.

-> 각 Forward RNN의 hidden state vector와 Backward RNN의 hidden state vector를 가져와서 concat시켜준다.(특정 timestep에)

-> concat이 되었으므로 2배의 차원의 vector를 go에 대한 encoding vector라고 할 수 있다.

### Transformer: Long-Term Dependency

![ㄷㄷ](https://user-images.githubusercontent.com/59636424/133091090-598199cf-5497-42c6-b460-87bfb327127d.PNG)

decoder의 hidden state vector라고 생각하면(I를!), encoder hidden state vector set는 I, go, home vector가 동일하게 사용!

    -> I(decoder hidden state vector)는 자신을 포함한 encoder hidden state vector와 내적한 후, softmax를 취한다.

    -> 이렇게 구한 것을 가중 평균을 구해서 I에 대한 encoding vector로 구할 수 있다!

**동일한 벡터 set 내에서 적용할 수 있다는 것에 self-attention**이라고 한다!

![ㅂㅈ](https://user-images.githubusercontent.com/59636424/133095289-02481443-5abe-4341-8774-51a2c4c1175b.PNG)

    -> 앞에서 말한 I가 Query vector라고 한다!
    
    -> Query와 내적이 되는 것들을 Key vector라고 한다!
    
    -> 가중 평균이 구해지는 vector들이 Values vector라고 한다!
    
**Key와 Value의 갯수는 같아야한다!**

![ㄱㄱ](https://user-images.githubusercontent.com/59636424/133097605-61acefea-722c-49a1-b5f3-e53b8c7ae520.PNG)

=> **이렇게 멀리 있던 정보를 손쉽게 가져와서 사용했으므로 long term dependency를 극복한다!**

---

Query와 Key의 내적으로 각 value 가중치를 구한다!

**Query와 Key는 같은 차원의 벡터여야 하는데 Value는 다른 차원이어도 된다!**

![ㄷㄷ](https://user-images.githubusercontent.com/59636424/133100219-1dde2000-cb4f-48a1-977f-53fea8b40588.PNG)

    Query와 Key가 3차원이고 Value가 5차원이 된다면 
    
    softmax를 거친 것으로 상수배를 해서 가중 평균을 내므로 Query와 Key는 Value의 벡터와 같지 않아도 된다!

![attt](https://user-images.githubusercontent.com/59636424/133100885-3e81a339-3921-4eb3-8399-12b7c22b6cac.PNG)

    분홍색 boundary는 softmax를 거친 확률값으로 i번째의 유사도값이다!
    
    => i번째 value vector i번째 확률값들과 곱해서 더하면 가중평균 값이다!
    
![xx](https://user-images.githubusercontent.com/59636424/133104066-81844a7c-0a14-450e-9e33-f8678de407df.PNG)

    Q는 Query 갯수, d_k는 하나의 Query의 차원
    
    K는 Key 갯수, d_k는 하나의 Key의 차원
    
    V는 Value 갯수, d_v는 하나의 Value의 차원
    
    1번째 query vector에 대한 attention module의 output vector로 오른쪽 행렬의 위에 빨간색 boundary이다!
    
### Transformer: Scaled Dot-Product Attention

![image](https://user-images.githubusercontent.com/59636424/133104722-2b289449-b1f3-4593-b71f-30ee8f5e97dd.png)

![softmax](https://user-images.githubusercontent.com/59636424/133107337-77b0f388-023d-40fb-bb48-60c7b1444770.jpg)

-> softmax로부터 큰 값이 몰릴 수 있다.(표준편차가 클수록)

-> 그래서 **루트 d_k로 나누므로 scaling을 해준다.** 


# 3. 실습 코드 분석(7. multi head attention)

우선, 6주차에 LSTM 등의 코드와 다른 부분을 위주로 설명하려 한다!

* multi head 갯수 설정

~~~
num_heads = 8  # head의 개수
~~~

* 각 input으로 linear transformation matrix을 통해 Query, Key, Value 설정

![xxxxxxx](https://user-images.githubusercontent.com/59636424/133111435-50a2a54c-4221-451d-894d-3c8281dbbab7.PNG)

~~~
w_q = nn.Linear(d_model, d_model)
w_k = nn.Linear(d_model, d_model)
w_v = nn.Linear(d_model, d_model)

q = w_q(batch_emb)  # (B, L, d_model) #실습에서는 (10,20,512)
k = w_k(batch_emb)  # (B, L, d_model)
v = w_v(batch_emb)  # (B, L, d_model)
~~~

* Query, Key, Value의 head 갯수만큼 차원 분할을 통한 여러 vector 생성

~~~
batch_size = q.shape[0] #10
d_k = d_model // num_heads

q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)
k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)
v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)

print(q.shape) # torch.Size([10, 20, 8, 64])
print(k.shape)
print(v.shape)

# head가 Length보다 앞쪽으로 오도록!

q = q.transpose(1, 2)  # (B, num_heads, L, d_k)
k = k.transpose(1, 2)  # (B, num_heads, L, d_k)
v = v.transpose(1, 2)  # (B, num_heads, L, d_k)

print(q.shape) #torch.Size([10, 8, 20, 64])
print(k.shape)
print(v.shape)
~~~

* Scaled dot-product self-attention 구현

![cccc](https://user-images.githubusercontent.com/59636424/133113435-8e5c2d34-546f-4ba5-863e-d7591511bd3a.PNG)

~~~
# k에서 d_k가 L보다 앞쪽으로 올 수 있게 한다!

attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)
attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)

print(attn_dists) 
print(attn_dists.shape) #torch.Size([10, 8, 20, 20])

attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)

print(attn_values.shape) #torch.Size([10, 8, 20, 64])
~~~

* 각 head의 결과물 병합

-> **각 head의 결과물을 concat하고 동일 차원으로 linear transformation한다!**

~~~
attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)
attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)

print(attn_values.shape) #torch.Size([10, 20, 512])
~~~

        contiguous()는 새로운 메모리 공간에 데이터를 복사해 주소값 연속성을 가변적으로 만듦
        
        연속적은 메모리 텐서를 반환하는 메서드!


* output

~~~
w_0 = nn.Linear(d_model, d_model) #linear transformation해주기
outputs = w_0(attn_values)

print(outputs)
print(outputs.shape) #torch.Size([10, 20, 512])
~~~



