# 1. 9월 13일 배운 것!

## 1. Transformer

sequence 데이터를 attention만을 사용해서 sequence data를 예측한다.

### RNN: Long-term dependency

![ㄱㄱㄱ](https://user-images.githubusercontent.com/59636424/133071138-a279f11d-cd1b-4c50-8ce2-ff010248acb0.PNG)

멀리 있는 timestep의 정보를 손실 및 분실로 인해 일어날 수 있다.

### Bi-Directional RNNs

![ㅈㅈㅈㅈㅈ](https://user-images.githubusercontent.com/59636424/133071976-107d4f28-6fcb-45ff-8498-f6e8084eb0c2.PNG)

Backward RNN에서 h_3^d에서 go나 home과 같은 단어의 정보를 담을 수 있다.

-> 각 Forward RNN의 hidden state vector와 Backward RNN의 hidden state vector를 가져와서 concat시켜준다.(특정 timestep에)

-> concat이 되었으므로 2배의 차원의 vector를 go에 대한 encoding vector라고 할 수 있다.

### Transformer: Long-Term Dependency

![ㄷㄷ](https://user-images.githubusercontent.com/59636424/133091090-598199cf-5497-42c6-b460-87bfb327127d.PNG)

decoder의 hidden state vector라고 생각하면(I를!), encoder hidden state vector set는 I, go, home vector가 동일하게 사용!

    -> I(decoder hidden state vector)는 자신을 포함한 encoder hidden state vector와 내적한 후, softmax를 취한다.

    -> 이렇게 구한 것을 가중 평균을 구해서 I에 대한 encoding vector로 구할 수 있다!

**동일한 벡터 set 내에서 적용할 수 있다는 것에 self-attention**이라고 한다!

![ㅂㅈ](https://user-images.githubusercontent.com/59636424/133095289-02481443-5abe-4341-8774-51a2c4c1175b.PNG)

    -> 앞에서 말한 I가 Query vector라고 한다!
    
    -> Query와 내적이 되는 것들을 Key vector라고 한다!
    
    -> 가중 평균이 구해지는 vector들이 Values vector라고 한다!
    
**Key와 Value의 갯수는 같아야한다!**

![ㄱㄱ](https://user-images.githubusercontent.com/59636424/133097605-61acefea-722c-49a1-b5f3-e53b8c7ae520.PNG)

=> **이렇게 멀리 있던 정보를 손쉽게 가져와서 사용했으므로 long term dependency를 극복한다!**

---

Query와 Key의 내적으로 각 value 가중치를 구한다!

**Query와 Key는 같은 차원의 벡터여야 하는데 Value는 다른 차원이어도 된다!**

![ㄷㄷ](https://user-images.githubusercontent.com/59636424/133100219-1dde2000-cb4f-48a1-977f-53fea8b40588.PNG)

    Query와 Key가 3차원이고 Value가 5차원이 된다면 
    
    softmax를 거친 것으로 상수배를 해서 가중 평균을 내므로 Query와 Key는 Value의 벡터와 같지 않아도 된다!

![attt](https://user-images.githubusercontent.com/59636424/133100885-3e81a339-3921-4eb3-8399-12b7c22b6cac.PNG)

    분홍색 boundary는 softmax를 거친 확률값으로 i번째의 유사도값이다!
    
    => i번째 value vector i번째 확률값들과 곱해서 더하면 가중평균 값이다!
    
![xx](https://user-images.githubusercontent.com/59636424/133104066-81844a7c-0a14-450e-9e33-f8678de407df.PNG)

    Q는 Query 갯수, d_k는 하나의 Query의 차원
    
    K는 Key 갯수, d_k는 하나의 Key의 차원
    
    V는 Value 갯수, d_v는 하나의 Value의 차원
    
    1번째 query vector에 대한 attention module의 output vector로 오른쪽 행렬의 위에 빨간색 boundary이다!
    
### Transformer: Scaled Dot-Product Attention

![image](https://user-images.githubusercontent.com/59636424/133104722-2b289449-b1f3-4593-b71f-30ee8f5e97dd.png)

![softmax](https://user-images.githubusercontent.com/59636424/133107337-77b0f388-023d-40fb-bb48-60c7b1444770.jpg)

-> softmax로부터 큰 값이 몰릴 수 있다.(표준편차가 클수록)

-> 그래서 **루트 d_k로 나누므로 scaling을 해준다.** 

### Transformer: Multi-Head Attention

![wwwww](https://user-images.githubusercontent.com/59636424/133175660-8f4f433f-951a-48b8-ae36-7fa3871e23c8.PNG)

병렬적으로 여러 버전의 attention을 수행한다!

=> 8은 head의 개수!!

### Complexity

![eee](https://user-images.githubusercontent.com/59636424/133178514-9788eaee-94d0-4531-b287-f551f23c2d37.PNG)

* self-attention의 계산량

Complexity per Layer -> Q x K^T의 부분에 해당! -> (n x d) x (d x n) -> d만큼의 곱셈이 이뤄지므로 n^2 x d가 이뤄진다!

Sequential Operations -> 행렬 연산의 병렬화로 O(1)으로 1번에 계산 가능

Maximum path Length -> 1번에 가지고 옴!


* RNN의 계산량

Complexity per Layer -> 각 timestep에서 계산할 때, d x d의 W_hh(linear transformation matrix)와 이전 hidden state vector h_(t-1)과 계산한다!

        매 timestep마다 순차적으로 계산하므로 timestep의 갯수 n과 각 timestep의 d^2만큼 계산하므로 n x d^2 이다!
        
        n은 길면 길수록 임의로 정할 수 없는 값이다!, d는 정할 수 있다!!
        
        앞에서의 self-attention의 복잡도는 RNN보다 더 많은 메모리가 요구된다!

Sequential Operations -> 각 timestep의 흐름에 따라 가야하므로 병렬화 불가! -> O(n)

Maximum path Length(LongTerm Dependency와 연관!) -> 순차적으로 가져옴!

### Transformer: Block-Based Model

![tttt](https://user-images.githubusercontent.com/59636424/133179378-56d5995e-8f84-4fac-81a5-9be91575a715.PNG)

Multi-Head-Attention -> Add & Norm( LayerNorm(x+sublayer(x)) ) -> feed forward -> Add & Norm( LayerNorm(x+sublayer(x)) )

* 입력 vector와 출력 vector가 같아야 residual connection이 가능!

* feed forward: Fully connected layer

### Layer normalize

![normalize](https://user-images.githubusercontent.com/59636424/133180537-af192c15-5661-48ba-be27-d598125c428d.PNG)

각 word별로 node들의 값을 모아서 평균 0, 분산 1로 만들어준다!

원하는 평균, 분산을 넣기 위해서 y=ax+b 연산 수행 시, 각각의 a,b를 각 node별로 여러 단어에 걸쳐서 공통적인 변환을 한다!!

### Positional Encoding

![rrrrr](https://user-images.githubusercontent.com/59636424/133181514-3f404491-c5a8-46b0-88bd-ec7836bdc996.PNG)

I의 입력벡터는 첫 번째 위치에 등장함을 vector에 정보를 포함시켜준다!

![tttt](https://user-images.githubusercontent.com/59636424/133181509-2e840e7b-d3d4-4303-9162-94d890d033ff.PNG)

첫번째를 0번째 index라고 생각한다면, 위치 idex가 짝수일 때, sin, 홀수일 때, cos이 나타난다.

**dimension에 따라 frequency가 줄어듦(sin/cos의 주기가 길어짐)**

**위치별로 다른 vector가 더해지도록 한다!!**

### learning rate

![ccccc](https://user-images.githubusercontent.com/59636424/133182231-1e26cd6a-2fd9-4bb8-bf66-d6be406f9ca0.PNG)

learning rate를 증가시키다 최저점에 가까워지면 learning rate를 점점 낮추어 최저점에 도달할 수 있도록 한다!!!!

### self attention으로부터 encoding한 후의 시각화

![gngn](https://user-images.githubusercontent.com/59636424/133182634-9198f40e-0102-4739-8bfe-f14d1211c775.PNG)

making이라는 단어는 2009, making, more, difficult의 정보를 가져가는 것으로 보인다!!

### Decoder

![x](https://user-images.githubusercontent.com/59636424/133183183-6c774280-68f4-4448-bf06-9d1eaa24cfe8.PNG)

### Masked self-attention

![zzzz](https://user-images.githubusercontent.com/59636424/133183429-b36feac0-41d0-4790-bd14-b0ead35b5ef4.PNG)

SOS에서 '나는'이라는 단어를 예측할 시, SOS가 '나는'과 '집에'라는 단어와의 유사도를 몰라야 하고 '나는'은 '집에'라는 단어 유사도를 몰라야 영향을 받지 않고 예측이 가능하다. 



# 3. 실습 코드 분석

## 7. multi head attention

우선, 6주차에 LSTM 등의 코드와 다른 부분을 위주로 설명하려 한다!

* multi head 갯수 설정

~~~
num_heads = 8  # head의 개수
~~~

* 각 input으로 linear transformation matrix을 통해 Query, Key, Value 설정

![xxxxxxx](https://user-images.githubusercontent.com/59636424/133111435-50a2a54c-4221-451d-894d-3c8281dbbab7.PNG)

~~~
w_q = nn.Linear(d_model, d_model)
w_k = nn.Linear(d_model, d_model)
w_v = nn.Linear(d_model, d_model)

q = w_q(batch_emb)  # (B, L, d_model) #실습에서는 (10,20,512)
k = w_k(batch_emb)  # (B, L, d_model)
v = w_v(batch_emb)  # (B, L, d_model)
~~~

* Query, Key, Value의 head 갯수만큼 차원 분할을 통한 여러 vector 생성

~~~
batch_size = q.shape[0] #10
d_k = d_model // num_heads

q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)
k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)
v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)

print(q.shape) # torch.Size([10, 20, 8, 64])
print(k.shape)
print(v.shape)

# head가 Length보다 앞쪽으로 오도록!

q = q.transpose(1, 2)  # (B, num_heads, L, d_k)
k = k.transpose(1, 2)  # (B, num_heads, L, d_k)
v = v.transpose(1, 2)  # (B, num_heads, L, d_k)

print(q.shape) #torch.Size([10, 8, 20, 64])
print(k.shape)
print(v.shape)
~~~

* Scaled dot-product self-attention 구현

![cccc](https://user-images.githubusercontent.com/59636424/133113435-8e5c2d34-546f-4ba5-863e-d7591511bd3a.PNG)

~~~
# k에서 d_k가 L보다 앞쪽으로 올 수 있게 한다!

attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)
attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)

print(attn_dists) 
print(attn_dists.shape) #torch.Size([10, 8, 20, 20])

attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)

print(attn_values.shape) #torch.Size([10, 8, 20, 64])
~~~

* 각 head의 결과물 병합

-> **각 head의 결과물을 concat하고 동일 차원으로 linear transformation한다!**

~~~
attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)
attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)

print(attn_values.shape) #torch.Size([10, 20, 512])
~~~

        contiguous()는 새로운 메모리 공간에 데이터를 복사해 주소값 연속성을 가변적으로 만듦
        
        연속적은 메모리 텐서를 반환하는 메서드!
        
-> 각 head들로 따로 연산한 것을 contiguous()와 view로 concat 시키기!


* output

~~~
w_0 = nn.Linear(d_model, d_model) #linear transformation해주기
outputs = w_0(attn_values)

print(outputs)
print(outputs.shape) #torch.Size([10, 20, 512])
~~~

## 8. Masked Multi-head Attention

우선, 이 실습 코드 전까지 겹치는 부분은 제외하고 바뀐 부분이자 핵심만 설명하려 한다.

* Mask 구축

~~~
# batch는 padding한 데이터이다. pad_id는 0으로 padding한 부분을 뜻한다.
# batch의 shape (5,10)
padding_mask = (batch != pad_id).unsqueeze(1)  # (B, 1, L)

print(padding_mask) # padding한 부분은 False로 안 한 부분은 True로 변환된다!
print(padding_mask.shape) # (5,1,10)
~~~

-> padding 한 부분은 False, 안 한 부분은 True로 만들기(padding한 데이터에서!)

~~~
# 대각선으로 masking할 행렬을 만든다.
# trill은 대각선 기준으로 오른쪽 위쪽부분을 0으로 만들어주는데 data type이 bool이므로 False로 만들어준다.
nopeak_mask = torch.ones([1, max_len, max_len], dtype=torch.bool)  # (1, L, L)
nopeak_mask = torch.tril(nopeak_mask)  # (1, L, L)

print(nopeak_mask)
print(nopeak_mask.shape) #(1,10,10)
~~~

-> 데이터를 masking 하기 위한 대각선 기준으로 오른쪽을 False로 만든 행렬

-> 이 행렬은 데이터에서 한 단어단어와 비교할 것이다!

~~~
mask = padding_mask & nopeak_mask  # (B, L, L) (5,10,10)

print(mask)
print(mask.shape) #(5,10,10)
~~~

-> padding_mask.shape은 (5,1,10)이고 nopeak_mask.shape은 (1,10,10)이다.

-> 그래서 padding_mask에서 행의 기준으로 한 줄씩(단어 하나씩)을 가지고 nopeak_mask로 masking 해준다!!

---

* Maksing이 적용된 self-attention 구현

~~~
# attention scores.shape = (5,2,10,10)

inf = 1e12
masks = mask.unsqueeze(1)  # (B, 1, L, L)

# masked_fill_ method로 masks 행렬에서 False인 부분은 -1 x inf로 채운다! -> 각 Batch 위치에 맞게!!
masked_attn_scores = attn_scores.masked_fill_(masks == False, -1 * inf)  # (B, num_heads, L, L) (5,2,10,10)

print(masked_attn_scores)
print(masked_attn_scores.shape) # (5,2,10,10)
~~~

-> softmax에 들어가기 전에, 각 batch에 맞게 mask 행렬로 masking을 해준다!

~~~
attn_dists = F.softmax(masked_attn_scores, dim=-1)  # (B, num_heads, L, L) (5,2,10,10)

print(attn_dists)
print(attn_dists.shape)
~~~

-> -1 x inf였던 부분은 softmax를 거친 뒤에 0이 된다!

* **Encoder-Decoder attention**

**Masked multi-head attention의 결과인 Query와 encoder에서 나온 Key와 Value로 multi-head attention을 수행하는 과정이다.**

![xcxcxcxcxc](https://user-images.githubusercontent.com/59636424/133187882-199f1ff2-adb7-4b56-8718-a5de88b34c4d.PNG)

Masked multi-head attention에서는 masked한 SOS, 나는, 집에 embedding vector를 decoder의 input으로 넣는다.(1방에 넣는 것 같다.)

만약, 영어->한글 번역이면 'I go home' 영어 문장과 'sos 나는 집에'까지의 최고 길이가 다를 수 있으므로 encoder output과 masked output과 사이즈가 조금 다르다.

~~~
q = w_q(trg_emb)  # (B, T_L, d_model)
k = w_k(src_emb)  # (B, S_L, d_model)
v = w_v(src_emb)  # (B, S_L, d_model)

batch_size = q.shape[0]
d_k = d_model // num_heads

q = q.view(batch_size, -1, num_heads, d_k)  # (B, T_L, num_heads, d_k)
k = k.view(batch_size, -1, num_heads, d_k)  # (B, S_L, num_heads, d_k)
v = v.view(batch_size, -1, num_heads, d_k)  # (B, S_L, num_heads, d_k)

q = q.transpose(1, 2)  # (B, num_heads, T_L, d_k)
k = k.transpose(1, 2)  # (B, num_heads, S_L, d_k)
v = v.transpose(1, 2)  # (B, num_heads, S_L, d_k)

print(q.shape) # torch.Size([5, 2, 12, 4])
print(k.shape) # torch.Size([5, 2, 10, 4])
print(v.shape) # torch.Size([5, 2, 10, 4])
~~~

-> 그 이후에는 다 똑같다!

