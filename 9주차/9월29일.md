# 9월29일 배운 것!

## (5강) BERT 기반 단일 문장 분류 모델 학습

### 1. KLUE 데이터셋 소개!

한국어 자연어 이해 벤치마크!

-> 문장 분류, 관계 추출(단일 문장 분류 task)

-> 문장 유사도(문장 임베딩 벡터의 유사도)

-> 자연어 추론(두 문장 관계 task)

-> 개체명 인식, 품사 태깅, 질의 응답 (문장 토큰 분류 task)

-> 목적형 대화 (DST)

-> 의존 구문 분석

#### 의존 구문 분석!

단어들 사이의 관계를 분석!

'의존소'와 '지배소'로 나뉨 -> '지배소'는 의미에 중심이 되는 존재, '의존소'는 지배소가 갖는 의미를 보완!


* 지배소: 후위언어로 의존소보다 뒤에 위치!!
* 의존소: 지배소는 1개이다!

---

* 분류 방법!

![ww](https://user-images.githubusercontent.com/59636424/135104431-c3537619-dac0-46db-88de-287a58d08e17.PNG)

* **어따 써요?**

복잡한 자연어 형태를 그래프로 구조화해서 표현 가능!! -> 각 대상에 대한 정보 추출이 가능!!

#### 단일 문장 분류 task

주어진 문장이 어떤 종류의 범주에 속하는가?!

* 감정분석: 혐오 발언 분류
* 주제 라벨링: 주어진 문장이 어느 범주에 속하는지
* 언어 감지: 입력된 문장이 어떤 나라 언어인지
* 의도 분류: 챗봇이 문장 의도 파악

---

**Kor_hate**

: 혐오 표현에 대한 데이터, 공격적 문장 등등

**Kor_sarcasm**

: 비꼬았는지, 비꼬지 않은 표현의 문장인지

**Kor_sae**

: 질문 유형 분류

**Kor_3i4k**

: 질문, 명령문 등등 -> 문장 분류

#### 모델 구조도

![qq](https://user-images.githubusercontent.com/59636424/135106437-c4e23400-4f5d-4a12-97ad-be72b826095d.PNG)

single sentence -> 긍/부정 나누기!

* 주요 매개변수

![ww](https://user-images.githubusercontent.com/59636424/135106668-cfb2602b-6f6a-4b6d-a51f-aa157155b67d.PNG)

## (6강) BERT 기반 두 문장 관계 분류 모델 학습

### 두 문장 관계 분류 task 소개

![eee](https://user-images.githubusercontent.com/59636424/135114154-7839ea4b-5d83-4d41-b0ee-705af2224ab3.PNG)

* 데이터

NLI(Natural Language inference)

: 언어 모델이 자연어의 맥락을 이해할 수 있는가? -> 전제문장과 가설문장을 모순, 중립, 함의 분류!

Semantic text pair

: 두 문장의 의미가 서로 같은 문장인지?

### 두 문장 관계 분류 모델 학습

#### IRQA

![wwwwww](https://user-images.githubusercontent.com/59636424/135115317-a9f2f632-4165-4777-9724-e9839d6e0c54.PNG)

사전에 이미 정한 Q - A set에서 가장 적절한 답변을 찾기!

-> Paraphrase Detection으로 진짜로 유사한 의미를 가지는지 filtering!

## (7강) BERT 언어모델 기반의 문장 토큰 분류

### 1. 문장 토큰 분류 task

![we](https://user-images.githubusercontent.com/59636424/135124734-cb0d80d1-65a9-40c7-964e-caf2e70c1196.PNG)

주어진 문장의 각 token이 어떤 범주에 속하는지 분류하는 task

#### NER

개체명 인식 -> 문맥 파악해서 특정한 의미를 가지는 단어 인식 과정!

**같은 단어라도 문맥에서 다양한 개체로 사용!**

#### POS Tagging

주어진 문맥 파악한 후, tagging!

### 2. 문장 token 분류를 위한 데이터

#### kor_ner

NR 데이터셋은 pos tagging도 함께 존재!

=> 개체명 인식이 가능한 코드 활용 -> pos tagging이 가능한 모듈도 학습 가능!

**BIO Tag로 개체명 인식!**

* BIO란?

B: Begin, I: Inner, O: out

### 2-1. 문장 토큰 분류 모델 학습 실습!

![weq](https://user-images.githubusercontent.com/59636424/135126541-6d52227f-026e-46f6-a50f-9182b63995c3.PNG)

**형태소 단위의 토큰을 음절 단위로 토큰 분해!!**

-> 이유는 tokenizer 문제(bert는 wordpiece -> wordpiece로 올바르게 자르지 못 하면 개체명도 error)




# 3. 실습!

## (5강) BERT 기반 단일 문장 분류 모델 학습 - 0_단일문장분류

* NSMC 데이터셋 받기!

~~~
dataset = datasets.load_dataset('nsmc') # nsmc, hate, sarcasm
~~~

* tokenizer로 토큰화 시키기

~~~
tokenized_test_sentences = tokenizer(
    list(test_data['document']),
    return_tensors="pt",
    padding=True,
    truncation=True,
    add_special_tokens=True,
    )
~~~

* Dataset

~~~
class SingleSentDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} # encoding한 data를 가져와서 사용!
        item['labels'] = torch.tensor(self.labels[idx]) # label index를 가져옴!
        return item

    def __len__(self):
        return len(self.labels)
~~~

* train & test dataset 만들기

~~~
train_dataset = SingleSentDataset(tokenized_train_sentences, train_label)
test_dataset = SingleSentDataset(tokenized_test_sentences, test_label)
~~~

* Train Argument 만들기

~~~
training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=1,              # total number of training epochs
    per_device_train_batch_size=32,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=500,
    save_steps=500,
    save_total_limit=2
)
~~~

* 모델 및 Trainer 설정

~~~
model = BertForSequenceClassification.from_pretrained(MODEL_NAME)
model.to(device)

trainer = Trainer(
    model=model,                         # the instantiated 🤗 Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
)
~~~

## (6강) BERT 기반 두 문장 관계 분류 모델 학습 - 0_학습_데이터_구축

## (7강) BERT 기반 문장 토큰 분류 모델 학습 - 0_문장_토큰_단위_학습

~~~
for tag in list(tag2id.keys()) : 
    print('{:>6} : {:>7,}'. format(tag, globals()[tag]))

 I-NOH :  23,967
 I-MNY :   6,930
 I-LOC :  16,537
 B-TIM :     371
 I-PNT :   4,613
 I-DAT :  14,433
 B-DAT :   5,383
 B-PER :  13,779
 I-POH :  37,156
 I-DUR :   4,573
 B-ORG :  13,089
 B-LOC :   6,313
 B-MNY :   1,440
     O : 983,746
 I-PER :  26,206
 I-ORG :  41,320
 B-POH :   6,686
 I-TIM :   1,876
 B-NOH :  11,051
 B-DUR :   1,207
 B-PNT :   1,672
 ~~~
 
 -> tag마다 적은 수는 성능이 떨어진다. -> 해당 tag dataset 추가!
 
 * 음절로 자르면 -> tokenizer에서 UNK로 되는것이 아닌가?

-> "bert-base-multilingual-cased"에서 vocab이 8000개이고 음절단위이므로 vocab id 어느정도 가능!

* **BertForTokenClassification**

token마다 classifcation label가 부착 -> 해당 token이 어떤 label값인지 분류 -> BertForTokenClassification 를 제공하니 이걸 사용하며 동일 진행 가능!

~~~
model = BertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(unique_tags))
~~~

## (7강) BERT 기반 문장 토큰 분류 모델 학습 - 1_기계_독해_학습


~~~
print(train_answers[index])

{'text':'Low', 'answer_start': 568}
~~~

-> 568번째 음절에 answer가 시작!

* add_end_idx method

모델은 answer start position, answer end position을 받는다!

-> 그래서 answer start와 answer end 추가!

* **add_token_positions method**

음절 단위의 숫자를 token index로 바꿔줘야한다! (char_to_token: 음절 숫자를 token index로 변환)

---

BertForQuestionAnswering class를 사용해서 기계 독해!
