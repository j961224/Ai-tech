# 9ì›”29ì¼ ë°°ìš´ ê²ƒ!

## (5ê°•) BERT ê¸°ë°˜ ë‹¨ì¼ ë¬¸ì¥ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ

### 1. KLUE ë°ì´í„°ì…‹ ì†Œê°œ!

í•œêµ­ì–´ ìì—°ì–´ ì´í•´ ë²¤ì¹˜ë§ˆí¬!

-> ë¬¸ì¥ ë¶„ë¥˜, ê´€ê³„ ì¶”ì¶œ(ë‹¨ì¼ ë¬¸ì¥ ë¶„ë¥˜ task)

-> ë¬¸ì¥ ìœ ì‚¬ë„(ë¬¸ì¥ ì„ë² ë”© ë²¡í„°ì˜ ìœ ì‚¬ë„)

-> ìì—°ì–´ ì¶”ë¡ (ë‘ ë¬¸ì¥ ê´€ê³„ task)

-> ê°œì²´ëª… ì¸ì‹, í’ˆì‚¬ íƒœê¹…, ì§ˆì˜ ì‘ë‹µ (ë¬¸ì¥ í† í° ë¶„ë¥˜ task)

-> ëª©ì í˜• ëŒ€í™” (DST)

-> ì˜ì¡´ êµ¬ë¬¸ ë¶„ì„

#### ì˜ì¡´ êµ¬ë¬¸ ë¶„ì„!

ë‹¨ì–´ë“¤ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ë¶„ì„!

'ì˜ì¡´ì†Œ'ì™€ 'ì§€ë°°ì†Œ'ë¡œ ë‚˜ë‰¨ -> 'ì§€ë°°ì†Œ'ëŠ” ì˜ë¯¸ì— ì¤‘ì‹¬ì´ ë˜ëŠ” ì¡´ì¬, 'ì˜ì¡´ì†Œ'ëŠ” ì§€ë°°ì†Œê°€ ê°–ëŠ” ì˜ë¯¸ë¥¼ ë³´ì™„!


* ì§€ë°°ì†Œ: í›„ìœ„ì–¸ì–´ë¡œ ì˜ì¡´ì†Œë³´ë‹¤ ë’¤ì— ìœ„ì¹˜!!
* ì˜ì¡´ì†Œ: ì§€ë°°ì†ŒëŠ” 1ê°œì´ë‹¤!

---

* ë¶„ë¥˜ ë°©ë²•!

![ww](https://user-images.githubusercontent.com/59636424/135104431-c3537619-dac0-46db-88de-287a58d08e17.PNG)

* **ì–´ë”° ì¨ìš”?**

ë³µì¡í•œ ìì—°ì–´ í˜•íƒœë¥¼ ê·¸ë˜í”„ë¡œ êµ¬ì¡°í™”í•´ì„œ í‘œí˜„ ê°€ëŠ¥!! -> ê° ëŒ€ìƒì— ëŒ€í•œ ì •ë³´ ì¶”ì¶œì´ ê°€ëŠ¥!!

#### ë‹¨ì¼ ë¬¸ì¥ ë¶„ë¥˜ task

ì£¼ì–´ì§„ ë¬¸ì¥ì´ ì–´ë–¤ ì¢…ë¥˜ì˜ ë²”ì£¼ì— ì†í•˜ëŠ”ê°€?!

* ê°ì •ë¶„ì„: í˜ì˜¤ ë°œì–¸ ë¶„ë¥˜
* ì£¼ì œ ë¼ë²¨ë§: ì£¼ì–´ì§„ ë¬¸ì¥ì´ ì–´ëŠ ë²”ì£¼ì— ì†í•˜ëŠ”ì§€
* ì–¸ì–´ ê°ì§€: ì…ë ¥ëœ ë¬¸ì¥ì´ ì–´ë–¤ ë‚˜ë¼ ì–¸ì–´ì¸ì§€
* ì˜ë„ ë¶„ë¥˜: ì±—ë´‡ì´ ë¬¸ì¥ ì˜ë„ íŒŒì•…

---

**Kor_hate**

: í˜ì˜¤ í‘œí˜„ì— ëŒ€í•œ ë°ì´í„°, ê³µê²©ì  ë¬¸ì¥ ë“±ë“±

**Kor_sarcasm**

: ë¹„ê¼¬ì•˜ëŠ”ì§€, ë¹„ê¼¬ì§€ ì•Šì€ í‘œí˜„ì˜ ë¬¸ì¥ì¸ì§€

**Kor_sae**

: ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜

**Kor_3i4k**

: ì§ˆë¬¸, ëª…ë ¹ë¬¸ ë“±ë“± -> ë¬¸ì¥ ë¶„ë¥˜

#### ëª¨ë¸ êµ¬ì¡°ë„

![qq](https://user-images.githubusercontent.com/59636424/135106437-c4e23400-4f5d-4a12-97ad-be72b826095d.PNG)

single sentence -> ê¸/ë¶€ì • ë‚˜ëˆ„ê¸°!

* ì£¼ìš” ë§¤ê°œë³€ìˆ˜

![ww](https://user-images.githubusercontent.com/59636424/135106668-cfb2602b-6f6a-4b6d-a51f-aa157155b67d.PNG)

## (6ê°•) BERT ê¸°ë°˜ ë‘ ë¬¸ì¥ ê´€ê³„ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ

### ë‘ ë¬¸ì¥ ê´€ê³„ ë¶„ë¥˜ task ì†Œê°œ

![eee](https://user-images.githubusercontent.com/59636424/135114154-7839ea4b-5d83-4d41-b0ee-705af2224ab3.PNG)

* ë°ì´í„°

NLI(Natural Language inference)

: ì–¸ì–´ ëª¨ë¸ì´ ìì—°ì–´ì˜ ë§¥ë½ì„ ì´í•´í•  ìˆ˜ ìˆëŠ”ê°€? -> ì „ì œë¬¸ì¥ê³¼ ê°€ì„¤ë¬¸ì¥ì„ ëª¨ìˆœ, ì¤‘ë¦½, í•¨ì˜ ë¶„ë¥˜!

Semantic text pair

: ë‘ ë¬¸ì¥ì˜ ì˜ë¯¸ê°€ ì„œë¡œ ê°™ì€ ë¬¸ì¥ì¸ì§€?

### ë‘ ë¬¸ì¥ ê´€ê³„ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ

#### IRQA

![wwwwww](https://user-images.githubusercontent.com/59636424/135115317-a9f2f632-4165-4777-9724-e9839d6e0c54.PNG)

ì‚¬ì „ì— ì´ë¯¸ ì •í•œ Q - A setì—ì„œ ê°€ì¥ ì ì ˆí•œ ë‹µë³€ì„ ì°¾ê¸°!

-> Paraphrase Detectionìœ¼ë¡œ ì§„ì§œë¡œ ìœ ì‚¬í•œ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ”ì§€ filtering!

## (7ê°•) BERT ì–¸ì–´ëª¨ë¸ ê¸°ë°˜ì˜ ë¬¸ì¥ í† í° ë¶„ë¥˜

### 1. ë¬¸ì¥ í† í° ë¶„ë¥˜ task

![we](https://user-images.githubusercontent.com/59636424/135124734-cb0d80d1-65a9-40c7-964e-caf2e70c1196.PNG)

ì£¼ì–´ì§„ ë¬¸ì¥ì˜ ê° tokenì´ ì–´ë–¤ ë²”ì£¼ì— ì†í•˜ëŠ”ì§€ ë¶„ë¥˜í•˜ëŠ” task

#### NER

ê°œì²´ëª… ì¸ì‹ -> ë¬¸ë§¥ íŒŒì•…í•´ì„œ íŠ¹ì •í•œ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ” ë‹¨ì–´ ì¸ì‹ ê³¼ì •!

**ê°™ì€ ë‹¨ì–´ë¼ë„ ë¬¸ë§¥ì—ì„œ ë‹¤ì–‘í•œ ê°œì²´ë¡œ ì‚¬ìš©!**

#### POS Tagging

ì£¼ì–´ì§„ ë¬¸ë§¥ íŒŒì•…í•œ í›„, tagging!

### 2. ë¬¸ì¥ token ë¶„ë¥˜ë¥¼ ìœ„í•œ ë°ì´í„°

#### kor_ner

NR ë°ì´í„°ì…‹ì€ pos taggingë„ í•¨ê»˜ ì¡´ì¬!

=> ê°œì²´ëª… ì¸ì‹ì´ ê°€ëŠ¥í•œ ì½”ë“œ í™œìš© -> pos taggingì´ ê°€ëŠ¥í•œ ëª¨ë“ˆë„ í•™ìŠµ ê°€ëŠ¥!

**BIO Tagë¡œ ê°œì²´ëª… ì¸ì‹!**

* BIOë€?

B: Begin, I: Inner, O: out

### 2-1. ë¬¸ì¥ í† í° ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì‹¤ìŠµ!

![weq](https://user-images.githubusercontent.com/59636424/135126541-6d52227f-026e-46f6-a50f-9182b63995c3.PNG)

**í˜•íƒœì†Œ ë‹¨ìœ„ì˜ í† í°ì„ ìŒì ˆ ë‹¨ìœ„ë¡œ í† í° ë¶„í•´!!**

-> ì´ìœ ëŠ” tokenizer ë¬¸ì œ(bertëŠ” wordpiece -> wordpieceë¡œ ì˜¬ë°”ë¥´ê²Œ ìë¥´ì§€ ëª» í•˜ë©´ ê°œì²´ëª…ë„ error)




# 3. ì‹¤ìŠµ!

## (5ê°•) BERT ê¸°ë°˜ ë‹¨ì¼ ë¬¸ì¥ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ - 0_ë‹¨ì¼ë¬¸ì¥ë¶„ë¥˜

* NSMC ë°ì´í„°ì…‹ ë°›ê¸°!

~~~
dataset = datasets.load_dataset('nsmc') # nsmc, hate, sarcasm
~~~

* tokenizerë¡œ í† í°í™” ì‹œí‚¤ê¸°

~~~
tokenized_test_sentences = tokenizer(
    list(test_data['document']),
    return_tensors="pt",
    padding=True,
    truncation=True,
    add_special_tokens=True,
    )
~~~

* Dataset

~~~
class SingleSentDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} # encodingí•œ dataë¥¼ ê°€ì ¸ì™€ì„œ ì‚¬ìš©!
        item['labels'] = torch.tensor(self.labels[idx]) # label indexë¥¼ ê°€ì ¸ì˜´!
        return item

    def __len__(self):
        return len(self.labels)
~~~

* train & test dataset ë§Œë“¤ê¸°

~~~
train_dataset = SingleSentDataset(tokenized_train_sentences, train_label)
test_dataset = SingleSentDataset(tokenized_test_sentences, test_label)
~~~

* Train Argument ë§Œë“¤ê¸°

~~~
training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=1,              # total number of training epochs
    per_device_train_batch_size=32,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=500,
    save_steps=500,
    save_total_limit=2
)
~~~

* ëª¨ë¸ ë° Trainer ì„¤ì •

~~~
model = BertForSequenceClassification.from_pretrained(MODEL_NAME)
model.to(device)

trainer = Trainer(
    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
)
~~~

## (6ê°•) BERT ê¸°ë°˜ ë‘ ë¬¸ì¥ ê´€ê³„ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ - 0_í•™ìŠµ_ë°ì´í„°_êµ¬ì¶•

## (7ê°•) BERT ê¸°ë°˜ ë¬¸ì¥ í† í° ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ - 0_ë¬¸ì¥_í† í°_ë‹¨ìœ„_í•™ìŠµ

~~~
for tag in list(tag2id.keys()) : 
    print('{:>6} : {:>7,}'. format(tag, globals()[tag]))

 I-NOH :  23,967
 I-MNY :   6,930
 I-LOC :  16,537
 B-TIM :     371
 I-PNT :   4,613
 I-DAT :  14,433
 B-DAT :   5,383
 B-PER :  13,779
 I-POH :  37,156
 I-DUR :   4,573
 B-ORG :  13,089
 B-LOC :   6,313
 B-MNY :   1,440
     O : 983,746
 I-PER :  26,206
 I-ORG :  41,320
 B-POH :   6,686
 I-TIM :   1,876
 B-NOH :  11,051
 B-DUR :   1,207
 B-PNT :   1,672
 ~~~
 
 -> tagë§ˆë‹¤ ì ì€ ìˆ˜ëŠ” ì„±ëŠ¥ì´ ë–¨ì–´ì§„ë‹¤. -> í•´ë‹¹ tag dataset ì¶”ê°€!
 
 * ìŒì ˆë¡œ ìë¥´ë©´ -> tokenizerì—ì„œ UNKë¡œ ë˜ëŠ”ê²ƒì´ ì•„ë‹Œê°€?

-> "bert-base-multilingual-cased"ì—ì„œ vocabì´ 8000ê°œì´ê³  ìŒì ˆë‹¨ìœ„ì´ë¯€ë¡œ vocab id ì–´ëŠì •ë„ ê°€ëŠ¥!

* **BertForTokenClassification**

tokenë§ˆë‹¤ classifcation labelê°€ ë¶€ì°© -> í•´ë‹¹ tokenì´ ì–´ë–¤ labelê°’ì¸ì§€ ë¶„ë¥˜ -> BertForTokenClassification ë¥¼ ì œê³µí•˜ë‹ˆ ì´ê±¸ ì‚¬ìš©í•˜ë©° ë™ì¼ ì§„í–‰ ê°€ëŠ¥!

~~~
model = BertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(unique_tags))
~~~

## (7ê°•) BERT ê¸°ë°˜ ë¬¸ì¥ í† í° ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ - 1_ê¸°ê³„_ë…í•´_í•™ìŠµ


~~~
print(train_answers[index])

{'text':'Low', 'answer_start': 568}
~~~

-> 568ë²ˆì§¸ ìŒì ˆì— answerê°€ ì‹œì‘!

* add_end_idx method

ëª¨ë¸ì€ answer start position, answer end positionì„ ë°›ëŠ”ë‹¤!

-> ê·¸ë˜ì„œ answer startì™€ answer end ì¶”ê°€!

* **add_token_positions method**

ìŒì ˆ ë‹¨ìœ„ì˜ ìˆ«ìë¥¼ token indexë¡œ ë°”ê¿”ì¤˜ì•¼í•œë‹¤! (char_to_token: ìŒì ˆ ìˆ«ìë¥¼ token indexë¡œ ë³€í™˜)

---

BertForQuestionAnswering classë¥¼ ì‚¬ìš©í•´ì„œ ê¸°ê³„ ë…í•´!
