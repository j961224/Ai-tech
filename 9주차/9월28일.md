# 9월 28일 배운 것!

## (3강) BERT 언어모델 소개

### 1. BERT 모델 소개

![xx](https://user-images.githubusercontent.com/59636424/134952636-d42f878d-8239-4527-a124-d1df68885320.PNG)

Mask를 함으로써, 더 어려운 문제로 해결!!

![wwww](https://user-images.githubusercontent.com/59636424/134952884-e827da3d-2061-4125-b91d-4db0f7f06282.PNG)

sentence 2개를 입력받음!

transformer 12 layer로 이뤄져있음!

#### BERT 학습 Corpus 데이터 & tokenizing

BooksCorpus (800M words)

English Wikipedia

3만 token vocabulary

#### 데이터의 tokenizing

WordPiece tokenizing

2개의 token sequence가 학습에 사용!

#### Masking하기!

![wwwwewewe](https://user-images.githubusercontent.com/59636424/134958391-aa872c45-05b7-46ab-903f-27b745a3ed2b.PNG)

* NLP 실험

![eeee](https://user-images.githubusercontent.com/59636424/134959110-54c3aa5b-0dfc-4d32-a68b-6f730d4682f0.PNG)

#### 감성 분석!

![sss](https://user-images.githubusercontent.com/59636424/134959461-8099a6ac-98ce-4828-af27-8db7064ac7ff.PNG)

#### 관계 추출

![yyyy](https://user-images.githubusercontent.com/59636424/134959709-80465ba4-7ce2-4ba2-821c-2024baf746a9.PNG)

관계가 대상이 되는 것이 entity이다!

**왜? 이것이 단일 문장 분류가 되는 것인가?**

subject(주어), object(목적어), sentence가 단일 문장으로 생각하면, subject와 object가 주어지면 어떤 관계를 가지는가?

#### 의미 비교

![qqq](https://user-images.githubusercontent.com/59636424/134960555-5729d13a-864d-4dda-9546-4d875eff9106.PNG)

의미적으로 두 문장이 관련이 있는가?

=> 너무 상관이 없는 데이터로 구축되었다!

#### 개체명 분석

![ww](https://user-images.githubusercontent.com/59636424/134960708-64986217-57f5-432f-94a3-cf5c64d8ee94.PNG)

#### 기계 독해

![rr](https://user-images.githubusercontent.com/59636424/134960913-97d6563e-6338-4668-90e5-6f00e46b4bdf.PNG)

### 한국어 BERT 모델

* ETRI KoBERT의 tokenizing

![qq](https://user-images.githubusercontent.com/59636424/134961796-d0cf89f4-6021-4ec4-9bbf-96696d37cd01.PNG)

형태소 단위로 분리 -> WordPiece Embedding 함!

### Advanced BERT model

![rrrr](https://user-images.githubusercontent.com/59636424/134963027-02f89e2e-904f-4a1c-a1ec-b8e1abd0f530.PNG)

BERT 내에 Entity 명시하는 것이 존재 X

-> 그래서 Entity tag 부착 -> BERT Embedding layer에 Entity Embedding layer 추가!

## (4강) 한국어 BERT 언어 모델 학습 및 다중 과제 튜닝 - BERT Pre-Training

### 1. BERT 학습하기

* 학습 단계

Tokenizer -> 데이터셋 확보 -> NSP -> Masking

**도메인 특화 task의 경우, 도메인 특화된 학습 데이터만 사용하는 것이 성능 더 좋다!!**

* 학습을 위한 데이터 만들기

![tttttt](https://user-images.githubusercontent.com/59636424/135083207-b1796caa-e8d0-4376-8615-181d7a2c3860.PNG)

=> 그리고 이 데이터로 Masking 작업을 해야한다!


# 3. 실습!

## (3강) BERT 언어 모델 소개 - 0_Huggingface 실습

~~~
# Store the model we want to use
MODEL_NAME = "bert-base-multilingual-cased"

# We need to create the model and tokenizer
model = AutoModel.from_pretrained(MODEL_NAME)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
~~~

-> 104개 언어를 pretrain한 모델인 bert-base-multilingual-cased!

* tokenizer.tokenize method: WordPiece로 쪼개기, 토큰화
* tokenizer.encode method: encoding하는 것(숫자화) (자동으로 speical token 부착!)
* tokenizer.decode method: 문자화!

~~~
tokenized_text = tokenizer.tokenize(
    text,
    add_special_tokens=False,
    max_length=5,
    truncation=True
    )
~~~

-> 이 코드 사용 시, token된 length 최대 5로 잘라준다!

* **이번엔 새로운 token을 추가**

  **보통 UNK token을 add_tokens method로 추가!**

* **특정 역할을 위한 special token도 추가 가능!**

예로, add_special_tokens({"additional_special_tokens":["[SHKIM]", "[/SHKIM]"]}) 식으로 추가 가능!

=> 이렇게 추가할 token 갯수를 하나의 변수로 저장해 Model의 resize 해줘야 한다!

=> 만약에 vocab을 새롭게 추가했다면, 반드시 model의 embedding layer 사이즈를 늘려야 한다!

## (3강) BERT 언어 모델 소개 - 1_BERT_유사도_기반_챗봇 실습

1. 사용자의 질문(query)를 입력 받는다.
2. query를 pretrained BERT의 입력으로 넣어, query 문장에 해당하는 [CLS] token hidden을 얻는다.
3. 사전에 준비된 질의응답 Dataset에 존재하는 모든 질문들을 pretrained BERT의 입력으로 넣어, 질문들에 해당하는 [CLS] token hidden을 얻는다.
4. query의 [CLS] token hidden과 질문들의 [CLS] token hidden간의 코사인 유사도를 구한다.
5. 가장 높은 코사인 유사도를 가진 질문의 답변을 반환시켜준다.

## (4강) 한국어 BERT 언어모델 학습 - 0_BERT_MASK_Attack

* 모델 load할 때마다, tokenizer 확인해주자!

~~~
nlp_fill = pipeline('fill-mask', top_k=5, model=model, tokenizer=tokenizer)
~~~

## (4강) 한국어 BERT 언어모델 학습 - 1_한국어_BERT_pre_training

* tokenizer 학습!

~~~
wp_tokenizer.train(
    files="my_data/wiki_20190620_small.txt",
    vocab_size=20000,   # vocab size 를 지정해줄 수 있습니다.
    min_frequency=2,
    show_progress=True,
    special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"],
    wordpieces_prefix="##"
)
~~~

* BERT tokenizer 정의

~~~
tokenizer = BertTokenizerFast(
    vocab_file='/content/wordPieceTokenizer/my_tokenizer-vocab.txt',
    max_len=128,
    do_lower_case=False,
    )
~~~

* BertConfig

~~~
config = BertConfig(    # https://huggingface.co/transformers/model_doc/bert.html#bertconfig
    vocab_size=20000,
    # hidden_size=512,
    # num_hidden_layers=12,    # layer num -> 빠르게 돌리기 위해서 layer 낮힐 수 있다!
    # num_attention_heads=8,    # transformer attention head number
    # intermediate_size=3072,   # transformer 내에 있는 feed-forward network의 dimension size
    # hidden_act="gelu",
    # hidden_dropout_prob=0.1,
    # attention_probs_dropout_prob=0.1,
    max_position_embeddings=128,    # embedding size 최대 몇 token까지 input으로 사용할 것인지 지정 -> 모델 최대 input token size!
    # type_vocab_size=2,    # token type ids의 범위 (BERT는 segmentA, segmentB로 2종류)
    # pad_token_id=0,
    # position_embedding_type="absolute"
)

model = BertForPreTraining(config=config)
~~~

-> **BertConfig의 default vocab_size는 영어로 맞춰져있어 vcoab size는 무조건 고쳐야 한다!**

-> **transformer 내부 hidden size**

-> **max_position_embeddings: 모델 최대 input token size!**

BertForPreTraining으로 config를 받을 수 있다!!

* TextDatasetForNextSentencePrediction 중의 코드!

~~~
cached_features_file = os.path.join(
            directory,
            "cached_nsp_{}_{}_{}".format(
                tokenizer.__class__.__name__,
                str(block_size),
                filename,
            ),
        )
~~~

-> 전처리 과정을 반복하지 않도록 caching data 사용!

~~~
logger.info(f"Creating features from dataset file at {directory}")
# 여기서부터 본격적으로 dataset을 만듭니다.
self.documents = [[]]
~~~

-> documents는 wiki 데이터를 예시로 들면, 이순신과 문재인의 document가 나오면 서로 다른 document로 여겨져야 한다!!

-> 그래서 documents 단위로 학습!

~~~
DataCollatorForLanguageModeling
~~~

-> Masking을 할 필요 없다!!

* 모델 Train

~~~
training_args = TrainingArguments(
    output_dir='model_output', 
    overwrite_output_dir=True, #새로운 모델 학습됨에 따라 overwrite
    num_train_epochs=10,
    per_gpu_train_batch_size=32,
    save_steps=1000, # step 수에 따라, 저장!
    save_total_limit=2, #마지막 2개 빼고 과거 삭제!
    logging_steps=100 # log 몇 step마다 찍어줌
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset
)
~~~

* 모델 load

~~~
my_model = BertForMaskedLM.from_pretrained('model_output')
~~~

-> BertForPreTraining class로 한 것을 BertForMaskedLM으로 받음

