# 9월 28일 배운 것!

## (3강) BERT 언어모델 소개

### 1. BERT 모델 소개

![xx](https://user-images.githubusercontent.com/59636424/134952636-d42f878d-8239-4527-a124-d1df68885320.PNG)

Mask를 함으로써, 더 어려운 문제로 해결!!

![wwww](https://user-images.githubusercontent.com/59636424/134952884-e827da3d-2061-4125-b91d-4db0f7f06282.PNG)

sentence 2개를 입력받음!

transformer 12 layer로 이뤄져있음!

#### BERT 학습 Corpus 데이터 & tokenizing

BooksCorpus (800M words)

English Wikipedia

3만 token vocabulary

#### 데이터의 tokenizing

WordPiece tokenizing

2개의 token sequence가 학습에 사용!

#### Masking하기!

![wwwwewewe](https://user-images.githubusercontent.com/59636424/134958391-aa872c45-05b7-46ab-903f-27b745a3ed2b.PNG)

* NLP 실험

![eeee](https://user-images.githubusercontent.com/59636424/134959110-54c3aa5b-0dfc-4d32-a68b-6f730d4682f0.PNG)

#### 감성 분석!

![sss](https://user-images.githubusercontent.com/59636424/134959461-8099a6ac-98ce-4828-af27-8db7064ac7ff.PNG)

#### 관계 추출

![yyyy](https://user-images.githubusercontent.com/59636424/134959709-80465ba4-7ce2-4ba2-821c-2024baf746a9.PNG)

관계가 대상이 되는 것이 entity이다!

**왜? 이것이 단일 문장 분류가 되는 것인가?**

subject(주어), object(목적어), sentence가 단일 문장으로 생각하면, subject와 object가 주어지면 어떤 관계를 가지는가?

#### 의미 비교

![qqq](https://user-images.githubusercontent.com/59636424/134960555-5729d13a-864d-4dda-9546-4d875eff9106.PNG)

의미적으로 두 문장이 관련이 있는가?

=> 너무 상관이 없는 데이터로 구축되었다!

#### 개체명 분석

![ww](https://user-images.githubusercontent.com/59636424/134960708-64986217-57f5-432f-94a3-cf5c64d8ee94.PNG)

#### 기계 독해

![rr](https://user-images.githubusercontent.com/59636424/134960913-97d6563e-6338-4668-90e5-6f00e46b4bdf.PNG)

### 한국어 BERT 모델

* ETRI KoBERT의 tokenizing

![qq](https://user-images.githubusercontent.com/59636424/134961796-d0cf89f4-6021-4ec4-9bbf-96696d37cd01.PNG)

형태소 단위로 분리 -> WordPiece Embedding 함!

### Advanced BERT model

![rrrr](https://user-images.githubusercontent.com/59636424/134963027-02f89e2e-904f-4a1c-a1ec-b8e1abd0f530.PNG)

BERT 내에 Entity 명시하는 것이 존재 X

-> 그래서 Entity tag 부착 -> BERT Embedding layer에 Entity Embedding layer 추가!

# 3. 실습!

## (3강) BERT 언어 모델 소개 - 0_Huggingface 실습

~~~
# Store the model we want to use
MODEL_NAME = "bert-base-multilingual-cased"

# We need to create the model and tokenizer
model = AutoModel.from_pretrained(MODEL_NAME)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
~~~

-> 104개 언어를 pretrain한 모델인 bert-base-multilingual-cased!

* tokenizer.tokenize method: WordPiece로 쪼개기, 토큰화
* tokenizer.encode method: encoding하는 것(숫자화) (자동으로 speical token 부착!)
* tokenizer.decode method: 문자화!

~~~
tokenized_text = tokenizer.tokenize(
    text,
    add_special_tokens=False,
    max_length=5,
    truncation=True
    )
~~~

-> 이 코드 사용 시, token된 length 최대 5로 잘라준다!

* **이번엔 새로운 token을 추가**

  **보통 UNK token을 add_tokens method로 추가!**

* **특정 역할을 위한 special token도 추가 가능!**

예로, add_special_tokens({"additional_special_tokens":["[SHKIM]", "[/SHKIM]"]}) 식으로 추가 가능!

=> 이렇게 추가할 token 갯수를 하나의 변수로 저장해 Model의 resize 해줘야 한다!

=> 만약에 vocab을 새롭게 추가했다면, 반드시 model의 embedding layer 사이즈를 늘려야 한다!

## (3강) BERT 언어 모델 소개 - 1_BERT_유사도_기반_챗봇 실습

1. 사용자의 질문(query)를 입력 받는다.
2. query를 pretrained BERT의 입력으로 넣어, query 문장에 해당하는 [CLS] token hidden을 얻는다.
3. 사전에 준비된 질의응답 Dataset에 존재하는 모든 질문들을 pretrained BERT의 입력으로 넣어, 질문들에 해당하는 [CLS] token hidden을 얻는다.
4. query의 [CLS] token hidden과 질문들의 [CLS] token hidden간의 코사인 유사도를 구한다.
5. 가장 높은 코사인 유사도를 가진 질문의 답변을 반환시켜준다.
