# 9월 28일 배운 것!

## (3강) BERT 언어모델 소개

### 1. BERT 모델 소개

![xx](https://user-images.githubusercontent.com/59636424/134952636-d42f878d-8239-4527-a124-d1df68885320.PNG)

Mask를 함으로써, 더 어려운 문제로 해결!!

![wwww](https://user-images.githubusercontent.com/59636424/134952884-e827da3d-2061-4125-b91d-4db0f7f06282.PNG)

sentence 2개를 입력받음!

transformer 12 layer로 이뤄져있음!

#### BERT 학습 Corpus 데이터 & tokenizing

BooksCorpus (800M words)

English Wikipedia

3만 token vocabulary

#### 데이터의 tokenizing

WordPiece tokenizing

2개의 token sequence가 학습에 사용!

#### Masking하기!

![wwwwewewe](https://user-images.githubusercontent.com/59636424/134958391-aa872c45-05b7-46ab-903f-27b745a3ed2b.PNG)

* NLP 실험

![eeee](https://user-images.githubusercontent.com/59636424/134959110-54c3aa5b-0dfc-4d32-a68b-6f730d4682f0.PNG)

#### 감성 분석!

![sss](https://user-images.githubusercontent.com/59636424/134959461-8099a6ac-98ce-4828-af27-8db7064ac7ff.PNG)

#### 관계 추출

![yyyy](https://user-images.githubusercontent.com/59636424/134959709-80465ba4-7ce2-4ba2-821c-2024baf746a9.PNG)

관계가 대상이 되는 것이 entity이다!

**왜? 이것이 단일 문장 분류가 되는 것인가?**

subject(주어), object(목적어), sentence가 단일 문장으로 생각하면, subject와 object가 주어지면 어떤 관계를 가지는가?

#### 의미 비교

![qqq](https://user-images.githubusercontent.com/59636424/134960555-5729d13a-864d-4dda-9546-4d875eff9106.PNG)

의미적으로 두 문장이 관련이 있는가?

=> 너무 상관이 없는 데이터로 구축되었다!

#### 개체명 분석

![ww](https://user-images.githubusercontent.com/59636424/134960708-64986217-57f5-432f-94a3-cf5c64d8ee94.PNG)

#### 기계 독해

![rr](https://user-images.githubusercontent.com/59636424/134960913-97d6563e-6338-4668-90e5-6f00e46b4bdf.PNG)

### 한국어 BERT 모델

* ETRI KoBERT의 tokenizing

![qq](https://user-images.githubusercontent.com/59636424/134961796-d0cf89f4-6021-4ec4-9bbf-96696d37cd01.PNG)

형태소 단위로 분리 -> WordPiece Embedding 함!

### Advanced BERT model

![rrrr](https://user-images.githubusercontent.com/59636424/134963027-02f89e2e-904f-4a1c-a1ec-b8e1abd0f530.PNG)

BERT 내에 Entity 명시하는 것이 존재 X

-> 그래서 Entity tag 부착 -> BERT Embedding layer에 Entity Embedding layer 추가!

