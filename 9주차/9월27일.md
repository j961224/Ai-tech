# 1. 9월 27일 배운 것!

## 1강 - 인공지능과 자연어 처리

### 1. 인공지능의 탄생과 자연어 처리

#### 1.1 자연어 처리 소개 

인공지능: 인간이 지능이 가지는 학습 따위를 갖춘 컴퓨터 시스템!

#### 1.2 자연어처리의 응용분야

문서 분류, 기계 독해, 텍스트 압축 등등!

encoding -> decoding

* 컴퓨터의 자연어 처리

Encoder: 벡터 형태로 자연어를 인코딩 -> 메세지 전송 -> Decoder: 벡터를 자연어로 디코딩!

#### 1.3 자연어 단어 임베딩

Word2Vec으로 서로 득립된 vocab으로 관리!!

* Fasttext

**subword 정보를 집중해서 만들어진 알고리즘!!**

중심 단어에서 주변 단어를 예측한다!

![ㅊㅊㅊ](https://user-images.githubusercontent.com/59636424/134851285-f7fd5125-8c34-4164-8659-55ddb8d42537.PNG)

-> **주변 단어와 중심 단어를 만들 때, n-gram을 이용!** -> n-gram vector를 합산한 후 평균을 통해 단어 벡터 획득!!

-> **오탈자, OOV, 등장 횟수가 적은 학습 단어에 강세!**

* Word2Vec과 Fasttext의 한계점

**동형어, 다의어에 대해서 Embedding 성능이 좋지 않음** -> 문맥을 고려하지 못함!

### 2. 딥러닝 기반의 자연어처리와 언어모델

#### 2.1 언어모델

자연 법칙을 컴퓨터로 모사함으로써 시뮬레이션이 가능!

**이전 state를 기반으로 미래 state를 예측!**

* 검색 엔진

![ㅌㅌ](https://user-images.githubusercontent.com/59636424/134851731-9028c18e-b080-47bb-889b-588a608b844e.PNG)

입력된 단어를 바탕으로 다음에 등장할 단어를 잘 예측하는 방법 사용!

* **Markov 기반 모델(Markov Chain Model)**

![ㅡㅁ가](https://user-images.githubusercontent.com/59636424/134851896-62b3fcaa-888a-4450-8118-041fa6c37663.PNG)

문장을 어절 단위로 쪼갬 -> 각 단어 빈도수 확인 -> ex) I 다음 단어는 like의 빈도수가 높으니 나올 확률이 높다. -> 이렇게 만든 것!

* **RNN 기반 언어 모델**

![구ㅜ](https://user-images.githubusercontent.com/59636424/134852089-caf4c989-cf7f-4761-8539-4e4f26c356e7.PNG)

이전 state 정보가 다음 state 예측에 사용!

앞선 단어 문맥을 고려한 최종 출력 vector -> **context vector**

![ㅁㅊㅁㅊㅁㅊㅁ](https://user-images.githubusercontent.com/59636424/134852234-f051cf03-5898-476c-8c8e-0e4d7b000e51.PNG)

* **Seq2Seq**

![ㅁㅁ](https://user-images.githubusercontent.com/59636424/134852349-f0170272-422c-44e9-9945-678f8f2d6988.PNG)

Encoder layer(Context vector 획득) + Decoder layer(Context vector로 출력 예측)

ex) 이미지 캡셔닝, 형태소 분석기, 음성 인식기

---

* **Attention**

RNN의 단점은 긴 sequence가 들어오면 앞에 정보가 정정 희석된다.

고정된 context vector 사이즈로 긴 sequence에 대한 정보를 함축이 어려움!

**그래서 attention 등장!**

**중요한 feature, 중요하지 않는 feature 고려해서 처리!!**

![ㅂㅂ](https://user-images.githubusercontent.com/59636424/134852716-2e5de591-eb7d-40d2-a48d-2407c924f863.PNG)

모든 token에 대해 관심을 가짐!!

**한계점으로 순차적으로 연산이 이뤄지는 것을 기다려야함 -> 연산 속도 느림!**

-> 그래서 나온 것이 self-attention

* **self-attention**

![ㅌㅌㅌㅌㅌ](https://user-images.githubusercontent.com/59636424/134852894-7df5dbe3-c81e-492f-926d-5c3f04c65e6f.PNG)

모든 token을 all-to-all로 연결!!

-> 이것으로 Transformer가 만들어짐!!

![ㅌㅌㅌ](https://user-images.githubusercontent.com/59636424/134852999-d368aedd-84e4-442e-b2d0-3f92a53f0c22.PNG)

Transformer는 encoder와 decoder를 하나의 network 내에 붙여놨다!!


## 2. 자연어의 전처리

### 1. 자연어 전처리

* 자연어 전처리 단계

Task 설계 -> 필요 데이터 수집 -> 통계학적 분석 -> 전처리 -> Tagging -> Tokenizing -> 모델 설계 -> 모델 구현 -> 성능 평가 -> 

* Task 설계

ex) 악성 댓글 필터링 task -> 악성 댓글 classifier 만들기!

* 필요 데이터 수집

ex) 댓글 데이터 수집

* 통계학적 분석

Toekn 개수 -> 아웃 라이어 제거!

빈도 확인 -> vocabulary 정의!

* 전처리

개행문자, 특수문자 제거, 공백 제거, 중복 표현 제거, 이메일과 링크 제거, 제목 제거, 불용어 제거, 조사 제거, 띄어쓰기와 문장분리 보정

* Tagging

악성 댓글인지를 판단해 Tagging

* Tokenizing - 어절 단위(띄어쓰기), 형태소 단위(의미를 가진 어절, 자음, 모음 등으로 분리), WordPiece 단위

![ㅂㅂㅂ](https://user-images.githubusercontent.com/59636424/134854158-3821f7d5-e72d-49d4-805b-b9a5c71e9bb7.PNG)

### 2. 한국어 토큰화

* 토큰화: 주어진 데이터를 토큰으로 나누는 작업

* 문장 토큰화: 문장 분리

* 단어 토큰화: 띄어쓰기로 분리


---

# 3. 실습 코드 분석!

## 0. 한국어 전처리

* corpus 수집하기

**newspapaer3k library로 url을 입력하면 제목과 content를 분리해서 수집 가능!!**

~~~
!pip install newspaper3k
~~~

* 저작권에 자유로운 wkikitree 뉴스 데이터 사용!

~~~
news_url = "https://www.wikitree.co.kr/articles/252931"
article = Article(news_url, language='ko')
~~~

* 전처리 1. HTML tag 제거하기!

* 전처리 2. 문장 분리하기!

    kss library 사용! -> kss.split_sentences 사용!

* Normalizing

    이메일 제거! (개인 정보 제거!)

    HashTag 제거!

    멘션 제거!

    URL 제거!

    한국어 크롤링으로 생기는 문제(u200b 등) 제거!

    언론 정보 제거!
 
    저작권 관련 텍스트를 제거!

    이미지에 대한 label을 제거!

    괄호 내부에 의미가 없는 정보를 제거!

    "ㅋㅋㅋㅋ" 와 같은 부분을 Normalize를 위해 **soynlp 사용!**
    
    기호 Normalize
    
    이중 space 치환
    
    중복된 문장 제거
    
    **띄어쓰기 보정 PyKoSpacing** -> 맞춤법에 맞게 띄어쓰기 해줌!
    
    **맞춤법 검사기 -> py-hanspell**
    
    **형태소 분석 기반 필터링 konlpy!**
    
    **mecab 형태소 분석기!**
    
~~~
from soynlp.normalizer import *
print(repeat_normalize('와하하하하하하하하하핫', num_repeats=2)) #num_repeats로 최대 반복수 정하기!

와하하핫
~~~

* Mecab 사용하기!

~~~
mecab.pos(문장,join=False)
~~~

-> 각 단어마다 형태소 분석이 된다!

---

* unicode 기반으로 filtering!


## 1. 한국어 토크나이징 

* 한국어 wikipedia 파일 사용!

### 어절 단위 tokenizing

~~~
tokenized_text = text.split(" ")    # split 함수는 입력 string에 대해서 특정 string을 기반으로 분리해줍니다.
~~~

-> 띄어쓰기!

* 형태소 분석기로는 mecab을 사용 - 관계 추출에 중요한 역할을 할 것이다.

~~~
from konlpy.tag import Mecab

mecab = Mecab()
print(mecab.pos("아버지가방에들어가신다."))

[('아버지', 'NNG'), ('가', 'JKS'), ('방', 'NNG'), ('에', 'JKB'), ('들어가', 'VV'), ('신다', 'EP+EF'), ('.', 'SF')]
~~~

### 음절 단위 tokenizing

~~~
text = "이순신은 조선 중기의 무신이다."
tokenized_text = list(text)    # split 함수는 입력 string에 대해서 특정 string을 기반으로 분리해줍니다.
print(tokenized_text)  

['이', '순', '신', '은', ' ', '조', '선', ' ', '중', '기', '의', ' ', '무', '신', '이', '다', '.']
~~~

### 자소 단위 tokenizing

한글은 하나의 문자도 최대 초성, 중성, 종성, 총 3개의 자소로 분리가 가능

hgtk 라이브러리 사용!!

~~~
text = "이순신은 조선 중기의 무신이다."
tokenized_text = list(hgtk.text.decompose(text))
print(tokenized_text)
# ㅇ ㅣ ㅅ ㅜ ㄴ ㅅ ㅣ ... 
~~~

'ᴥ' 는 음절 단위를 끊어 주는 것 ex) ㅇ ㅣ 'ᴥ' ㅅ ㅜ ...

### Wordpiece tokenizing

~~~
wp_tokenizer = BertWordPieceTokenizer(
    clean_text=True,    # [이순신, ##은, ' ', 조선]
    handle_chinese_chars=True,
    strip_accents=False,    # True: [YepHamza] -> [Yep, Hamza]
    lowercase=False,
)

# And then train
wp_tokenizer.train(
    files="my_data/wiki_20190620_small.txt",
    vocab_size=10000,
    min_frequency=2,
    show_progress=True,
    special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"],
    limit_alphabet=1000,
    wordpieces_prefix="##"
)
~~~

~~~
text = "이순신은 조선 중기의 무신이다."
tokenized_text = wp_tokenizer.encode(text)
print(tokenized_text)
print(tokenized_text.tokens)
print(tokenized_text.ids)

Encoding(num_tokens=10, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])
['이', '##순', '##신은', '조선', '중', '##기의', '무', '##신이', '##다', '.']
[707, 1489, 7579, 2000, 756, 2602, 453, 8446, 1031, 16]
~~~





