# 1. 8월 26일 배운 것!

## 7-1. Training & Inference

### 오차역전파

Loss 함수 = Cost 함수 = Error 함수

### Loss도 사실은 nn.Moudle Family이다!

### loss.backward()

: 이 함수가 실행되면 모델의 하라미터의 grad 값이 업데이트된다.

-> forward와 전체적인 chain을 이루고 있다.

### loss

* Focal Loss

: Class imbalance 문제가 있을 경우, 맞춘 확률이 높은 class는 조금의 loss를 맞춘 확률이 낮은 class는 loss를 훨씬 높게 부여한다!

* Label smoothing Loss

: Class target label을 Onehot 표현으로 사용하기 보다는 조금 Soft하게 표현해서 일반화 성능을 높이기 위해서 사용한다!

### Optimizer

: 영리하게 움직일수록 빨라진다!!!

### LR scheduler

: learning rate 변화를 볼 수 있다!

#### StepLR

: 특정 step 마다 LR 감소

~~~
scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=2,gamma=0.1)
~~~

#### CosineAnnealingLR

: Cosine 함수 형태처럼 LR을 급격히 변경시킨다!

#### REduceLROnPlateau

: 더 이상 성능 향상이 없을 때 LR 감소

### Metric

: 모델의 평가와 Score의 허와 실을 판별한다.

ex) f1-score, precision, accuracy, MAE, MSE 등등

## 8. Traning & Inference(2) - Process

### Training 준비!

* optimizer.zero_grad()

* loss를 마지막으로 chain이 만들어진다. 

* loss의 grad_fn chain -> loss.backward()

* optimizer.step()

### More: Gradient Accumulation

### Inference Process

* model.eval()

* with torch.no_grad():

-> 안의 모든 tensor가 grad가 grad_enabled가 False가 된다.

### Validation 확인

: 추론 과정에 Validation 셋이 들어가면 검증이다.

### Checkpoint

: 중간에 좋은 결과를 저장하기 위해서 checkpoint로 저장한다.

## 2. 8/26 (목) ONE AI 피어세션

팀원 상은 님 리더보드 3위 축하의 시간 ^0^///

페이스넷 or 이미지넷 pre-trained 모델 논의

상하 님 유닛 테스트 트라이 (?)

일요일 19시 줌 약속

컴피티션 데이터 중 age class 가 불균형한 문제

깃 아무 practice 

## 3. 마스크 착용 상태 분류 대회 4일차

### 시도. EfficientNetb4 (정확도 76.873%, F1-score: 0.7032)

* 시도한 특징

      마스크 착용 유무, 나이, 성별 이렇게 18 class를 분류하는데 1번에 18 class를 분류했다.
      
      Data Crop을 (380,380) 수행/ Normalization 수행
      
      Stratified K-Fold 4회 수행, epoch 30, batch_size 32 수행

* 개선할 점

      epoch을 많이 늘려서 overfitting이 일어난 것으로 추정된다.
      
      특히, 나이 age 부분이 특히 문제가 되는 것 같다!
      
      또한 사전학습모델의 모든 파라미터를 다 학습한 것도 문제가 된 것 같다.
 
