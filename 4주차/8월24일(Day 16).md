# 1. 8월 24일 공부한 것!

## 3강 Dataset

: Dataset을 만드는 것이 중요하다!

### Pre-processing(전처리)

#### Bounding box

: 가끔 필요 이상으로 많은 정보를 가지고 있기도 한다. (이번 것은 bounding box가 없지만 마스크가 중간에 있는 것을 생각해볼 수 있다.)

#### Resize

: 큰 사이즈의 경우, 학습을 하는데 많은 시간이 소요될 수 있다.

-> 효율적인 size를 찾는 것이 중요하다!!!

ex) 이미지를 밝기를 조절하는 것도 좋을 수 있다.


### Generalization

#### Bias & Variance

- Variance가 높으면 Overfitting이 일어나고 Bias가 높으면 Underfitting이 일어난다.

#### Train / Validation

: 훈련 셋 중 일정 부분을 따로 분리, 검징 셋으로 활용한다.

#### Data Augmentation

주어진 데이터가 가질 수 있는 경우, 상태의 다양성이 존재한다. (데이터 다양화 시키기!)

-> 밝기 조절, 회전 등 여러 noise로 좀 더 다양한 이미지를 훈련시킨다.

=> 모델의 쓰임새를 살펴보자!!

#### torchvision.transforms

#### Albumentations

: 좀 더 빠르고 다양한 것을 제공한다. (transforms보다)

=> 이 라이브러리를 이번 마스크 대회에 1번 사용해보자!!

## 4강 Data Generation

### Data Feeding

- Feed: 대상의 상태를 고려해서 적정한 양을 준다.

* 모델에 먹이를 준다는 의미는?

=> 모델에 처리할 수 있는 양이 있다면 Data Generator는 그보다 적다면 아무리 모델이 빠른 성능을 가지고 있더라도 Generator 이상으로는 안 나옴

* Transforms 시, resize 순서에 따라도 걸리는 시간 차이가 늘어난다.

### torch.utils.data

### Datasets의 구조

~~~
from torch.utils.data import Dataset #torch.utils.data의 Dataset 라이브러리 상속!

class Mydataset(Dataset):
  def __init__(self): # MyDataset 클래스가 처음 선언 되었을 때 호출
     pass
  
  def __getitem__(self, index): # MyDataset의 데이터 중 index 위치의 아이템을 리턴
     return None
  
  def __len__(self): # MyDataset 아이템의 전체 길이
     return None
~~~

### DataLoader

내가 만든 Dataset을 효율적으로 사용할 수 있도록 관련 기능 추가한 것이다.

-> batch_size, num_workers, shuffle, samepler 등을 사용할 수 있다.

-> collen_fn 기능은 배치마다 작업하고 적용하고 싶다면(함수를 적용하고 싶다면) 사용할 수 있다.

#### num_workers

: 클수록 thread를 더 사용하므로 학습 시간이 줄어든다.


