# 1. 8월 12일 배운 내용!

## 1-1. RNN

### Sequential Model

: 과거 고려해야하는 정보량이 늘어난다!

* Autoregressive model

![ㅊㅊㅊ](https://user-images.githubusercontent.com/59636424/129120488-cfe9ecb0-0179-490c-a227-31eb180c026e.PNG)

=> 과거의 몇개만 보는 방법이 있다.

* Markov model(first order autoregressive mode)

![ㅇㄹㅇㄹ](https://user-images.githubusercontent.com/59636424/129120588-b73c06b1-b10c-42a5-9cd5-435ad02b92cf.PNG)

: 내가 가정하기에는 바로 전 과거에만 현재가 의존한다.

-> 많은 정보를 버리게 되지만 joint distribution 표현하기에 쉽다.

* Latent autoregressive model

![ㅍㅍㅍㅍㅍㅍㅍㅍㅍㅍㅍ](https://user-images.githubusercontent.com/59636424/129120694-50061f9d-24c2-4568-9d27-c94107b06676.PNG)

: 중간에 히든 state가 과거 정보를 요약하고 있다. -> 과거 이전에 정보를 요약하고 있다.

### RNN

![ㅇㄹㅇㄹㅇㄹ](https://user-images.githubusercontent.com/59636424/129120855-31c1ce58-46e2-48e4-9183-30ab7ab842ff.PNG)

: 자기 자신으로 돌아오는 구조가 더해져 있다. (hidden state time stamp는 Xt에만 의존하는 것이 아니라 이전에 t-1에도 의존한다.)


![ㅊㅊㅊㅊㅊㅊㅊㅊ](https://user-images.githubusercontent.com/59636424/129120992-64989234-68a5-4b52-9db3-2ee2a41ba90c.PNG)

: 시간 순으로 풀면 각각의 네트워크의 파라미터를 shared 한다.

* **RNN의 단점**

![ㅔㅔ](https://user-images.githubusercontent.com/59636424/129121226-e824a5f2-0915-4395-b464-f638e8d8a46e.PNG)

: RNN의 단점으로 long term dependency로 과거의 정보를 취합해서 미래에서 고려해야하는데 하나의 fixed rule로 계속 정보를 취합하기에 과거 정보가 미래까지 살아남기가 힘들다.


* RNN의 학습 원리

![ㅣㄴ](https://user-images.githubusercontent.com/59636424/129121337-ed465c92-f323-452f-828b-f57be64b9eee.PNG)

: 중첩되는 구조로 activation 함수로 줄여버리면 점점 과거의 정보가 줄어들다 결국 long term dependency가 일어난다. -> gradient 손실


### LSTM

![ㅣㄴ스](https://user-images.githubusercontent.com/59636424/129121406-fd282252-b89c-4f6c-bf19-c21642b4345d.PNG)

(LSTM의 구조)

![ㅊㅊㅊㅊㅊㅊㅊㅊㅊㅊㅊㅊㅊㅊㅊㅊㅊㅊ](https://user-images.githubusercontent.com/59636424/129121813-8007dfdf-fd37-415e-8e07-cdb4b98ab822.PNG)

-> x는 입력으로 단어이다.

-> previous cell state는 내부에서만 흘러가고 지금까지의 정보를 취합한 것이다.

-> output은 다음 번에 previous hidden state로 들어간다.

    * forget gate
    
    * input gate
    
    * output gate
    
---

* Core idea

: cell state가 핵심이다!

-> 매번 t마다 정보가 올라오면 정보를 조작해 어떠한 정보가 유용함에 따라 조작해 넘겨준다. (gate 이다.)

---

#### Forget gate

![forget](https://user-images.githubusercontent.com/59636424/129121997-06cc084c-28cb-43dd-8499-e18e0560c49a.PNG)

: 어떤 것을 버릴지 선정

#### Input Gate

![input](https://user-images.githubusercontent.com/59636424/129122315-92d95c80-3a38-4217-8fa8-560d6c391ec5.PNG)

: 현재 입력이 들어오면 무작정 올리는 것이 아니라 어떠한 정보를 올릴지 결정

-> i_t는 이전의 previous hidden state와 현재 입력을 가지고 만든다. (어떤 정보를 올릴지/추가할지)

-> cell state candidate는 이전의 cell state와 현재 입력으로 다른 학습 neural net으로 tanh로 정규화 시킨 값 (올릴 정보를 알아야 하는데 그 역할)

#### Update cell

![vf](https://user-images.githubusercontent.com/59636424/129122527-417d09c1-504f-4abc-9a28-ff8f9190473d.PNG)

: cell state를 업데이트 시킨다.

=> 버릴 것은 버리고 가져갈 것은 가져간다.

#### Output gate

![output](https://user-images.githubusercontent.com/59636424/129122557-c5cd8b28-3103-4b6f-b001-8e372afbcce7.PNG)

: 어떤 값을 밖으로 내보낼지 결정한다.


### GRU

![gru](https://user-images.githubusercontent.com/59636424/129122853-5020fd5a-7b27-43c0-b551-f4f701ac3bd8.PNG)

-> cell state가 없고 reset gate와 update가 있다. => 오직 hidden state밖에 없다.












