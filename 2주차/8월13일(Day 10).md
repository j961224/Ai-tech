# 8월 13일 공부 내용!

## 9. Generative Models

* Generation: 강아지와 같은 이미지를 만들 수 있다. -> 데이터에 없는 강아지 이미지

* Density estimation: 어떤 이미지가 들어갔을 때, 확률값 하나가 나와서 구분(분류)해내는 것 -> 이상 탐지에 쓰일 수 있다.

(explicity model이라고도 한다. => 입력이 주어졌을 때, 확률값을 얻어낼 수 있는 모델)

* Unsupervied representation learning: feature learning을 할 수 있다. (이미지가 공통적으로 가지는 것을 학습)

### Basic Discrete Distributions

* Bernoulli distribution: 표현하는데 숫자가 1개가 필요하다.

* Categorical dirtribution: m-1개의 파라미터가 필요하다.

* RGB joint distribution은 256 x 256 x 256경우가 있다.

    파라미터는 총 255 x 255 x 255개가 필요하다.
    
    FCN하는데 하나의 픽셀에 대해서 필요한 파라미터가 많다.
    
![dffffff](https://user-images.githubusercontent.com/59636424/129292507-b7676d0e-a255-4408-8ce6-532aadd7ceb7.PNG)

: n개의 binary 픽셀이 있다고 가정하자

-> n개의 필섹이 있다면 2^n - 1의 파라미터 경우의 수가 있다.

* n개의 binary pixel에 n개를 다 쓰지 말고 조금 더 쉽게 하는 방법은?

    n개의 pixel들이 독립적이라고 생각한다면?? -> 가능한 state는 2^n이다.
    
    => distribution을 표현하는데 필요한 파라미터 수는 n개만 있으면 된다. => 각각의 pixel에 대해 n개가 독립적이므로 다 더하면 된다.

### Conditional Independence

* Chain rule: n개의 joint distribution을 n개의 conditional distribution으로 바꾼다.

* Bayes' rule: 이거 역시 exact

* Conditional independence: z가 주어지면 x와 y가 독립적이다.  -> p(x|y,z)=p(x|z)

    이유는 z라는 랜덤 변수를 주어지는 x와 y는 독립적이므로 x라는 랜덤변수 표현 시, z가 주어지면 y는 상관이 없다.
    
    -> conditional 부분을 날려준다. => chain rule가 잘 섞어서 좋은 모델을 만들 수 있다.
    
* chain rule을 사용한다면?

![ch](https://user-images.githubusercontent.com/59636424/129293686-b2aef3d5-8ddd-4d54-bd40-edc6c3c19b57.PNG)

    fully dependent model과 같은 숫자의 파라미터를 갖는 것을 알 수 있다.
    
    -> 이유는 어떠한 가정도 하지 않고 했으므로
    
    p(x1): 1개 파라미터
    
    p(x2|x1): 2개 파라미터 ( p(x2|x1=0)인 확률과 p(x2|x1=1)일 확률이 필요하다. )
    
    p(x3|x1,x2): 4개 파라미터
    
    따라서 2^n - 1개이다.

* Markov assumption을 생각해보자!

: X_i+1은 X_i만 관련 있다고 가정

![ccccc](https://user-images.githubusercontent.com/59636424/129293944-48d52512-a580-4217-be94-18df4c4cfb1a.PNG)

: 차이점은 independent한거를 날려버린다.

=> 2n-1의 파라미터 수가 필요하다. -> exponenetial reduction하다!

=> 이러한 것을 **auto-gegressive model**이라고 한다.

### Auto-regressive Model

![dffffff](https://user-images.githubusercontent.com/59636424/129294099-b02aa269-0c2e-4206-abb4-98d4c41b91ff.PNG)

-> 28 x 28 binary pixel이라고 가정하자!

* 어떻게 표현할까? (p(x)를)

    chain rule을 가지고 나눈다!
    
    ![vvvv](https://user-images.githubusercontent.com/59636424/129294241-05475987-926c-4098-a388-ccd8ba5cc872.PNG)
    
    이러한 방법을 autoregressive model라고 한다.
    
    순서를 다 매겨야한다!
    
    ARN 모델은 이전 정보 N개를 고려하는 것을 말한다.
    
* autoregressive model는 하나의 정보가 이전 정보에 의존한다.


### NADE

![vvvvvvvvvv](https://user-images.githubusercontent.com/59636424/129294791-3fe480b8-7a86-403d-ab9e-2cf2e356ae05.PNG)

: i 번째 pixel을 첫 번째부터 i-1번째 pixel에 의존하도록 한다.

-> neural net 입장에서는 입력 차원이 달라지므로 weight가 계속 커진다!

* explicit 모델로 generation만 하는게 아니라 임의의 784개 주어지면 확률을 계산할 수 있다.

* 어떻게 계산이 가능한가?

    joint 확률을 chain rule로 coditional 분포로 바꾼다.
    
    우리의 모델이 첫 번째 픽셀 확률분포를 알고 있고 이게 주어지면 두 번째 픽셀 확률분포를 알 수 있다.
    
    이렇게 계산을 계속하게 되면 확률값을 구할 수 있다.
    
 => continuous random이라면, Gaussian mixture 모델을 마지막에 활용해 continuous distribution을 만들겠다!
 
### Pixel RNN
 
![kkk](https://user-images.githubusercontent.com/59636424/129295371-16b91611-62e6-48c2-b7b7-7c60b8dd1cf7.PNG)
 
: 이미지에 있는 pixel을 만드는 것이다.
 

-> i번째 픽셀에 R을 먼저 만들고 그 뒤에도 비슷하게 만든다.
 
-> 앞에랑 다르게, RNN를 만든다. => RNN을 통해 generation을 하겠다.
 
#### order을 어떻게 하느냐에 따라 다른 것이 있다!!

![ddd](https://user-images.githubusercontent.com/59636424/129295552-9cb813ea-b3bb-44cf-8bee-2950f4b33cdb.PNG)

##### Row LSTM

: i번째 픽셀을 만들 때 위쪽 정보를 이용한다.

##### Diagonal BiLSTM

: BiLSTM을 활용하되, 이전 정보들을 다 활용하는 것이다.






    
    

