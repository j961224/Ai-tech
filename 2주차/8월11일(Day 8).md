# 1. 8월 11일 배운 것!

## 4. Convolution이란?!

![convo](https://user-images.githubusercontent.com/59636424/128952568-936700aa-49b2-4e84-b87a-a7d02f7578bc.PNG)

마지막 수식에서, I는 전체 이미지 공간이고 K는 적용하고자 하는 convolution filter이다!

![convolution계산](https://user-images.githubusercontent.com/59636424/128952902-1620dc94-8582-4072-ab4d-3fd0163d8e04.PNG)
(Convolution 계산법)

* **2D Convolution한다는 것은?!**

![ㅊㅊㅊㅊ](https://user-images.githubusercontent.com/59636424/128953029-a006323b-1ac9-4bd8-8f3f-f1dca0e51c24.PNG)

: 해당 Convolution filter모양을 해당 이미지에 찍는다!

=> 적용 filter 모양에 따라 Convolution output이 위에 3개가 된다!

* RGB Image Convolution

![채ㅜㅊㅊ](https://user-images.githubusercontent.com/59636424/128953545-31364be4-15f9-47ca-b1ef-b3db86d1906c.PNG)

: filter 크기는 항상 같다! => 5 x 5 filter를 한다는 말은 5 x 5 x 3 filter/kernel을 사용한다는 말이다!

* Convolution filter channel 갯수에 맞게 output도 똑같은 갯수로 나온다!

![ㅇㅇㅇㅇㅇㅇㅇㅇㅇㅇㅇㅇㅇㅇㅇㅇ](https://user-images.githubusercontent.com/59636424/128953999-061592da-6448-4cce-8d1a-aa9787caa13c.PNG)

: 1번 Convolution을 거치고 나면 nonlinear acitvation이 들어가게 된다. => 여기서는 Relu가 들어간다. (4개 convolution filter가 필요)

---

### Convolution Neural Networks

![좀ㅅ소](https://user-images.githubusercontent.com/59636424/128954229-a0ca8a7c-1bf8-48be-b31d-ff01367ff83e.PNG)

* convolution layer: counvolution filter로 훑어서 값을 얻어내는 것 (feature extraction)

* polling layer: average polling 등 (feature extraction)

* fully connected layer: 마지막에 다 합쳐서 최종적 결과값을 내는 것 (decision making) => 요새 최소화나 없애는 추세!

* **fully connected layer를 왜 없애는 추세인가?**

: 내가 학습시키고자 하는 파라미터 숫자가 늘어나면 학습이 어렵고 generalization performance(학습에서 얻어진 결과가 다른 테스트 데이터가 얼마나 동작할지)가 떨어진다!

### Stride

![ㄴㅅ걍ㄷ](https://user-images.githubusercontent.com/59636424/128954727-dcd60d24-45c9-4f23-8a6f-c96b532806d5.PNG)

: convolution filter를 얼마나 자주 찍을지를 말함

### Padding

![ㅊㅊㅊㅊㅊㅊㅊㅊㅊㅊㅊㅊㅊ](https://user-images.githubusercontent.com/59636424/128954915-20464cf8-d5ec-4146-b86d-51b567148b26.PNG)

: zero padding을 하게 되면 위의 그림과 같이 spacial demension이 같아진다!

### Convolution Arithmetic

![계산](https://user-images.githubusercontent.com/59636424/128955189-1cc189b9-64cf-42a5-935e-e39e6cdf2502.PNG)

-> channel 64이므로 convolution filter가 64개 필요하므로 3 x 3 x 128 x 64이다.

![믿ㅌ둣 게산](https://user-images.githubusercontent.com/59636424/128955490-d022ad4f-0a1c-4e09-8612-5e72cc488ba9.PNG)

-> 2가 곱해진 이유는 96 channel짜리 filter map을 만들어야하는데 gpu 메모리가 크지 않아서 맞추기 위해 48짜리를 2개 만들었다.

![좀ㅅ'](https://user-images.githubusercontent.com/59636424/128956834-1e57932d-b7f4-4b22-a393-9a4fe6eab388.PNG)

* Dense layer(fully connected layer) 차원은 input 파라미터 개수(neural net 개수)와 output neural net 개수를 곱한 것 만큼이다.

* Dense layer 파라미터 개수가 convolution layer에 비해 엄청 많다.

    * 왜?: convolution operator가 각각의 하나의 커널이 모든 위치에 대해서 동일하게 적용되기 때문이다.

    * convolution operator는 shared parameter이다.
    
    * 그래서 파라미터를 줄이기 위해, fully connected layer를 줄이려고 한다.

~~~
첫 번째 layer는 입력 이미지는 244 x 244이다!
한 개의 kernel은 11 x 11 x 3이다.


두 번째 layer는 전의 kernel size는 5 x 5 x 48 kernel size이고 channel은 128로 늘었으므로 5 x 5 x 48 x 128 * 2 파라미터 개수를 가지고 있다.

세 번째 layer는 전의 kernel size는 3 x 3 x 128 kernel size가 2개이고 channel은 192이므로 3 x 3 x 128 x 2 x 192 x 2 파라미터 개수를 가지고 있다.

네 번째 layer는 전의 kernel size는 3 x 3 x 192 kernel size이고 channel은 192이므로 3 x 3 x 192 x 192 x 2 파라미터 개수를 가지고 있다.

마지막 layer는 전의 kernel size는 3 x 3 x 192 kernel size이고 channel은 128이므로 3 x 3 x 192 x 128 x 2 파라미터 개수를 가지고 있다.

첫 번째 Dense layer은 전의 kernel size는 13 x 13 x 128 kernel size이고 channel은 2048 이므로 13 x 13 x 128 x 2 x 2048 x 2 파라미터 개수를 가지고 있다.

두 번째 Dense layer는 input 파라미터 개수인 2048 x 2 와 현재 channel은 2048 size이므로 2048 x 2 x 2048 x 2 파라미터 개수를 가지고 있다.

마지막은 2048 x 2 input 파라미터 개수와 channel size는 1000이므로 2048 x 2 x 1000 파라미터 개수를 가지고 있다.
~~~

### 1 x 1 convolution

![ㅋㅋㅋㅋㅋㅋㅋㅋ](https://user-images.githubusercontent.com/59636424/128957031-15333341-a9c6-4cca-a097-9d6b4bdc7fae.PNG)

: 한 픽셀만 보는 것이고 channel 방향으로 줄인다.

=> 차원 줄이기 위해서 사용한다!

    * 1 x 1 convolution context dimension은 channel을 말한다.
    
    * convolution layer를 깊게 쌓으면서 파라미터 숫자를 줄일 수 있다. => bottleneck architecture이다!
    
 
## Modern CNN

### AlexNet

![alexnet](https://user-images.githubusercontent.com/59636424/128981491-96abae62-c41f-4dbf-81e5-b76a234e3b1a.PNG)

: 네트워크가 2개로 나눠어져 있다.(8개의 layer) => gpu를 최대한 활용하기 위해서!

- input을 11 x 11을 사용하는 것은 현명하지 않다! => convolution kernel이 볼 수 있는 이미지 영역은 커지지만 상대적으로 더 많은 파라미터가 필요하다!

#### key ideas

1. RELU activation 사용 -> nonlinear이고 기울기가 1이기에 원하는 gradient가 사라질 성질들이 많이 없다.

      linear model의 좋은 성질을 가지고 있다.
      
      gradient descent로 최적화 용이
      
      **gradient 소실 문제 극복** -> 다른 것들은 0에서 많이 벗어나게되면 기울기가 굉장히 0에 가까워지는데 relu는 그렇지 않다.

2. 2 GPU 사용

3. local response normalization(어떠한 입력공간에서 response가 많이 나오는 몇 개를 죽인다.) : 복수의 feature map 간에 정규화하는 방법이다.

4. Overlapping pooling(3x3 영역을 2픽셀 단위로 pooling 하여 조금씩 겹치는 부분이 있도록 pooling하여 overfitting 현상 개선)

5. Data augmentation

6. Dropout: 뉴런 중 몇 개를 0으로 바꾼다.

### VGGNet

![rrr](https://user-images.githubusercontent.com/59636424/128982537-7f68abe2-5934-45f2-b9f5-937467a93530.PNG)

-> **3 x 3 convolution filter만 사용!**

* 왜 3 x 3을 이용했을까?

![vgg](https://user-images.githubusercontent.com/59636424/128982744-a6f9c2b5-6aed-454e-8006-15bcc3c4a169.PNG)

      보통은 convolution이 커짐으로써 고려되는 input의 크기가 커진다.
      
      3 x 3 convolution을 2번 했다면, 사실상 마지막 layer의 하나의 값은 input layer의 5 x 5 pixel 값이 합쳐진 것과 같다!
      
      두 개의 파라미터 갯수를 비교하면 왼쪽은, 3 x 3 x 128 x 128 + 3 x 3 x 128 x 128 = 294912이고 오른쪽은, 5 x 5 x 128 x 128 = 409600개가 나온다.
      
      따라서 파라미터 개수가 이러한 차이가 나므로 3 x 3을 이용했다!
      

-> **레이어의 깊이에 따른 모델의 성능 이기 때문에 이미지 사이즈를 최대한 유지시키기 위해 3 x 3  filter size를 사용한것이다. **

~~~
( 3 x 3 보다 큰 필터를 사용하면 feature map의 크기가 작아지므로) (ex. imagesize = 224, filtersize=3, stride=1, padding=1  -> (image_size - filtersize + (2 x padding) / stride) + 1 = (224 - 3 + (2 x 1) / 1) + 1 = 224)
~~~


### GoogleNet

![google](https://user-images.githubusercontent.com/59636424/128984011-bb4765a8-33a8-4b1d-900d-0f52ba8b3de2.PNG)

-> 비슷한 network가 반복된다. => 네트워크 안에서 네트워크가 일어난다해서 네트워킹 네트워크(NIN)구조라고 한다.

-> **inception blocks을 재활용했다.**

* Inception blocks이란 뭘까?

![inc](https://user-images.githubusercontent.com/59636424/128984392-ad04eeaa-c4d3-4254-af66-4cd1c16631b5.PNG)

      하나의 입력이 들어오면 여러 개로 퍼졌다가 다시 합쳐진다.
      
      3 x 3 conv 전에 1 x 1 conv이 들어가고, 5 x 5 conv 전에 1 x 1 conv이 들어간다. => conv filter 전에 1 x 1 conv이 들어간다!
      
* **Inception block이 왜 중요한가?**

      하나의 입력에 대해서 여러 개의 receptive field를 갖는 filter를 거치고 여러 개의 반응들을 concatenate하는 효과가 있다.
      
      하지만! 1 x 1 conv로 파라미터를 줄이게 된다!

* **왜 1 x 1 conv가 들어감으로써 파라미터가 줄어드는가?**

![zzzzzzzzzzzzz](https://user-images.githubusercontent.com/59636424/128985541-335d4a7a-fa63-4fc0-a697-2bdde48e446e.PNG)

      1 x 1 conv은 채널 방향으로 차원을 줄인다.
      
      왼쪽 사진의 파라미터는 3 x 3 x 128 x 128 = 147456개이다.
      
      오른쪽 사진은 128을 32로 채널을 줄였다. (special demension은 그대로) -> 이렇게 만든 것은 4096+36864=40960개이다.
      (channel 방향으로 dimension 줄이기)
      
      두 개는 입력과 출력은 같다! -> 하지만 오른쪽 사진이 파라미터 수가 줄었다!!

* **파라미터 갯수는 GoogleNet<AlexNet<VGGNet순으로 많다.**
      
   뒤의 Dense layer를 줄이고 AlexNet은 11 x 11 파라미터를 줄이고 1 x 1 convolution filter dimension을 줄이기 때문에 이런 효과가 생겼다.


### ResNet

![ccc](https://user-images.githubusercontent.com/59636424/128986234-f0a9e0ea-0fc9-4691-ae9e-a529963ac6f5.PNG)

-> training error가 더 작음에도 불구하고 test error가 더 큰 경우는 학습이 안 되는 것이다.

-> 56 layer자체가 아무리 잘해도 20 layer가 더 잘 된다.

* **앞에서의 오류로 identity map을 추가하게 된다.**

![zzz](https://user-images.githubusercontent.com/59636424/128986785-a14ba53a-eed0-4322-b143-e019a88352dd.PNG)

      skip connection이란 입력(x)이 올라오게 되면 x를 neural network 출력값(1단 짜리 convolution layer)에 더해준다.
      
      -> 차이만 학습한다! (x에 f(x)를 더했으므로 실제 f(x) 학습하는 것은 차이만 학습한다.)
      
      -> 그렇게 해서 layer가 많아도 학습시킬 수 있게 되었다.

---

![캡처](https://user-images.githubusercontent.com/59636424/128987361-9e45a18c-83c0-4f72-b944-8115a8040071.PNG)

* simple Shortcut

: 앞에서 말한 identity map 방법과 동일하다.

* Projected Shortcut

: 더하려면 차원이 같아야 하는데, 다를수도 있으므로 1 x 1 convolution으로 channel을 바꿔준다.

=> 보통은 simple shortcut을 자주 사용!

* **Batch normalization이 convolution 뒤에 일어난다!**

---

* Bottleneck 구조

![채널차원](https://user-images.githubusercontent.com/59636424/128987823-7283153b-4036-488a-b0f4-670167f2d916.PNG)

: 3 x 3 conv 하기 전에 input channel을 줄이고 그 다음에 input channel을 늘리기 위해 1 x 1 conv가 1번 더 들어간다.

=> 궁극적으로 원하는 채널 차원을 맞춘다.

---

**1 x 1 conv로 채널을 줄이고 줄인 채널에 3 x 3 conv or 5 x 5 conv해서 receptive field를 키워 다시 1 x 1 conv으로 원하는 채널을 맞출 수 있다는게 핵심**

---

### DenseNet

![ㅊㄴㅊ](https://user-images.githubusercontent.com/59636424/128992203-45db1a67-9004-4c7b-8f1c-00e79a1f31c0.PNG)

: 더하지 말고 concatenate를 하자는 것은 DesNet이다.

=> 하지만 점점 채널이 커진다! => convolution filter map이 커지므로 parameter 수가 엄청 커진다.

=> 그러므로 중간에 1번씩 채널을 줄어야한다. -> 1 x 1 convolution을 해준다!

=> **따라서 filter map 키우고 마지막에 batch norm과 1 x 1 conv를 통해 conv filter size를 확 줄인다. => 그 다음에 Dense layer로 늘리고를 반복한다.**

---

~~~
VGGNet: receptable field 늘린 입장에서 3 x 3 block을 쓰는 것이 좋다.

GoogleNet: 1 x 1 convolution

ResNet: skip-connection

DenseNet: concatenation
~~~

---

## Computer Vision Applications

### Semantic Segmentation

![semantic](https://user-images.githubusercontent.com/59636424/128995365-608d7377-1a91-4788-85bb-a7835712cd3b.PNG)

: 어떤 이미지가 있을 때, 픽셀마다 분류하는 것이다.

=> **자율 주행에 많이 사용된다!** => 카메라는 사람인지 인도인지 자동차인지 다 분류해야하므로

### Fully Convolutional Network

![fc](https://user-images.githubusercontent.com/59636424/128995996-a9f0d9fe-ca4f-445a-aa5f-a32a23fd0c71.PNG)

: Dense layer를 없애는 과정을 convolutionalization이라고 한다.

![cccccccc](https://user-images.githubusercontent.com/59636424/128997819-65f58805-5914-4e0d-90d1-e4d5c4039fa7.PNG)

(오른쪽 과정을 진행하면, Convolutionalization)

=> 원래 CNN과 fully convolutional network와 파라미터 수가 같다. (입출력이)


* **왜 convolutionalizaion을 할까?**

      fully convolutional network가 가지는 가장 큰 특징은 input 차원은 독립적이다.
      
      -> output이 커지게 되면 비례해서 뒤에 network가 커진다. -> convolution이 가지는 shared parameter 때문이다!
      
      -> heatmap과 같은 영향!
      
* FCN

: 어떠한 input size(special demension)가 돌아갈 수 있지만 output size(special demension)도 역시 줄어둔다.

* Deconvolution(conv transpose)

![de](https://user-images.githubusercontent.com/59636424/129005154-bff3d3bd-8e47-4419-9e1b-c49d963ce320.PNG)

: convolution 역연산

=> 15 x 15를 30 x 30으로 늘리게 된다. => special demension을 키워준다.

-> convolution을 역으로 계산은 불가능한다. (복원)

* FCN 결과

![결과](https://user-images.githubusercontent.com/59636424/129005596-e6056de7-4a1c-49fa-a9f5-583c6c9c7e6c.PNG)

### Detection

#### R-CNN

![rcnn](https://user-images.githubusercontent.com/59636424/129005966-b60a0508-dcb7-480b-9e65-6afe8ce0c7ea.PNG)

: 이미지 안에서 patch를 뽑는 것이다.

=> 이미지 안에서 2000개의 region을 뽑는다.(랜덤으로) => 똑같은 크기로 맞춘다.(CNN 돌리기 위해) -> SVM으로 분류한다. 

* R-CNN 결과

![rcnn](https://user-images.githubusercontent.com/59636424/129006086-9be99784-0ff8-4e77-9be1-b71fee8295e7.PNG)

: 이미지 안에서 대략 어느 위치에 뭐가 있는지 나온다. -> detaction 문제를 풀고자 한다.

### SPPNet

![네ㅔㅜㄷㅅ](https://user-images.githubusercontent.com/59636424/129006877-782c9937-df62-47d8-bd29-ebfadc578ba3.PNG)

=> R-CNN의 문제는 이미지 안에서 2000개의 이미지를 뽑으면 모두 다, convolution을 돌려야 한다.

=> SPPNet에서는 이미지 안에서 CNN을 1번만 돌리자! => 이미지 안에서 box를 뽑고 이미지 전체에 대해서 convolutional feature map을 만든 다음, **region 별로, 뽑힌 bounding box 위치에 해당하는 convolution feature map의 tensor만 뽑자!**

=> **가장 큰 장점은 cnn을 1번만 돌려서 얻어지는 feature map 위에서 얻어지는 bounding box에 patch를 뜯어오는 것이다.**

### Fast R-CNN

![ㅊㅋㅊㅋㅊㅋㅊㅋㅊㅋㅊ](https://user-images.githubusercontent.com/59636424/129007403-d75c33b5-858a-4757-889f-b7d4503b9a1e.PNG)

=> 하지만 SPPNet도 느리다! => cnn 1번 돌리지만 bounding box에 해당하는 tensor를 여러 개 뜯어 와서 special pooling으로 하나의 vector를 만들고 분류해야하므로

-> SPPNet의 개념과는 유사!

**neural net을 통해서 모든 것(ROI pooling으로)을 다 할 수 있게 했다.**

### Faster R-CNN

![ㄻㄴㅅ ㄱ추ㅜ](https://user-images.githubusercontent.com/59636424/129007837-44fd4cbe-b548-48c0-9735-9e7f76bca663.PNG)

: bounding box를 뽑아내는 region proposal도 학습하자!

-> 이 때까지 임의로 뽑았기 때문이다. (Region Proposal Network가 핵심!)

#### Region Proposal Network

![ㅣㅣㅣㅣ](https://user-images.githubusercontent.com/59636424/129008140-83f2bce6-ad94-4de8-85a0-9c01cfe5abc7.PNG)

: 이미지에 특정 영역이 patch가 bounding box로 의미가 있는지 확인! (이 쯤에 물체가 있을 것 같다.)

-> ancor boxs: 미리 정해놓은 bounding box이다. (대충 어떤 물체의 크기가 있는지 알고 있는 것이다.)

![ㅊㅊㅊㅊㅊㅊㅊㅊ](https://user-images.githubusercontent.com/59636424/129008648-c596d233-a42a-434d-b652-3754bb37dde3.PNG)

: FCN(모든영역을 돌아가면 찍는다 -> 해당 영역에 과연 물체가 있는지 들고 있다)도 사용한다.

=> 9: pre-defined kernel size가 9개가 있다. (9개의 region size 중에 1개를 고른다.)

=> 4: bounding box를 얼마나 키우고 줄일지 (네모 박스에 width, height를 바꾸고 x와 y offset을 주므로 4개이다.) 

=> 2: 해당 박스가 쓸모 있는지 본다. (yes or no 2개 파라미터)

* Faster R-CNN 결과

![와유](https://user-images.githubusercontent.com/59636424/129009209-843f1d41-e10b-42bb-ac31-23162a4eacc6.PNG)

#### YOLO

![ㅌㅌㅌ](https://user-images.githubusercontent.com/59636424/129009549-ca51b686-f5cd-4ec6-8258-142a857cdaa1.PNG)

-> Faster R-CNN보다 빠르다. => 그냥 이미지 1개에서 바로 output이 나오므로

-> **1번에 bounding boxes를 바로 뽑는다. => Faster R-CNN은 따로 bounding box들을 선정했다.**

~~~
동작 방법

1. 이미지가 들어오면 S x S grid로 나누게 된다. (찾고 싶은 물체 중앙이 해당 grid 안에 들어가면 해당 물체에 대한 bounding box와 해당 물체가 무엇인지 같이 예측)

2. 각각의 셀은 B(보통 5)개이다. -> box가 실제로 쓸모 있는지 예상도 같이한다. => 또 동시에 중점에 있는 object가 어디 class인지 예측한다.

3. 2개의 정보를 취합하면 박스가 어느 class인지 나온다. (S x S x (B*5+C) size tensor를 만든다.)
~~~

* YOLO 결과

![ㅛㅐㅣㅐ](https://user-images.githubusercontent.com/59636424/129010151-e5cc311c-97ea-4023-acae-8a5102a6d09e.PNG)



# 3. 과제 수행 과정 / 결과물 정리

## CNN을 pytorch로 구현하자!

### Convolution layer 구현!

: 우선 입력값 차원을 저장한다! 그리고 hidden layer의 차원과 함께 Conv2d로 kernel_size와 stride를 넣고 padding을 넣게 된다. **여기서! padding에는 kernel size를 2로 나눈 몫을 넣게 된다**

* 왜 padding size는 kernel size를 2로 나눈 것으로 하는 것일까?

      input convolution filter map에 special dimension이 출력해서 똑같이 나오게 하기 위해서다!
      
      -> 입력 크기가 한 line이 28로 받았으니 kernel_size 3을 적용하면 28-3+1로 26이 되는데 padding을 3//2=1을 하게 되면 padding은 양 옆에 지정한 padding_size만큼 붙으므로 양 옆에 1개씩 붙으면 26+2=28로 dimension이 똑같이 나오게 할 수 있다.
      

그리고 BatchNorm, relu activation, max pooling과 dropout을 거쳐 layer에 저장한다. 그리고 이전 차원(prev_cdim)은 계속 현재 차원(cdim)으로 갱신시킨다.

### Dense layer 구현!

: 우선, layer를 flatten을 통해 핀다! 그리고 핀 layer의 차원을 계산하고자 한다. 마지막 차원인 prev_cdim(64)와 각 입력 크기 x와 y에 2^(hidden layer 길이)만큼 나눈 값을 곱한다.

* 왜 입력 크기에 2^(hidden layer 길이)만큼 나누는가?

      Convolution layer 구성 시, maxpooling을 이용했기에 layer 한 개씩 지나갈 때마다 반 개 씩 줄어드므로 이렇게 나눴다!
      
      => 28 x 28 -> 14 x 14 x 32 -> 7 x 7 x 64 식이다.

위에서 구한 차원값을 prev_hdim에 저장하여 이전 차원 값(prev_dim)과 현재 차원 값(hdim)과 linear 해준다! 그리고 마지막에 구한 차원 값을 출력 차원 값과 linear로 layer에 추가시켜준다.

### 전체 layer 연결(Concatenate all layers)

: 우선, Sequetial()을 통해 순차적으로 연결시킬 것을 알리고 add_module을 통해 각 layer 이름을 정하면서 layer를 하나씩 넣어준다.

### parameter 초기화 함수

: conv2d인 경우, weight와 bias를 각각 kaming he normal로 zeros_로 초기화 시켜준다. batchnorm2d인 경우, weight를 1로 bias를 0으로 constant_로 초기화 시켜준다. linear인 경우 conv2d와 동일하다.

### forward 함수

: forward는 앞에서 전체 layer을 연결한 상태를 그대로 사용한다.

**나머지는 앞에서 실습한 mlp와 거의 동일하다!**

### 결과

![결과](https://user-images.githubusercontent.com/59636424/128965561-3e01e2ef-302a-42f2-9001-40ddc5975f37.PNG)

: 매우 예측이 정확함을 알 수 있다.
