# 1. 8월 11일 배운 것!

## 4. Convolution이란?!

![convo](https://user-images.githubusercontent.com/59636424/128952568-936700aa-49b2-4e84-b87a-a7d02f7578bc.PNG)

마지막 수식에서, I는 전체 이미지 공간이고 K는 적용하고자 하는 convolution filter이다!

![convolution계산](https://user-images.githubusercontent.com/59636424/128952902-1620dc94-8582-4072-ab4d-3fd0163d8e04.PNG)
(Convolution 계산법)

* **2D Convolution한다는 것은?!**

![ㅊㅊㅊㅊ](https://user-images.githubusercontent.com/59636424/128953029-a006323b-1ac9-4bd8-8f3f-f1dca0e51c24.PNG)

: 해당 Convolution filter모양을 해당 이미지에 찍는다!

=> 적용 filter 모양에 따라 Convolution output이 위에 3개가 된다!

* RGB Image Convolution

![채ㅜㅊㅊ](https://user-images.githubusercontent.com/59636424/128953545-31364be4-15f9-47ca-b1ef-b3db86d1906c.PNG)

: filter 크기는 항상 같다! => 5 x 5 filter를 한다는 말은 5 x 5 x 3 filter/kernel을 사용한다는 말이다!

* Convolution filter channel 갯수에 맞게 output도 똑같은 갯수로 나온다!

![ㅇㅇㅇㅇㅇㅇㅇㅇㅇㅇㅇㅇㅇㅇㅇㅇ](https://user-images.githubusercontent.com/59636424/128953999-061592da-6448-4cce-8d1a-aa9787caa13c.PNG)

: 1번 Convolution을 거치고 나면 nonlinear acitvation이 들어가게 된다. => 여기서는 Relu가 들어간다. (4개 convolution filter가 필요)

---

### Convolution Neural Networks

![좀ㅅ소](https://user-images.githubusercontent.com/59636424/128954229-a0ca8a7c-1bf8-48be-b31d-ff01367ff83e.PNG)

* convolution layer: counvolution filter로 훑어서 값을 얻어내는 것 (feature extraction)

* polling layer: average polling 등 (feature extraction)

* fully connected layer: 마지막에 다 합쳐서 최종적 결과값을 내는 것 (decision making) => 요새 최소화나 없애는 추세!

* **fully connected layer를 왜 없애는 추세인가?**

: 내가 학습시키고자 하는 파라미터 숫자가 늘어나면 학습이 어렵고 generalization performance(학습에서 얻어진 결과가 다른 테스트 데이터가 얼마나 동작할지)가 떨어진다!

### Stride

![ㄴㅅ걍ㄷ](https://user-images.githubusercontent.com/59636424/128954727-dcd60d24-45c9-4f23-8a6f-c96b532806d5.PNG)

: convolution filter를 얼마나 자주 찍을지를 말함

### Padding

![ㅊㅊㅊㅊㅊㅊㅊㅊㅊㅊㅊㅊㅊ](https://user-images.githubusercontent.com/59636424/128954915-20464cf8-d5ec-4146-b86d-51b567148b26.PNG)

: zero padding을 하게 되면 위의 그림과 같이 spacial demension이 같아진다!

### Convolution Arithmetic

![계산](https://user-images.githubusercontent.com/59636424/128955189-1cc189b9-64cf-42a5-935e-e39e6cdf2502.PNG)

-> channel 64이므로 convolution filter가 64개 필요하므로 3 x 3 x 128 x 64이다.

![믿ㅌ둣 게산](https://user-images.githubusercontent.com/59636424/128955490-d022ad4f-0a1c-4e09-8612-5e72cc488ba9.PNG)

-> 2가 곱해진 이유는 96 channel짜리 filter map을 만들어야하는데 gpu 메모리가 크지 않아서 맞추기 위해 48짜리를 2개 만들었다.

![좀ㅅ'](https://user-images.githubusercontent.com/59636424/128956834-1e57932d-b7f4-4b22-a393-9a4fe6eab388.PNG)

* Dense layer(fully connected layer) 차원은 input 파라미터 개수(neural net 개수)와 output neural net 개수를 곱한 것 만큼이다.

* Dense layer 파라미터 개수가 convolution layer에 비해 엄청 많다.

    * 왜?: convolution operator가 각각의 하나의 커널이 모든 위치에 대해서 동일하게 적용되기 때문이다.

    * convolution operator는 shared parameter이다.
    
    * 그래서 파라미터를 줄이기 위해, fully connected layer를 줄이려고 한다.

~~~
첫 번째 layer는 입력 이미지는 244 x 244이다!
한 개의 kernel은 11 x 11 x 3이다.


두 번째 layer는 전의 kernel size는 5 x 5 x 48 kernel size이고 channel은 128로 늘었으므로 5 x 5 x 48 x 128 * 2 파라미터 개수를 가지고 있다.

세 번째 layer는 전의 kernel size는 3 x 3 x 128 kernel size가 2개이고 channel은 192이므로 3 x 3 x 128 x 2 x 192 x 2 파라미터 개수를 가지고 있다.

네 번째 layer는 전의 kernel size는 3 x 3 x 192 kernel size이고 channel은 192이므로 3 x 3 x 192 x 192 x 2 파라미터 개수를 가지고 있다.

마지막 layer는 전의 kernel size는 3 x 3 x 192 kernel size이고 channel은 128이므로 3 x 3 x 192 x 128 x 2 파라미터 개수를 가지고 있다.

첫 번째 Dense layer은 전의 kernel size는 13 x 13 x 128 kernel size이고 channel은 2048 이므로 13 x 13 x 128 x 2 x 2048 x 2 파라미터 개수를 가지고 있다.

두 번째 Dense layer는 input 파라미터 개수인 2048 x 2 와 현재 channel은 2048 size이므로 2048 x 2 x 2048 x 2 파라미터 개수를 가지고 있다.

마지막은 2048 x 2 input 파라미터 개수와 channel size는 1000이므로 2048 x 2 x 1000 파라미터 개수를 가지고 있다.
~~~

### 1 x 1 convolution

![ㅋㅋㅋㅋㅋㅋㅋㅋ](https://user-images.githubusercontent.com/59636424/128957031-15333341-a9c6-4cca-a097-9d6b4bdc7fae.PNG)

: 한 픽셀만 보는 것이고 channel 방향으로 줄인다.

=> 차원 줄이기 위해서 사용한다!

    * 1 x 1 convolution context dimension은 channel을 말한다.
    
    * convolution layer를 깊게 쌓으면서 파라미터 숫자를 줄일 수 있다. => bottleneck architecture이다!


# 3. 과제 수행 과정 / 결과물 정리

## CNN을 pytorch로 구현하자!

### Convolution layer 구현!

: 우선 입력값 차원을 저장한다! 그리고 hidden layer의 차원과 함께 Conv2d로 kernel_size와 stride를 넣고 padding을 넣게 된다. **여기서! padding에는 kernel size를 2로 나눈 몫을 넣게 된다**

* 왜 padding size는 kernel size를 2로 나눈 것으로 하는 것일까?

      input convolution filter map에 special dimension이 출력해서 똑같이 나오게 하기 위해서다!
      
      -> 입력 크기가 한 line이 28로 받았으니 kernel_size 3을 적용하면 28-3+1로 26이 되는데 padding을 3//2=1을 하게 되면 padding은 양 옆에 지정한 padding_size만큼 붙으므로 양 옆에 1개씩 붙으면 26+2=28로 dimension이 똑같이 나오게 할 수 있다.
      

그리고 BatchNorm, relu activation, max pooling과 dropout을 거쳐 layer에 저장한다. 그리고 이전 차원(prev_cdim)은 계속 현재 차원(cdim)으로 갱신시킨다.

### Dense layer 구현!

: 우선, layer를 flatten을 통해 핀다! 그리고 핀 layer의 차원을 계산하고자 한다. 마지막 차원인 prev_cdim(64)와 각 입력 크기 x와 y에 2^(hidden layer 길이)만큼 나눈 값을 곱한다.

* 왜 입력 크기에 2^(hidden layer 길이)만큼 나누는가?

      Convolution layer 구성 시, maxpooling을 이용했기에 layer 한 개씩 지나갈 때마다 반 개 씩 줄어드므로 이렇게 나눴다!
      
      => 28 x 28 -> 14 x 14 x 32 -> 7 x 7 x 64 식이다.

위에서 구한 차원값을 prev_hdim에 저장하여 이전 차원 값(prev_dim)과 현재 차원 값(hdim)과 linear 해준다! 그리고 마지막에 구한 차원 값을 출력 차원 값과 linear로 layer에 추가시켜준다.

### 전체 layer 연결(Concatenate all layers)

: 우선, Sequetial()을 통해 순차적으로 연결시킬 것을 알리고 add_module을 통해 각 layer 이름을 정하면서 layer를 하나씩 넣어준다.

### parameter 초기화 함수

: conv2d인 경우, weight와 bias를 각각 kaming he normal로 zeros_로 초기화 시켜준다. batchnorm2d인 경우, weight를 1로 bias를 0으로 constant_로 초기화 시켜준다. linear인 경우 conv2d와 동일하다.

### forward 함수

: forward는 앞에서 전체 layer을 연결한 상태를 그대로 사용한다.

**나머지는 앞에서 실습한 mlp와 거의 동일하다!

### 결과

![결과](https://user-images.githubusercontent.com/59636424/128965561-3e01e2ef-302a-42f2-9001-40ddc5975f37.PNG)

: 매우 예측이 정확함을 알 수 있다.
