# 1. 8월 11일 배운 것!

## 4. Convolution이란?!

![convo](https://user-images.githubusercontent.com/59636424/128952568-936700aa-49b2-4e84-b87a-a7d02f7578bc.PNG)

마지막 수식에서, I는 전체 이미지 공간이고 K는 적용하고자 하는 convolution filter이다!

![convolution계산](https://user-images.githubusercontent.com/59636424/128952902-1620dc94-8582-4072-ab4d-3fd0163d8e04.PNG)
(Convolution 계산법)

* **2D Convolution한다는 것은?!**

![ㅊㅊㅊㅊ](https://user-images.githubusercontent.com/59636424/128953029-a006323b-1ac9-4bd8-8f3f-f1dca0e51c24.PNG)

: 해당 Convolution filter모양을 해당 이미지에 찍는다!

=> 적용 filter 모양에 따라 Convolution output이 위에 3개가 된다!

* RGB Image Convolution

![채ㅜㅊㅊ](https://user-images.githubusercontent.com/59636424/128953545-31364be4-15f9-47ca-b1ef-b3db86d1906c.PNG)

: filter 크기는 항상 같다! => 5 x 5 filter를 한다는 말은 5 x 5 x 3 filter/kernel을 사용한다는 말이다!

* Convolution filter channel 갯수에 맞게 output도 똑같은 갯수로 나온다!

![ㅇㅇㅇㅇㅇㅇㅇㅇㅇㅇㅇㅇㅇㅇㅇㅇ](https://user-images.githubusercontent.com/59636424/128953999-061592da-6448-4cce-8d1a-aa9787caa13c.PNG)

: 1번 Convolution을 거치고 나면 nonlinear acitvation이 들어가게 된다. => 여기서는 Relu가 들어간다. (4개 convolution filter가 필요)

---

### Convolution Neural Networks

![좀ㅅ소](https://user-images.githubusercontent.com/59636424/128954229-a0ca8a7c-1bf8-48be-b31d-ff01367ff83e.PNG)

* convolution layer: counvolution filter로 훑어서 값을 얻어내는 것 (feature extraction)

* polling layer: average polling 등 (feature extraction)

* fully connected layer: 마지막에 다 합쳐서 최종적 결과값을 내는 것 (decision making) => 요새 최소화나 없애는 추세!

* **fully connected layer를 왜 없애는 추세인가?**

: 내가 학습시키고자 하는 파라미터 숫자가 늘어나면 학습이 어렵고 generalization performance(학습에서 얻어진 결과가 다른 테스트 데이터가 얼마나 동작할지)가 떨어진다!

### Stride

![ㄴㅅ걍ㄷ](https://user-images.githubusercontent.com/59636424/128954727-dcd60d24-45c9-4f23-8a6f-c96b532806d5.PNG)

: convolution filter를 얼마나 자주 찍을지를 말함

### Padding

![ㅊㅊㅊㅊㅊㅊㅊㅊㅊㅊㅊㅊㅊ](https://user-images.githubusercontent.com/59636424/128954915-20464cf8-d5ec-4146-b86d-51b567148b26.PNG)

: zero padding을 하게 되면 위의 그림과 같이 spacial demension이 같아진다!

### Convolution Arithmetic

![계산](https://user-images.githubusercontent.com/59636424/128955189-1cc189b9-64cf-42a5-935e-e39e6cdf2502.PNG)

-> channel 64이므로 convolution filter가 64개 필요하므로 3 x 3 x 128 x 64이다.

![믿ㅌ둣 게산](https://user-images.githubusercontent.com/59636424/128955490-d022ad4f-0a1c-4e09-8612-5e72cc488ba9.PNG)

-> 2가 곱해진 이유는 96 channel짜리 filter map을 만들어야하는데 gpu 메모리가 크지 않아서 맞추기 위해 48짜리를 2개 만들었다.

![좀ㅅ'](https://user-images.githubusercontent.com/59636424/128956834-1e57932d-b7f4-4b22-a393-9a4fe6eab388.PNG)

* Dense layer(fully connected layer) 차원은 input 파라미터 개수(neural net 개수)와 output neural net 개수를 곱한 것 만큼이다.

* Dense layer 파라미터 개수가 convolution layer에 비해 엄청 많다.

    * 왜?: convolution operator가 각각의 하나의 커널이 모든 위치에 대해서 동일하게 적용되기 때문이다.

    * convolution operator는 shared parameter이다.
    
    * 그래서 파라미터를 줄이기 위해, fully connected layer를 줄이려고 한다.

~~~
첫 번째 layer는 입력 이미지는 244 x 244이다!
한 개의 kernel은 11 x 11 x 3이다.


두 번째 layer는 전의 kernel size는 5 x 5 x 48 kernel size이고 channel은 128로 늘었으므로 5 x 5 x 48 x 128 * 2 파라미터 개수를 가지고 있다.

세 번째 layer는 전의 kernel size는 3 x 3 x 128 kernel size가 2개이고 channel은 192이므로 3 x 3 x 128 x 2 x 192 x 2 파라미터 개수를 가지고 있다.

네 번째 layer는 전의 kernel size는 3 x 3 x 192 kernel size이고 channel은 192이므로 3 x 3 x 192 x 192 x 2 파라미터 개수를 가지고 있다.

마지막 layer는 전의 kernel size는 3 x 3 x 192 kernel size이고 channel은 128이므로 3 x 3 x 192 x 128 x 2 파라미터 개수를 가지고 있다.

첫 번째 Dense layer은 전의 kernel size는 13 x 13 x 128 kernel size이고 channel은 2048 이므로 13 x 13 x 128 x 2 x 2048 x 2 파라미터 개수를 가지고 있다.

두 번째 Dense layer는 input 파라미터 개수인 2048 x 2 와 현재 channel은 2048 size이므로 2048 x 2 x 2048 x 2 파라미터 개수를 가지고 있다.

마지막은 2048 x 2 input 파라미터 개수와 channel size는 1000이므로 2048 x 2 x 1000 파라미터 개수를 가지고 있다.
~~~

### 1 x 1 convolution

![ㅋㅋㅋㅋㅋㅋㅋㅋ](https://user-images.githubusercontent.com/59636424/128957031-15333341-a9c6-4cca-a097-9d6b4bdc7fae.PNG)

: 한 픽셀만 보는 것이고 channel 방향으로 줄인다.

=> 차원 줄이기 위해서 사용한다!

    * 1 x 1 convolution context dimension은 channel을 말한다.
    
    * convolution layer를 깊게 쌓으면서 파라미터 숫자를 줄일 수 있다. => bottleneck architecture이다!
    
 
## Modern CNN

### AlexNet

![alexnet](https://user-images.githubusercontent.com/59636424/128981491-96abae62-c41f-4dbf-81e5-b76a234e3b1a.PNG)

: 네트워크가 2개로 나눠어져 있다.(8개의 layer) => gpu를 최대한 활용하기 위해서!

- input을 11 x 11을 사용하는 것은 현명하지 않다! => convolution kernel이 볼 수 있는 이미지 영역은 커지지만 상대적으로 더 많은 파라미터가 필요하다!

#### key ideas

1. RELU activation 사용 -> nonlinear이고 기울기가 1이기에 원하는 gradient가 사라질 성질들이 많이 없다.

      linear model의 좋은 성질을 가지고 있다.
      
      gradient descent로 최적화 용이
      
      **gradient 소실 문제 극복** -> 다른 것들은 0에서 많이 벗어나게되면 기울기가 굉장히 0에 가까워지는데 relu는 그렇지 않다.

2. 2 GPU 사용

3. local response normalization(어떠한 입력공간에서 response가 많이 나오는 몇 개를 죽인다.) : 복수의 feature map 간에 정규화하는 방법이다.

4. Overlapping pooling(3x3 영역을 2픽셀 단위로 pooling 하여 조금씩 겹치는 부분이 있도록 pooling하여 overfitting 현상 개선)

5. Data augmentation

6. Dropout: 뉴런 중 몇 개를 0으로 바꾼다.

### VGGNet

![rrr](https://user-images.githubusercontent.com/59636424/128982537-7f68abe2-5934-45f2-b9f5-937467a93530.PNG)

-> **3 x 3 convolution filter만 사용!**

* 왜 3 x 3을 이용했을까?

![vgg](https://user-images.githubusercontent.com/59636424/128982744-a6f9c2b5-6aed-454e-8006-15bcc3c4a169.PNG)

      보통은 convolution이 커짐으로써 고려되는 input의 크기가 커진다.
      
      3 x 3 convolution을 2번 했다면, 사실상 마지막 layer의 하나의 값은 input layer의 5 x 5 pixel 값이 합쳐진 것과 같다!
      
      두 개의 파라미터 갯수를 비교하면 왼쪽은, 3 x 3 x 128 x 128 + 3 x 3 x 128 x 128 = 294912이고 오른쪽은, 5 x 5 x 128 x 128 = 409600개가 나온다.
      
      따라서 파라미터 개수가 이러한 차이가 나므로 3 x 3을 이용했다!
      

-> **레이어의 깊이에 따른 모델의 성능 이기 때문에 이미지 사이즈를 최대한 유지시키기 위해 3 x 3  filter size를 사용한것이다. **

~~~
( 3 x 3 보다 큰 필터를 사용하면 feature map의 크기가 작아지므로) (ex. imagesize = 224, filtersize=3, stride=1, padding=1  -> (image_size - filtersize + (2 x padding) / stride) + 1 = (224 - 3 + (2 x 1) / 1) + 1 = 224)
~~~


### GoogleNet

![google](https://user-images.githubusercontent.com/59636424/128984011-bb4765a8-33a8-4b1d-900d-0f52ba8b3de2.PNG)

-> 비슷한 network가 반복된다. => 네트워크 안에서 네트워크가 일어난다해서 네트워킹 네트워크(NIN)구조라고 한다.

-> **inception blocks을 재활용했다.**

* Inception blocks이란 뭘까?

![inc](https://user-images.githubusercontent.com/59636424/128984392-ad04eeaa-c4d3-4254-af66-4cd1c16631b5.PNG)

      하나의 입력이 들어오면 여러 개로 퍼졌다가 다시 합쳐진다.
      
      3 x 3 conv 전에 1 x 1 conv이 들어가고, 5 x 5 conv 전에 1 x 1 conv이 들어간다. => conv filter 전에 1 x 1 conv이 들어간다!
      
* **Inception block이 왜 중요한가?**

      하나의 입력에 대해서 여러 개의 receptive field를 갖는 filter를 거치고 여러 개의 반응들을 concatenate하는 효과가 있다.
      
      하지만! 1 x 1 conv로 파라미터를 줄이게 된다!

* **왜 1 x 1 conv가 들어감으로써 파라미터가 줄어드는가?**

![zzzzzzzzzzzzz](https://user-images.githubusercontent.com/59636424/128985541-335d4a7a-fa63-4fc0-a697-2bdde48e446e.PNG)

      1 x 1 conv은 채널 방향으로 차원을 줄인다.
      
      왼쪽 사진의 파라미터는 3 x 3 x 128 x 128 = 147456개이다.
      
      오른쪽 사진은 128을 32로 채널을 줄였다. (special demension은 그대로) -> 이렇게 만든 것은 4096+36864=40960개이다.
      (channel 방향으로 dimension 줄이기)
      
      두 개는 입력과 출력은 같다! -> 하지만 오른쪽 사진이 파라미터 수가 줄었다!!

* **파라미터 갯수는 GoogleNet<AlexNet<VGGNet순으로 많다.**
      
   뒤의 Dense layer를 줄이고 AlexNet은 11 x 11 파라미터를 줄이고 1 x 1 convolution filter dimension을 줄이기 때문에 이런 효과가 생겼다.


### ResNet

![ccc](https://user-images.githubusercontent.com/59636424/128986234-f0a9e0ea-0fc9-4691-ae9e-a529963ac6f5.PNG)

-> training error가 더 작음에도 불구하고 test error가 더 큰 경우는 학습이 안 되는 것이다.

-> 56 layer자체가 아무리 잘해도 20 layer가 더 잘 된다.

* **앞에서의 오류로 identity map을 추가하게 된다.**

![zzz](https://user-images.githubusercontent.com/59636424/128986785-a14ba53a-eed0-4322-b143-e019a88352dd.PNG)

      skip connection이란 입력(x)이 올라오게 되면 x를 neural network 출력값(1단 짜리 convolution layer)에 더해준다.
      
      -> 차이만 학습한다! (x에 f(x)를 더했으므로 실제 f(x) 학습하는 것은 차이만 학습한다.)
      
      -> 그렇게 해서 layer가 많아도 학습시킬 수 있게 되었다.

---

![캡처](https://user-images.githubusercontent.com/59636424/128987361-9e45a18c-83c0-4f72-b944-8115a8040071.PNG)

* simple Shortcut

: 앞에서 말한 identity map 방법과 동일하다.

* Projected Shortcut

: 더하려면 차원이 같아야 하는데, 다를수도 있으므로 1 x 1 convolution으로 channel을 바꿔준다.

=> 보통은 simple shortcut을 자주 사용!

* **Batch normalization이 convolution 뒤에 일어난다!**

---

* Bottleneck 구조

![채널차원](https://user-images.githubusercontent.com/59636424/128987823-7283153b-4036-488a-b0f4-670167f2d916.PNG)

: 3 x 3 conv 하기 전에 input channel을 줄이고 그 다음에 input channel을 늘리기 위해 1 x 1 conv가 1번 더 들어간다.

=> 궁극적으로 원하는 채널 차원을 맞춘다.

---

**1 x 1 conv로 채널을 줄이고 줄인 채널에 3 x 3 conv or 5 x 5 conv해서 receptive field를 키워 다시 1 x 1 conv으로 원하는 채널을 맞출 수 있다는게 핵심**

---

### DenseNet

![ㅊㄴㅊ](https://user-images.githubusercontent.com/59636424/128992203-45db1a67-9004-4c7b-8f1c-00e79a1f31c0.PNG)

: 더하지 말고 concatenate를 하자는 것은 DesNet이다.

=> 하지만 점점 채널이 커진다! => convolution filter map이 커지므로 parameter 수가 엄청 커진다.

=> 그러므로 중간에 1번씩 채널을 줄어야한다. -> 1 x 1 convolution을 해준다!

=> **따라서 filter map 키우고 마지막에 batch norm과 1 x 1 conv를 통해 conv filter size를 확 줄인다. => 그 다음에 Dense layer로 늘리고를 반복한다.**

---

~~~
VGGNet: receptable field 늘린 입장에서 3 x 3 block을 쓰는 것이 좋다.

GoogleNet: 1 x 1 convolution

ResNet: skip-connection

DenseNet: concatenation
~~~

---

# 3. 과제 수행 과정 / 결과물 정리

## CNN을 pytorch로 구현하자!

### Convolution layer 구현!

: 우선 입력값 차원을 저장한다! 그리고 hidden layer의 차원과 함께 Conv2d로 kernel_size와 stride를 넣고 padding을 넣게 된다. **여기서! padding에는 kernel size를 2로 나눈 몫을 넣게 된다**

* 왜 padding size는 kernel size를 2로 나눈 것으로 하는 것일까?

      input convolution filter map에 special dimension이 출력해서 똑같이 나오게 하기 위해서다!
      
      -> 입력 크기가 한 line이 28로 받았으니 kernel_size 3을 적용하면 28-3+1로 26이 되는데 padding을 3//2=1을 하게 되면 padding은 양 옆에 지정한 padding_size만큼 붙으므로 양 옆에 1개씩 붙으면 26+2=28로 dimension이 똑같이 나오게 할 수 있다.
      

그리고 BatchNorm, relu activation, max pooling과 dropout을 거쳐 layer에 저장한다. 그리고 이전 차원(prev_cdim)은 계속 현재 차원(cdim)으로 갱신시킨다.

### Dense layer 구현!

: 우선, layer를 flatten을 통해 핀다! 그리고 핀 layer의 차원을 계산하고자 한다. 마지막 차원인 prev_cdim(64)와 각 입력 크기 x와 y에 2^(hidden layer 길이)만큼 나눈 값을 곱한다.

* 왜 입력 크기에 2^(hidden layer 길이)만큼 나누는가?

      Convolution layer 구성 시, maxpooling을 이용했기에 layer 한 개씩 지나갈 때마다 반 개 씩 줄어드므로 이렇게 나눴다!
      
      => 28 x 28 -> 14 x 14 x 32 -> 7 x 7 x 64 식이다.

위에서 구한 차원값을 prev_hdim에 저장하여 이전 차원 값(prev_dim)과 현재 차원 값(hdim)과 linear 해준다! 그리고 마지막에 구한 차원 값을 출력 차원 값과 linear로 layer에 추가시켜준다.

### 전체 layer 연결(Concatenate all layers)

: 우선, Sequetial()을 통해 순차적으로 연결시킬 것을 알리고 add_module을 통해 각 layer 이름을 정하면서 layer를 하나씩 넣어준다.

### parameter 초기화 함수

: conv2d인 경우, weight와 bias를 각각 kaming he normal로 zeros_로 초기화 시켜준다. batchnorm2d인 경우, weight를 1로 bias를 0으로 constant_로 초기화 시켜준다. linear인 경우 conv2d와 동일하다.

### forward 함수

: forward는 앞에서 전체 layer을 연결한 상태를 그대로 사용한다.

**나머지는 앞에서 실습한 mlp와 거의 동일하다!**

### 결과

![결과](https://user-images.githubusercontent.com/59636424/128965561-3e01e2ef-302a-42f2-9001-40ddc5975f37.PNG)

: 매우 예측이 정확함을 알 수 있다.
