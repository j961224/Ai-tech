# 1. 8월 9일 공부한 것!

## 1. Historical Review [DL Basic]

### Introduction

- Artificial Inteligence: 사람의 지식을 모방하는 것이다!

- Machine Learning: 무언가를 학습하고자 할 때. 데이터를 가지고 학습한다.

- Deep Learning: 뉴럴 네트워크로 사용하는 세부적인 분야

---

* 딥러닝의 주요 요소

    data: 모델을 학습시키는 요소
    
    model: 데이터를 어떻게 transform 할지
    
    loss: model의 나쁨을 평가하는 함수
    
    알고리즘: loss를 최소화하는 파라미터를 적용하는 것
    
* Data

: 문제를 풀고자 하는 type에 따라 의존한다.

* Model

: 어떤 데이터를 가지고 모델의 성질에 따라 결과가 나오는 것이 다르다.

* loss

: 모델과 데이터가 정해져 있을 때, 모델을 어떻게 학습시킬지 정의한다.

    - Regression 문제: 뉴럴 네트워크의 출력값과 내가 맞추고자 하는 타겟점 사이의 제곱을 최소화 시키는 것 -> 제곱을 평균내서 줄인다!
    
    - classification 문제: 뉴럴 네트워크의 출력값과 내가 맞추고자 하는 라벨 데이터 사이의 cross entropy loss를 최소화한다.
    
    - Probabilistic 문제: 확률적 모델 활용 시, 갓에 대한 평균값, 분산 등과 같은 것은 이와 같은 관점을 사용한다.

![task](https://user-images.githubusercontent.com/59636424/128651831-6d09e19d-cac6-4e23-918e-ad0036e14259.PNG)

* 알고리즘 최적화
 
: Dropout, k-fold validation, Batch normalization을 활용해서 학습 데이터에만 아닌 다른 데이터도 잘 적용하는 것을 목적으로 한다.

### Historical Review

* AlexNet

: 5개의 컨볼루션 레이어와 3개의 full-connected 레이어로 구성되어있다.

![alexnet](https://user-images.githubusercontent.com/59636424/128652793-dd41247e-1734-4ad6-8528-ee7d9d7306fe.PNG)


* DQN

: 강화학습의 큰 축이자, state-action value Q값을 Deep Learning을 통해서 Approximate하는 방식이다.

![dqn](https://user-images.githubusercontent.com/59636424/128652914-6ead011f-8d5a-4c70-894e-6ffe59f0fecf.PNG)


* Encoder/Decoder

: 단어 연속이 주어졌을 때, 어떻게 잘 표현해서 원하는 단어 연속으로 만드는 게 목표

    Encoder와 Decoder: 단어 시퀀스를 어떤 벡터에 인코딩하고 다른 언어의 시퀀스를 만든다.
    
![ec-dc](https://user-images.githubusercontent.com/59636424/128652926-7e7e26c8-9959-4b1f-be04-12eb6cb79381.PNG)

    
* Adam Optimizer

: Adagrad, Adadelta, RMSprop 처럼 각 파라미터마다 다른 크기의 업데이트를 적용하는 방법이다.


* GAN

: 이미지를 어떻게 만들어낼 수 있을지를 본다. -> 네트워크가 generator와 discriminator를 만들어서 학습한다.

![gan](https://user-images.githubusercontent.com/59636424/128653152-cb338305-168e-4217-af04-b5468b66bfee.PNG)


* Residual Networks(ResNet)

: 어느정도 layer를 깊게 쌓으면 전에는 학습이 안 되었다. -> 이것으로 layer가 깊게 좀 쌓아도 적용이 된다.

![resnet](https://user-images.githubusercontent.com/59636424/128653174-20f7cd2a-7b37-4d28-b3a7-f0e3b7dd95d1.PNG)


* Transformer

: 기존의 seq2seq의 구조인 인코더-디코더를 따르면서도, 논문의 이름처럼 어텐션(Attention)만으로 구현한 모델

![transformer](https://user-images.githubusercontent.com/59636424/128653177-ebe220fe-3ae1-4103-a550-ccb3b522fd52.PNG)


* BERT

: Birdirection을이용한 encoder -> 굉장히 다양한 단어들을 이용해 pre-training을 하여 풀고자 하는 데이터 소수에 fine-tuning을 한다.

![bert](https://user-images.githubusercontent.com/59636424/128653180-a6210b80-89b8-43c6-be15-98d4ad03ea25.PNG)



* GPT-X

: 약간의 fine-tuning을 통해 시퀀스 모델을 만들 수 있다. -> 굉장히 많은 파라미터를 가지고 있다.

![gpt-x](https://user-images.githubusercontent.com/59636424/128653258-a34af977-0c79-4849-8690-4676ad1bb534.PNG)

: 간단히 설명하자면 GPT는 자동 회귀이다. GPT Transformer는 모든 토큰이 왼쪽의 컨텍스트에만 주의를 기울일 수 있는 제한된 자기주의를 사용한다. 입력은 순차적으로 제공되며 출력은 한 번에 한 단어씩 생성된다.

* self supervised learning

: 학습 데이터 외에, 라벨을 모르는 데이터를 사용하겠다.

![강화학습](https://user-images.githubusercontent.com/59636424/128653179-531ccd3b-1f70-4caf-b5d5-10d384bfd3c3.PNG)


## 2. 뉴럴 네트워크 - MLP (Multi-Layer Perceptron)

* 뉴럴 네트워크

: 비선형 연산이 반복적으로 일어나는 모델을 칭한다.


### Linear Neural Networks

* linear function

: 입력이 1차원이고 출력이 1차원이면, 선형 회귀(입력과 출력을 연결하는 모델 찾기 -> line 기울기, 절편, 2개의 파라미터를 찾는 문제)

![zz](https://user-images.githubusercontent.com/59636424/128653708-10b73e00-509a-4827-9026-c73990bb1e21.PNG)

: 뉴럴 네트워크 출력값과 나의 데이터의 차이를 줄이는 것이 중요하다!

![zzzzz](https://user-images.githubusercontent.com/59636424/128653848-76d6de45-78a6-48ac-a401-1e9e20f3bd35.PNG)

: loss function을 줄이는 것이 목표니, 파라미터가 어느 방향으로 움직여야 줄이는 것을 보기 위해서 각 파라미터에 미분한 방향에 역수방향으로 업데이트하면 loss가 최소화되는 지점으로 간다.

![z_z](https://user-images.githubusercontent.com/59636424/128653967-a96a3dd4-3ac4-4838-a7f5-01d67585b041.PNG)

: w와 b를 계속 업데이트하는 것이 gradient descent이다. -> 적절한 stepsize를 잡는 것이 중요하다!

![nonlinear](https://user-images.githubusercontent.com/59636424/128654097-b5f80dd0-7ef3-457e-b3e5-485a5cb17a33.PNG)

: Nonlinear transform으로 네트워크가 표현할 수 있는 정도를 최대화시킨다!

---

* Activation functions

![relu](https://user-images.githubusercontent.com/59636424/128654153-d3ec3f5c-30ec-4442-b5d7-14a1741c1a19.PNG)

: nonlinear transform을 하는 activation function들이다.


---

* **Multi-Layer Preceptron**

![mlp](https://user-images.githubusercontent.com/59636424/128654351-d7a2d61a-2f29-4ce3-9588-e446466c608b.PNG)


: 입력이 주어져있고 linear와 nonlinear 변화를 거쳐 hidden vector가 나와 hidden layer에서 다시 계산하는 한 칸짜리 이상의 hidden layer가 있는 것


* loss function

![loss function](https://user-images.githubusercontent.com/59636424/128654554-24ad27a3-31a8-4508-baf4-9c856d7e01ef.PNG)

-> 분류에서 d개 label를 갖는 문제를 풀 때, output이 나왔을 때, 제일 큰 숫자 index만 고려한다.

-> 분류는 크로스 엔트로피를 이용해서 손실(loss, cost) 함수를 정의한다. (두 랜덤변수 X, Y가 있고, 각각의 분포를 p, q)


(크로스 엔트로피와 분류: https://theeluwin.postype.com/post/6080524)


## 1-2. 시각화의 요소

### 정형 데이터

: csv, tsv파일로 한 줄에 한 item이 나오고 cell이 하나의 column이다.

=> 이러한 데이터는 통계적 특성과 관계 등을 시각화 한다.

### 시계열 데이터

: Time-series데이터로 정형 주가 등이 해당한다. -> 이는 정형 데이터고 음성, 비디오와 같은 비정형 데이터도 존재한다.

### 지도 데이터

: 지도 정보, 거리, 경로, 분포 등의 지도 정보를 단순화 시키는 경우이다.

### 관계 데이터

: graph, network라고 말하고 객체는 node, 관계로 link라고 말한다. 크기, 색, 수등으로 객체와의 가중치를 표현한다.

-> 엄청 많은 node들이 있어 어떻게 mapping이 중요하고, node 배치 시에 휴리스틱을 많이 사용한다.

### 계층적 데이터

: 회사 조직도, 가계도 등 계층이 있는 데이터로 네트워크 식으로도 가능하지만 tree 구조로 보여준다. -> tree, treemap, sunburst 등이 대표적이다.

### 데이터 종류

> * 수치형
> > * 연속형: 길이, 무게 등
> > * 이산형: 주사위 눈금, 사람 수 등
> * 범주형
> > * 명목형: 혈액형, 종교 등
> > * 순서형: 학년, 벌점, 등급 등

## 2. 시각화 이해하기

* mark: 점, 선, 면으로 이루어진 데이터 시각화

* channel: 각 마크를 변경시킬 수 있는 요소들을 말한다.

### 전주의적 속성

: 변경시킬 수 있는 속성 중에 주의를 주지 않아도 인지할 수 있는 요소

-> 시각적으로 바로 분리되어 보이는 것을 visual pop-out이다!


## 1-3. Python과 Matplotlib

* matplotlib: python으로 가장 기본적인 시각화 기법 (제일 범용적인 방법) -> 행렬 연산과 과학적 연산을 베이스로 한다!

### figure와 axes

: figure라는 큰 틀에 ax라는 subplot으로 그린다!

-> 서브플롯을 최쇠 1개 이상 추가해야 그릴 수 있다.

~~~
fig = plt.figure()
ax = fig.add_subplot()
plt.show()

#figure를 선언할 때, figsize로 가로, 세로 길이를 설정할 수 있다!
~~~

![zzz](https://user-images.githubusercontent.com/59636424/128662112-a6dd4ef2-1185-4923-9d78-a431c9285007.PNG)

~~~
fig = plt.figure()
ax = fig.add_subplot(121) # -> 세로축을 1개, 가로축을 2개로 나눴을 때, 첫 번째 칸에 넣어달라!
ax = fig.add_subplot(122)
plt.show()
~~~

![z1](https://user-images.githubusercontent.com/59636424/128662346-066f3968-0e39-4e16-ae24-6f0c69b5dd3c.png)


### plt로 그래프 그리기

~~~
fig = plt.figure()
ax = fig.add_subplot()

x = np.array([1,2,3])
plt.plot(x)
plt.show()
~~~

![zd](https://user-images.githubusercontent.com/59636424/128662547-ea992b6c-b8e9-4918-a85c-15634fb999b7.png)

### 서브플롯 객체 ax에 그리기!

: ax 객체에 직접 그리기!

* Python API: 순차적 방법

* 객체지향 API: 그래프에서 각 객체에 대해 직접적으로 수정하는 방법

### plot의 요소 알아보기

#### 한 서브플롯에서 여러 개 그리기

~~~
fig = plt.figure()
ax = fig.add_subplot(111) 
# 3개의 그래프 동시에 그리기
ax.plot([1, 1, 1]) # 파랑
ax.plot([1, 3, 2]) # 주황
ax.plot([3, 3, 3]) # 초록

plt.show()
~~~

![zzzzzzzzz](https://user-images.githubusercontent.com/59636424/128662700-e612f5e6-5aac-4235-b779-c37532660d88.png)

~~~
fig = plt.figure()
ax = fig.add_subplot(111) 

# 선그래프와 막대그래프 동시에 그리기
# 다른 종류의 그래프를 그린다면 생성이 바뀌지 않고 사진과 같이 같은 색깔로 나온다.
ax.plot([1, 2, 3], [1, 2, 3]) 
ax.bar([1, 2, 3], [1, 2, 3]) 

plt.show()
~~~

![zczczcc](https://user-images.githubusercontent.com/59636424/128662743-df2c5867-13f3-4c96-9931-b5030bcd288f.png)


#### 색상 지정하기

~~~
ax.plot([1, 1, 1], color='r') # red
ax.plot([3, 3, 3], color='#000000') # hex code (BLACK)
~~~

#### 텍스트 사용하기

~~~
# 그래프 제목을 선언할 수 있다.
ax.set_title('Basic Plot')
# ax.plot으로 그릴 그래프 설정 후에 legend 명시하면 범례를 추가할 수 있다.
ax.legend()
~~~

* ax에서 특정 데이터를 변경하는 경우 `.set_{}()` 형태의 메서드가 많이 사용한다!!

* `set`으로 세팅하는 정보들은 반대로 해당 정보를 받아오는 경우에는 `.get_{}()` 형태의 메서드를 사용한다!

~~~
ax.set_title('Basic Plot')
print(ax.get_title()) # Basic Plot -> 그래프의 제목을 받아왔다.
~~~

* 축이 적히는 범위나 위치는 'ticks'와 범위에 따른 텍스트는 'ticklabels'로 구분

~~~
ax.set_xticks([0, 1, 2]) # x축의 눈금 설정
~~~

![zzzzzzzzzzzzzz](https://user-images.githubusercontent.com/59636424/128663205-b307b5bc-cf32-4fa5-8884-e35618e5e3bd.png)

~~~
ax.set_xticks([0, 1, 2]) # x축의 눈금 설정
ax.set_xticklabels(['zero', 'one', 'two']) # -> x축을 0,1,2로 설정한 것을 zero, one, two로 이름을 바꾼다.
~~~

~~~
ax.text(x=1, y=2, s='This is Text') # (1,2)에 This is Text라는 글을 남긴다.
~~~


## 2-1. Bar plot 사용하기

* Bar plot: 직사각형 막대를 사용하여 데이터의 값을 표현하는 차트/그래프

* 수직: x축에 범주, y축에 값을 표기!

* 수평: y축에 범주, x축에 값을 표기!

-> 1개의 blot에 동시에 많은 bar를 보여주는게 좋다.

* stacked bar plot

: 2개 이상의 그룹을 쌓아서 표현하는 bar plot

![barbar](https://user-images.githubusercontent.com/59636424/128664676-362d5e7f-4365-4a4c-8dc3-cdbb25b852f5.PNG)

.bar()에서는 bottom 파라미터 사용

.barh()에서는 left 파라미터를 사용

* percentage Stacked Bar Chart: 각각의 범주에 대해서 퍼센트로 비율을 나타낸다.

* overlapped bar plot: 겹쳐서 그리기 -> 3개 이상의 그래프는 지양! (이 경우에는 bar plot 보다는 area plot이 좋다.)

* grouped bar plot: 그룹별 범주에 따른 bar를 이웃되게 배치하는 방법


### Principle of Proportion lnk

: 실제 값과 그에 표현되는 그래픽으로 표현되는 잉크 양은 비례한다.

-> x축의 시작은 0이다!

### 데이터 정렬하기

: sort_values(), sort_index()로 정렬이 가능하다.

### 적절한 공간 활용

: 가독성에 영향을 줄 수 있다.

### 복잡함과 단순함

: 무의미한 3D는 안 하는 것이 좋다.

* grid: 정확하게 어떤 값인지 보여주는 도구 -> 큰틀에서 비교하기에는 안 쓰는 것이 좋다.

### ETC

: 오차 막대를 추가하여 정보를 추가 가능하다!

* histogram: bar 사이에 gap이 0으로 만들어서 이웃된 값을 bar graph로 표현한다.


# 3. 과제 수행 과정 / 결과물 정리

## MLP를 통한 mnist 분류 문제!

* torch.utils.data.DataLoader로 batch_size만큼 데이터를 잘라서 shuffle을 통해 섞는다!

* lin_1과 lin_2를 연산을 할 수 있도록 lin_1에 입력차원과 히든차원, lin_2에는 히든차원과 출력차원을 저장한다.

* pytorch의 set_printoptions(precision=3)을 통해, 정밀도 자릿수를 3자리까지 인쇄하도록 설정한다.

* 설정한 모델이 아닌, 매우 기본적인 모델로 mnist를 예측했을 때, 정확도 0.115가 나오는 것을 알 수 있다.

* pytorch 코드 중, batch_in.view(-1,28 x 28).to(device)는 (batch size=256, 1, 28,28)크기의 tensor를 (batch_size=256,xdim)으로 변경해준다.

* 설정한 모델로 학습에서 update 부분은, 우선, zero_grad로 gradient를 reset을 해주고 loss_out을 backpropgate를 통해 weight에 대해 loss를 쌓는다. 그리고 optm.step()을 통해 backpropgate하고 나오는 결과를 weight에 옮겨주어 optimizer update를 해준다!

=> 이를 통해, 예측한 값과 실제 값과 비교한다!
