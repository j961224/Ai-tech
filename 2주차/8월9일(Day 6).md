# 1. 8월 9일 공부한 것!

## 1. Historical Review [DL Basic]

### Introduction

- Artificial Inteligence: 사람의 지식을 모방하는 것이다!

- Machine Learning: 무언가를 학습하고자 할 때. 데이터를 가지고 학습한다.

- Deep Learning: 뉴럴 네트워크로 사용하는 세부적인 분야

---

* 딥러닝의 주요 요소

    data: 모델을 학습시키는 요소
    
    model: 데이터를 어떻게 transform 할지
    
    loss: model의 나쁨을 평가하는 함수
    
    알고리즘: loss를 최소화하는 파라미터를 적용하는 것
    
* Data

: 문제를 풀고자 하는 type에 따라 의존한다.

* Model

: 어떤 데이터를 가지고 모델의 성질에 따라 결과가 나오는 것이 다르다.

* loss

: 모델과 데이터가 정해져 있을 때, 모델을 어떻게 학습시킬지 정의한다.

    - Regression 문제: 뉴럴 네트워크의 출력값과 내가 맞추고자 하는 타겟점 사이의 제곱을 최소화 시키는 것 -> 제곱을 평균내서 줄인다!
    
    - classification 문제: 뉴럴 네트워크의 출력값과 내가 맞추고자 하는 라벨 데이터 사이의 cross entropy loss를 최소화한다.
    
    - Probabilistic 문제: 확률적 모델 활용 시, 갓에 대한 평균값, 분산 등과 같은 것은 이와 같은 관점을 사용한다.

![task](https://user-images.githubusercontent.com/59636424/128651831-6d09e19d-cac6-4e23-918e-ad0036e14259.PNG)

* 알고리즘 최적화
 
: Dropout, k-fold validation, Batch normalization을 활용해서 학습 데이터에만 아닌 다른 데이터도 잘 적용하는 것을 목적으로 한다.

### Historical Review

* AlexNet

: 5개의 컨볼루션 레이어와 3개의 full-connected 레이어로 구성되어있다.

![alexnet](https://user-images.githubusercontent.com/59636424/128652793-dd41247e-1734-4ad6-8528-ee7d9d7306fe.PNG)


* DQN

: 강화학습의 큰 축이자, state-action value Q값을 Deep Learning을 통해서 Approximate하는 방식이다.

![dqn](https://user-images.githubusercontent.com/59636424/128652914-6ead011f-8d5a-4c70-894e-6ffe59f0fecf.PNG)


* Encoder/Decoder

: 단어 연속이 주어졌을 때, 어떻게 잘 표현해서 원하는 단어 연속으로 만드는 게 목표

    Encoder와 Decoder: 단어 시퀀스를 어떤 벡터에 인코딩하고 다른 언어의 시퀀스를 만든다.
    
![ec-dc](https://user-images.githubusercontent.com/59636424/128652926-7e7e26c8-9959-4b1f-be04-12eb6cb79381.PNG)

    
* Adam Optimizer

: Adagrad, Adadelta, RMSprop 처럼 각 파라미터마다 다른 크기의 업데이트를 적용하는 방법이다.


* GAN

: 이미지를 어떻게 만들어낼 수 있을지를 본다. -> 네트워크가 generator와 discriminator를 만들어서 학습한다.

![gan](https://user-images.githubusercontent.com/59636424/128653152-cb338305-168e-4217-af04-b5468b66bfee.PNG)


* Residual Networks(ResNet)

: 어느정도 layer를 깊게 쌓으면 전에는 학습이 안 되었다. -> 이것으로 layer가 깊게 좀 쌓아도 적용이 된다.

![resnet](https://user-images.githubusercontent.com/59636424/128653174-20f7cd2a-7b37-4d28-b3a7-f0e3b7dd95d1.PNG)


* Transformer

: 기존의 seq2seq의 구조인 인코더-디코더를 따르면서도, 논문의 이름처럼 어텐션(Attention)만으로 구현한 모델

![transformer](https://user-images.githubusercontent.com/59636424/128653177-ebe220fe-3ae1-4103-a550-ccb3b522fd52.PNG)


* BERT

: Birdirection을이용한 encoder -> 굉장히 다양한 단어들을 이용해 pre-training을 하여 풀고자 하는 데이터 소수에 fine-tuning을 한다.

![bert](https://user-images.githubusercontent.com/59636424/128653180-a6210b80-89b8-43c6-be15-98d4ad03ea25.PNG)



* GPT-X

: 약간의 fine-tuning을 통해 시퀀스 모델을 만들 수 있다. -> 굉장히 많은 파라미터를 가지고 있다.

![gpt-x](https://user-images.githubusercontent.com/59636424/128653258-a34af977-0c79-4849-8690-4676ad1bb534.PNG)

: 간단히 설명하자면 GPT는 자동 회귀이다. GPT Transformer는 모든 토큰이 왼쪽의 컨텍스트에만 주의를 기울일 수 있는 제한된 자기주의를 사용한다. 입력은 순차적으로 제공되며 출력은 한 번에 한 단어씩 생성된다.

* self supervised learning

: 학습 데이터 외에, 라벨을 모르는 데이터를 사용하겠다.

![강화학습](https://user-images.githubusercontent.com/59636424/128653179-531ccd3b-1f70-4caf-b5d5-10d384bfd3c3.PNG)



