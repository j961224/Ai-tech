# 1. 8월 9일 공부한 것!

## 1. Historical Review [DL Basic]

### Introduction

- Artificial Inteligence: 사람의 지식을 모방하는 것이다!

- Machine Learning: 무언가를 학습하고자 할 때. 데이터를 가지고 학습한다.

- Deep Learning: 뉴럴 네트워크로 사용하는 세부적인 분야

---

* 딥러닝의 주요 요소

    data: 모델을 학습시키는 요소
    
    model: 데이터를 어떻게 transform 할지
    
    loss: model의 나쁨을 평가하는 함수
    
    알고리즘: loss를 최소화하는 파라미터를 적용하는 것
    
* Data

: 문제를 풀고자 하는 type에 따라 의존한다.

* Model

: 어떤 데이터를 가지고 모델의 성질에 따라 결과가 나오는 것이 다르다.

* loss

: 모델과 데이터가 정해져 있을 때, 모델을 어떻게 학습시킬지 정의한다.

    - Regression 문제: 뉴럴 네트워크의 출력값과 내가 맞추고자 하는 타겟점 사이의 제곱을 최소화 시키는 것 -> 제곱을 평균내서 줄인다!
    
    - classification 문제: 뉴럴 네트워크의 출력값과 내가 맞추고자 하는 라벨 데이터 사이의 cross entropy loss를 최소화한다.
    
    - Probabilistic 문제: 확률적 모델 활용 시, 갓에 대한 평균값, 분산 등과 같은 것은 이와 같은 관점을 사용한다.

![task](https://user-images.githubusercontent.com/59636424/128651831-6d09e19d-cac6-4e23-918e-ad0036e14259.PNG)

* 알고리즘 최적화
 
: Dropout, k-fold validation, Batch normalization을 활용해서 학습 데이터에만 아닌 다른 데이터도 잘 적용하는 것을 목적으로 한다.

### Historical Review

* AlexNet

: 5개의 컨볼루션 레이어와 3개의 full-connected 레이어로 구성되어있다.

![alexnet](https://user-images.githubusercontent.com/59636424/128652793-dd41247e-1734-4ad6-8528-ee7d9d7306fe.PNG)


* DQN

: 강화학습의 큰 축이자, state-action value Q값을 Deep Learning을 통해서 Approximate하는 방식이다.

![dqn](https://user-images.githubusercontent.com/59636424/128652914-6ead011f-8d5a-4c70-894e-6ffe59f0fecf.PNG)


* Encoder/Decoder

: 단어 연속이 주어졌을 때, 어떻게 잘 표현해서 원하는 단어 연속으로 만드는 게 목표

    Encoder와 Decoder: 단어 시퀀스를 어떤 벡터에 인코딩하고 다른 언어의 시퀀스를 만든다.
    
![ec-dc](https://user-images.githubusercontent.com/59636424/128652926-7e7e26c8-9959-4b1f-be04-12eb6cb79381.PNG)

    
* Adam Optimizer

: Adagrad, Adadelta, RMSprop 처럼 각 파라미터마다 다른 크기의 업데이트를 적용하는 방법이다.


* GAN

: 이미지를 어떻게 만들어낼 수 있을지를 본다. -> 네트워크가 generator와 discriminator를 만들어서 학습한다.

![gan](https://user-images.githubusercontent.com/59636424/128653152-cb338305-168e-4217-af04-b5468b66bfee.PNG)


* Residual Networks(ResNet)

: 어느정도 layer를 깊게 쌓으면 전에는 학습이 안 되었다. -> 이것으로 layer가 깊게 좀 쌓아도 적용이 된다.

![resnet](https://user-images.githubusercontent.com/59636424/128653174-20f7cd2a-7b37-4d28-b3a7-f0e3b7dd95d1.PNG)


* Transformer

: 기존의 seq2seq의 구조인 인코더-디코더를 따르면서도, 논문의 이름처럼 어텐션(Attention)만으로 구현한 모델

![transformer](https://user-images.githubusercontent.com/59636424/128653177-ebe220fe-3ae1-4103-a550-ccb3b522fd52.PNG)


* BERT

: Birdirection을이용한 encoder -> 굉장히 다양한 단어들을 이용해 pre-training을 하여 풀고자 하는 데이터 소수에 fine-tuning을 한다.

![bert](https://user-images.githubusercontent.com/59636424/128653180-a6210b80-89b8-43c6-be15-98d4ad03ea25.PNG)



* GPT-X

: 약간의 fine-tuning을 통해 시퀀스 모델을 만들 수 있다. -> 굉장히 많은 파라미터를 가지고 있다.

![gpt-x](https://user-images.githubusercontent.com/59636424/128653258-a34af977-0c79-4849-8690-4676ad1bb534.PNG)

: 간단히 설명하자면 GPT는 자동 회귀이다. GPT Transformer는 모든 토큰이 왼쪽의 컨텍스트에만 주의를 기울일 수 있는 제한된 자기주의를 사용한다. 입력은 순차적으로 제공되며 출력은 한 번에 한 단어씩 생성된다.

* self supervised learning

: 학습 데이터 외에, 라벨을 모르는 데이터를 사용하겠다.

![강화학습](https://user-images.githubusercontent.com/59636424/128653179-531ccd3b-1f70-4caf-b5d5-10d384bfd3c3.PNG)


## 2. 뉴럴 네트워크 - MLP (Multi-Layer Perceptron)

* 뉴럴 네트워크

: 비선형 연산이 반복적으로 일어나는 모델을 칭한다.


### Linear Neural Networks

* linear function

: 입력이 1차원이고 출력이 1차원이면, 선형 회귀(입력과 출력을 연결하는 모델 찾기 -> line 기울기, 절편, 2개의 파라미터를 찾는 문제)

![zz](https://user-images.githubusercontent.com/59636424/128653708-10b73e00-509a-4827-9026-c73990bb1e21.PNG)

: 뉴럴 네트워크 출력값과 나의 데이터의 차이를 줄이는 것이 중요하다!

![zzzzz](https://user-images.githubusercontent.com/59636424/128653848-76d6de45-78a6-48ac-a401-1e9e20f3bd35.PNG)

: loss function을 줄이는 것이 목표니, 파라미터가 어느 방향으로 움직여야 줄이는 것을 보기 위해서 각 파라미터에 미분한 방향에 역수방향으로 업데이트하면 loss가 최소화되는 지점으로 간다.

![z_z](https://user-images.githubusercontent.com/59636424/128653967-a96a3dd4-3ac4-4838-a7f5-01d67585b041.PNG)

: w와 b를 계속 업데이트하는 것이 gradient descent이다. -> 적절한 stepsize를 잡는 것이 중요하다!

![nonlinear](https://user-images.githubusercontent.com/59636424/128654097-b5f80dd0-7ef3-457e-b3e5-485a5cb17a33.PNG)

: Nonlinear transform으로 네트워크가 표현할 수 있는 정도를 최대화시킨다!

---

* Activation functions

![relu](https://user-images.githubusercontent.com/59636424/128654153-d3ec3f5c-30ec-4442-b5d7-14a1741c1a19.PNG)

: nonlinear transform을 하는 activation function들이다.


---

* **Multi-Layer Preceptron**

![mlp](https://user-images.githubusercontent.com/59636424/128654351-d7a2d61a-2f29-4ce3-9588-e446466c608b.PNG)


: 입력이 주어져있고 linear와 nonlinear 변화를 거쳐 hidden vector가 나와 hidden layer에서 다시 계산하는 한 칸짜리 이상의 hidden layer가 있는 것


* loss function

![loss function](https://user-images.githubusercontent.com/59636424/128654554-24ad27a3-31a8-4508-baf4-9c856d7e01ef.PNG)

-> 분류에서 d개 label를 갖는 문제를 풀 때, output이 나왔을 때, 제일 큰 숫자 index만 고려한다.


### MLP 실습!

* torch.utils.data.DataLoader로 batch_size만큼 데이터를 잘라서 shuffle을 통해 섞는다!

* lin_1과 lin_2를 연산을 할 수 있도록 lin_1에 입력차원과 히든차원, lin_2에는 히든차원과 출력차원을 저장한다.

* pytorch의 set_printoptions(precision=3)을 통해, 정밀도 자릿수를 3자리까지 인쇄하도록 설정한다.

* 설정한 모델이 아닌, 매우 기본적인 모델로 mnist를 예측했을 때, 정확도 0.115가 나오는 것을 알 수 있다.

* pytorch 코드 중, batch_in.view(-1,28 x 28).to(device)는 (batch size=256, 1, 28,28)크기의 tensor를 (batch_size=256,xdim)으로 변경해준다.

* 설정한 모델로 학습에서 update 부분은, 우선, zero_grad로 gradient를 reset을 해주고 loss_out을 backpropgate를 통해 weight에 대해 loss를 쌓는다. 그리고 optm.step()을 통해 backpropgate하고 나오는 결과를 weight에 옮겨주어 optimizer update를 해준다!
