# 배운 것 정리!

## 2.3. Knowledge Distillation

**Teacher의 어떤 정보를 사용할 것인가?**

**Student를 어떻게 결정할 것인가?** 

![ㅅㅅㅅㅅ](https://user-images.githubusercontent.com/59636424/144697876-85afa12d-b119-4162-8c1a-0ff6a26032cf.PNG)

Distillation from output logits -> CV에서 Response based이다.(Hinton)

Distillation from encoder outputs / Distillation from attention maps -> feature based 이다!

![ㅕㅕㅕㅕ](https://user-images.githubusercontent.com/59636424/144698205-2054ac0b-a082-4b02-b2bf-5caffba8b12e.PNG)

(a) encoder의 크기(차원)을 줄이는 방법

(b) depth를 줄이기!

(c), (d) 아예 다른 구조를 가져감 -> 성능을 좀 포기하고 속도를 가져간다!

### DistilBERT

* Overview

pretrain 과정에서, Knowledge Distillation 사용!

masked language modeling loss, distillation(Hinton) loss, cosine-similarity loss(feature based loss) 사용!

**model size 40% 줄이고, 성능은 원래보다 97%의 성능을 보임!**

* **Triple loss= MLM loss + Hinton loss + Cosine embedding loss 사용!**

Distillation loss(teacher와 student softmax prob에 temperature를 둔다!)

Cosine embedding loss(teacher, student hidden state vectors)

* **Hinton loss recap**

하나의 정답에 대한 학습을 하는 것보다, 여러 개가 될 수 있다는 가능성과 distribution을 학습하는 것이 효과적이다.

* Student architecture & initialization

layer 수를 줄이는 것을 focus 맞춤!!

-> Student의 initialization을 factor of two이므로 BERT의 12 layer 중, 뒷쪽의 것을 가져온다! -> ex. Stduent 첫번째 - BERT 2번째

**token type embedding 제거!, 마지막에 pooler 제거!**

* Experiment

triple loss에서 하나씩 빼보기!!

--> 3개 중에 머가 더 기여가 높은지 알 수 있음! (cosine entropy가 가장 큰 역할을 준다!)

### TinyBERT

* Overview

Transformer distillation method를 사용!!

* Transformer distillation method: 3 types of loss

1) output embedding layer
2) hidden states and attention matrices
3) logits output (Hinton loss)

* **Trnasformer-layer Distillation(Attention based)**

feature based loss와 유사!

![ㅛㅛㅛ](https://user-images.githubusercontent.com/59636424/144699208-91a1fdcf-7813-4916-ac6d-52735a08d23c.PNG)

A: teacher와 student의 attnetion matrix이다!

* **Trnasformer-layer Distillation(Hidden state)**

![ㅕㅕㅕ](https://user-images.githubusercontent.com/59636424/144699265-60a13dcb-cdcb-494f-bf10-2c9162addeed.PNG)

H^S(Student hidden state), H^T(Teacher hidden state)

-> W_h를 두는 이유는?: 2개 사이의 dimension 차이때문에!!

* **Embedding-layer distillation loss**

![ㅊㅊㅊㅊ](https://user-images.githubusercontent.com/59636424/144699323-502c7667-6042-47ad-a35f-02023c7192ac.PNG)

* TinyBert Learning

![ㅕㅕㅕㅕㅕㅕㅕㅑㅑ](https://user-images.githubusercontent.com/59636424/144699411-5c70f050-9e8c-4aa3-bea1-74e41ed9f81c.PNG)

**Distillation을 2번 함!!**(General Distillation, Task-specific Distillation!)

* Experiments

기존 대비 성능은 약 2점 정도 낮지만, 확실히 속도와 파라미터 수 면에서는 줄었다!!

* 하나씩 빼보기!!

|System|Avg|
|---|----|
|TinyBERT|75.6|
|Embedding빼기|74.1|
|Prediction빼기|73.5|
|Transformer빼기|56.3|

## 2.4. Quantization

- 장점: memory가 정해져 있는데, 더 작은 메모리를 넣는다!
- 단점: accuracy drop이 이 과정에서 일어난다.

### Q-BERT

mixed-precision quantization 적용!

### Hessian spectrum

NN layer에서 eigenvalue가 크면 -> quantization에 대해서 더 민감!

> * Large sparse matrix에서 더 빠른 수렴! -> power iteration method
> * 같은 데이터셋에서 Hessian spectrum의 var이 매우 큼! -> mean, std를 함께 고려하여 민감도를 정렬

### Group-wise Quantization method

Quantization range를 정해야함 -> key, query, value and output matrices에 대한 동일한 range로 quantization했음 -> 너무 단위가 크니 분포가 다를 수 잇어 error가 발생 가능!

**그래서, multi head 별로 따로 따로 주었다.(더 세밀하고 range 구분 및 quantization error가 줄어듦)**

- 장점: 모델 사이즈 감소, 적은 성능 하락
- 단점: 속도 향상 불투명!




