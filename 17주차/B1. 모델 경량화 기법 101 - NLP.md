# 배운 것 정리!

## 1.0. BERT Profiling

![ererer](https://user-images.githubusercontent.com/59636424/143766847-78d6f463-bd27-496f-b234-d65574fd6049.PNG)

-> BERT의 encoder 구조(총 12개의 layer)

## 1.1. Model size and computations

![wwww](https://user-images.githubusercontent.com/59636424/143767040-4412deb9-cbf3-403e-bf79-4f5d7da85384.PNG)


* Memory 

> * Embedding layer: look up table로, FLOPs는 없음
> * Linear before Attn: k, q, v mat 연산으로 after 대비 3배
> * MHA: matmul, softmax 등의 연산, 별도 파라미터는 없음
> * FFN: 가장 많은 파라미터, 연산횟수

* Runtime

> * MHA: 속도가 매우 느린데, matmul, softmax 때문에 이렇게 보여짐!
> * Linear layer: GPU에서는 빠른 속도를 보여주나, CPU는 이론 연산 횟수와 유사한 경향 보임!

## 1.2. Paper Review(Pruning)

* Structured(encoder unit, embedding size, attention head)
* Unstructured(각 weight matrix를 sparse하게 만든다!)

## 1.3. Are Sixteen Heads Really Better than One? (NeurlPS 2019)

* Are All Attention Heads Important?

하나의 HEAD만 남기고 지우면, 성능을 각각 재어서 drop이 가장 낮은 것을 확인해보자!

-> 크게 성능 drop이 없음!

-> **하지만, 실제로 MHA가 필요한 layer는 last layer에 대해서는 꼭 필요!**

* Are important Heads the Same Across Datasets?

이제는 여러 layer에 대한 여러 head 제거 시, 어떤 현상이 발생할까?!

-> 중요도를 간접적으로 계산해서 Head를 하나하나 지워본다!

## 1.4. Movement Pruning: Adaptive Sparsity by Fine-Tuning(NeurlPS 2019) -> unstructured pruning

fine tuning(Trasnfer Learning) 과정에서, weight가 점점 커지는 과정인가?(0이랑 멀어지는 과정인가?) -> Movement pruning

* Method Interpretation: Movement Pruning의 score를 유도

* Experiment

정도가 작은 pruning에 대해서는 다른 기법들이 우세!

pruning 정도가 커질수록 효과성 증대!

## 1.5. On the Effect of Dropping Layers of Pre-trained Transformer Models

-> Encoder의 각 위치별 어떤 knowledge를 가지는가?!

* Pretained information은 input에 가까운 encoder들에 저장
* head에 가까운 부분들은 task specific한 정보를 저장

-> pretraining 모델에서 head 쪽 layer를 없애도 fine-tuning 시, 성능이 크게 떨어지지 않는다.

## 1.6. Pretraining-fine-tuning paradigm이 왜 성능, generalization capability가 더 좋은가?

학습 이전의 surface를 볼 수 있음!

## 2.1. Pruning

> * Structured
>> * 장점: 모델 사이즈 감소, 속도 향상
>> * 단점: 성능 drop
> * Unstructured
>> * 장점: 모델 사이즈 감소, 적은 성능 drop
>> * 단점: 속도 향상 X

## 2.2 Weight Factorization & Weight Sharing

* Cross-layer parameter sharing: Parameter 수 감소
* NSP -> SOP -> 성능 향사
* Factored Embedding Parameterization: Parameter 수 감소

## 2.3. Cross-layer parameter sharing

network params stabilizing하는 것이 효과가 컸다.

## 2.4. Sentence Ordering Objectives

NSP task는 쉬웠다.(topic prediction 등을 학습하려는 의도) -> SOP task로 강화!

NSP task만 학습한 것은 SOP task의 성능에는 안 좋았지만 SOP task학습한 것은 NSP에서도 어느정도 맞췄다!

## 2.5. Factorized Embedding Parameterization

* 주된 claim!

BERT는 context-dependent representations의 학습에 효과적 구조!!

-> but BERT layer가 context-independent representations인 WordPiece embeddings에 묶여야되나?

추가적으로, Vocab size가 매우 큼!

Factorized Embedding Parameterization을 하면, O(V x H) -> O(V x E + E x H)로 줄일 수 있다.

