# 1. 내용 정리!

## 1.1. Conventional DL Training Pipeline

![s](https://user-images.githubusercontent.com/59636424/142808717-c1c16cf6-9643-4c4b-bfce-81932a8cc88c.PNG)

Dataset -> 1. Model Architecture, 2. Hyperparameter -> train & Evaluate

위의 과정을 좋은 configuration을 찾을 때까지 반복! -> 주로 사람이 진행!

## 1.2. Objectives of AutoML

![scscsc](https://user-images.githubusercontent.com/59636424/142809038-08056429-1934-44af-99ac-84dc9d605688.PNG)

이러한 반복적인 프로세스 안에서, 사람을 빼내자는 것이 목표!!

* AutoML(HPO: Hyperparameter Optimization)의 문제 정의

![www](https://user-images.githubusercontent.com/59636424/142809344-9b2d9bb0-79c2-4a5c-b505-9262a4a2e1fa.PNG)

**아래 식은 데이터와 validation set, 알고리즘, configuration이 주어지면, loss를 가장 최소화 해주는 hyperparmeter configuration을 찾는 것!**

## 1.3. Properties of configurations in DL

* 주요 타입 구분

> * Categorical
>> * optimizer: Adam, SGD, AdamW 등
>> * module: Conv, BottleNeck, InvertedResidual 등
> * Continuous
>> * learning rate, regularizer param
> * Integer
>> * batch_size, epochs

* **Conditional: 한 configuration에 따라 search space가 달라질 수 있음!**

ex) optimizer의 sample에 따라서 optimizer parameter의 종류, search space도 달라진다.

ex) Module의 sample(Vanilla Conv, BottleNeck 등등)에 따라서 해당 module의 parameter의 종류, search space도 달라진다!


## 1.4. 모델경량화 관점에서의 AutoML

모델 경량화의 접근 두가지: 주어진 모델을 경량화하자, 새로운 경량 모델을 찾자

* 주어진 모델을 경량화하는 기법

: Pruning, Tensor decomposition

* **Search를 통하여 경량 모델을 찾는 기법**

: NAS, AutoML

## 2.1. AutoML Pipeline

![wwwq](https://user-images.githubusercontent.com/59636424/142810880-8b3ef476-c025-4e08-b297-e67ddaf36c0e.PNG)

blackbox의 optimization을 max하는 것이 목표!!

* Bayesian Optimization(BO)

![aax](https://user-images.githubusercontent.com/59636424/142811602-eefc3d42-a682-41dc-9cbf-44a5a0fa9739.PNG)

## 2.2. Bayesian Optimization with Gaussian Process Regression

![xxx](https://user-images.githubusercontent.com/59636424/142814906-bf8bd679-77a2-4f61-93fa-9c2929a6e4cb.PNG)

그림을 보면, 까만 점이 evaluate한 f값이다.

점선은 실제 알 수 없는 objective f 함수의 계형이다.

보라색영역이 surrogate function의 output이다.

sample은 y값 -> 까만 점으로 surrogate function을 update한다. -> 점점 점선과 보라색영역들이 정밀해진다.(surrogate model 업데이트) -> 초록색 부분(Acquisition function 업데이트)

Acquisition function은 좋은 configuration이 어디인지 결정하는 함수(다음 어디를 시도할지 결정!)

* Gaussian Process Regression

데이터를 가장 가깝게 fitting하는 함수 f를 찾는 것이 일반적인 regression의 문제 정의이다.

![vvvv](https://user-images.githubusercontent.com/59636424/142816779-a6649430-9f67-4f5a-887c-664e1a2be136.PNG)

f(아는 것): 50개, f_q(모르는 것): 1개라고 가정 시, K(X,X): 50 x 50, K(X,X_q): 50 x 1, K(X_q,X): 1 x 50, K(X_q,X_q): 1 x 1이 된다.

* Surrogate Model(function): f(람다)의 Regression model

-> Objective f(람다) 값을 예측하는 모델

-> Objective를 estimate하는 surrogate model 학습, 다음 좋은 람다를 선택하는 기준으로 사용!

* Acquisition Function: 다음은 어디를 시도하면 좋을까?

-> Surrogate model의 output으로부터, 다음 시도해보면 좋을 람다를 계산하는 함수

-> Acquisition function의 max 지점을 다음 iteration에서 trial

![sssss](https://user-images.githubusercontent.com/59636424/142830590-81a744cf-15c3-465f-9af6-4c85dba07cbe.PNG)

## 2.3. Tree-structured Parzen Estimator(TPE)

![ccccc](https://user-images.githubusercontent.com/59636424/142831246-92a5df3b-6120-471c-9bbd-9320014cc0c6.PNG)

* TPE를 통한 다음 step의 람다 계산 방법

> * 현재까지의 observation들을 특정 quantile(Inverse CDF)로 구분
> * KDE(Kernel density estimation)으로 good observations 분포(p(g)), bad observations 분포(p(b))를 추정

**EI 증명**

-> Likelihood를 Quantile로 구분되는 두 함수로 정의

![ㅌㅌㅌ](https://user-images.githubusercontent.com/59636424/142836905-91192f9d-5f82-4add-b1fc-44c7928376f4.PNG)

![ㅊㅊ](https://user-images.githubusercontent.com/59636424/142836910-a161ddcc-9a42-4d2d-afaf-2b12eb675de0.PNG)

**l(x)(좋았던 관측 분포), g(x)(안 좋았던 관측 분포) ==> l(x)/g(x)가 가장 높은 지점을 탐색하지만, g(x) 낮은 곳도 안 좋은지 알 수 없는 것도 찾아보자!**

## 3. Further Studies

