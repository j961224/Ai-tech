# 1. 최적화 소개 및 강의 개요 배운 것!

## 1. 경량화의 목적!

다양한 Application이 나오고 있는 상황이다!

### 1.1. On device AI

RAM 메모리 제약, 저장공간, Power usage 등의 제한이 있다.

### 1.2. AI on cloud(or server)

베터리, 저장 공간 등의 제약은 줄지만 latency와 throughput의 제약이 존재!

-> 한 요청의 소요 시간, 단위 시간당 처리 가능한 요청 수!

### 1.3. Computation as a key component of AI progress

![rr](https://user-images.githubusercontent.com/59636424/142800441-c8c54b4c-c854-4af2-9d78-c5e4b35820f9.PNG)

### 1.4. 경량화는?!

* 산업에 적용되기 위해서 거쳐야하는 과정!
* 요구조건들 간의 trade-off를 고려하여 모델경량화/최적화 수행!

### 1.5. 경량화, 최적화의 종류

> * 네트워크 구조 관점
>> * Efficient Architecture Design: Vision에서의 module block 등, AutoML
>> * Network Pruning: 학습된 네트워크를 가지고 있다면, 중요도가 낮은 파라미터를 제거하며 접근하는 법
>> * Knowledge Distillation: 학습된 큰 규모의 teacher network가 있다면, 작은 network에 teacher network의 지식을 전달!
>> * Matrix/Tensor Decomposition: 학습된 network에 대해서, 더 작은 단위의 곱과 합으로 표현 -> weight 갯수 및 연산량 줄어듬!
> * 하드웨어 관점
>> * Network Quantization: float32가 아닌 float16이나 더 작은 걸로 표현하는 것!
>> * Network Compiling

* Efficient architecture design: AutoML, Neural Architecutre Search

![xxxx](https://user-images.githubusercontent.com/59636424/142801571-113ca5ff-08c0-4ad2-ae2b-a5b789f75997.PNG)

controller(모델을 찾는 네트워크)이 architecture를 제안하고 그것으로 학습시켜 accuracy 계산 -> accuracy로 모델을 찾는 네트워크를 또 학습!

이런 식으로 반복!

**사람의 직관보다 상회하는 성능의 모듈들을 찾아낼 수 있다.**

* Network Pruning: 찾은 모델 줄이기

**중요도가 낮은 파라미터를 제거! -> 좋은 중요도를 정의!**

* Structured pruning

파라미터를 그룹 단위로 pruning하는 기법!!

![zxzxzx](https://user-images.githubusercontent.com/59636424/142801950-8f03f293-911c-41b9-afe9-7e439520557c.PNG)

하얀색 부분이 날릴 filter 부분!

channel scaling factors가 중요도!

* Unstructured pruning

파라미터를 각각 독립적으로 pruning하는 기법! -> 하면 할수록, 네트워크 내부의 행렬이 점차 희소해짐!

![aaa](https://user-images.githubusercontent.com/59636424/142802088-e20b389e-7849-4c82-a462-6cd9fde4f737.PNG)

* Knowledge distillation

**학습된 큰 네트워크를 작은 네트워크를 학습하는데 보조로 사용하는 방법!**

![zzzz](https://user-images.githubusercontent.com/59636424/142802324-05c35b82-3b83-44b8-bff3-ee59713de9a8.PNG)

1) Student network와 ground truth label의 cross-entropy
2) teacher network와 student network의 inference 결과에 대한 KLD loss로 구성

![az](https://user-images.githubusercontent.com/59636424/142802531-f09c6263-0cae-4914-88de-c725e58b7023.PNG)

T는 large teacher network의 출력을 smoothing하는 역할

알파는 두 loss의 균형을 조절하는 파라미터

* Matrix/Tensor decomposition

하나의 Tensor를 작은 Tensor들의 operation들의 조합(합,곱)으로 표현!

* Network Quantization

float16, int8로 데이터타입을 변환하여 연산을 수행!

![xs](https://user-images.githubusercontent.com/59636424/142802782-08012514-6cd3-4f62-a1ee-ef4bf57d9a48.PNG)

-> 사이즈는 감소, 성능(Acc)는 일반적으로 약간 하락, 속도는 Hardware 지원 여부 및 사용 라이브러리에 따라 다름!

* Network Compiling

학습이 완료된 Network를 deploy하려는 target hardware에서 inference가 가능하도록 compile하는 것!

**속도에 가장 큰 영향을 미치는 기법!**

꽤 상당한 성능 차이가 있다!

![www](https://user-images.githubusercontent.com/59636424/142803057-a21e3d39-fc9d-44e2-b998-8de3718a2bcd.PNG)

특정 연산을 하나의 fusion으로 묶을 수 있다. -> Compile 시, layer fusion 등의 최적화가 수행!

Framework와 hardware backends 사이에 수많은 조합이 존재!

**layer fusion의 조합에 따라 성능차이가 발생!**

![ssss](https://user-images.githubusercontent.com/59636424/142803178-ebb34527-37d7-4dcd-b2e7-1fd2a85e0c14.PNG)

