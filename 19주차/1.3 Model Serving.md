# 1. 배운 것 정리!

## Web Server Basic

Serving 해주는 사람 = **Server**, 손님 = **Client**

Server가 Client가 원하는 것을 Response함!!

* **new client가 Request 하는 경우?!**

자리 있는지, 없는지 Response 및 client와 server가 Request 및 Response를 주고받는다!

## API

운영체제나 프로그래밍 언어가 제공하는 기능을 제어할 수 있게 만든 인터페이스

하나 서버에서 여러 API가 존재한다!

## Online Serving Basic

Request에 대해서 실시간으로 예측!!

Client -(HTTP 요청)-> ML 모델 서버 -(Response)-> 예측 값

* Serving Input - Single Data Point

단일 데이터를 받아 실시간으로 예측!!

* Online Serving 구현 방식!

>* API 웹 서버 개발: Flask, FastAPI 등을 사용해 서버 구축
>* 클라우드 서비스 활용: AWS의 SageMaker, GCP 등
>* Serving 라이브러리 활용: Tensorflow Serving, Torch Serve 등

## 클라우드 서비스 활용

비용 문제!! ㅠㅠ

## Serving 라이브러리 활용

BentoML, MLFlow 등에 Fast API나 Flask같은 것이 포함되어 있다. -> 웹 서버 개발 패턴 추상화되어 있어 이것만 잘 써도 좋다!!

* BentoML example!

[uuuuu](https://user-images.githubusercontent.com/59636424/144787180-c5f8f627-1a1d-4736-934a-4a2f344eb357.PNG)

-> Classifier를 import 후, 모델을 학습한 객체를 pack으로 모델을 저장!!

CLI에서 "bentoml serve IrisClassifier:latest"하면 끝!

* 클라우드 비용이 괜찮다면, 추천 방식!

>1. 프로토타입 모델을 클라우드 서비스 활용 배포!
>2. FstAPI 등을 활용해 서버 개발
>3. SErving 라이브러리 활용해 개발!

* **우리가 적용할 방법!!!!!!!!!**

>1. 프로토타입 개발
>2. Fast API로 서버 개발
>3. Serving 라이브러리로 개발

## Online Serving에서 고려할 부분!!

* 재현되지 않는 코드는 risk가 크다!

* Input 데이터를 기반으로 DB에 있는 데이터를 추출해서 모델 예측하는 경우!
* 모델을 수행하는 연산(모델 경량화 작업이 필요할 수도 있음! -> 모델 줄이는 것이나 간단한 모델이 좋을 수 있다!!)
* 결과 값에 대한 보정이 필요한 경우(유효하지 않는 예측값 도출 시, 후처리를 해야될 수 있다.)

* 처리 방법!

>* 데이터 전처리 서버 분리! -> 미리 가공!
>* 모델 경량화
>* 병렬처리
>* 예측 결과 캐싱(중간 저장하여 다시 사용하는 방법)

## Batch Serving

Workflow Scheduler로 자동으로 python 파일을 실행해준다!!

Airflow, Cron Job등으로 스케쥴링 작업 가능!!!

* Batch Serving 장점!

>* 구현이 간단하고 한번에 많은 데이터 처리!!

* Batch Serving 단점!

>* 실시간으로 활용 불가
>* Cold start 문제: 오늘 새로 생긴 컨텐츠는 추천 불가


* 어떤 경우에 Online를 쓰고, Batch를 쓸까?!

API 형태로 바로 결과 반환 시, Online

서버와 통신 시, Online

1시간에 1번씩 예측하는 경우, Batch

**실시간으로 Online serving을 추천!!!!!!**


[Rules of Machine Learning](https://developers.google.com/machine-learning/guides/rules-of-ml) -> 읽고 정리 추천!!



