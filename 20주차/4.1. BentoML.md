# BentoML

## 1. BentoML

### 소개 및 배경

많은 모델을 만들다보니 반복되는 작업이 존재!! -> 여전히 Serving은 어렵다! -> **더 빠르게 간단하게 하고 싶다면?**

### 해결하려는 문제!!

#### 문제 1. Model Serving infra의 어려움!

BentoML 사용 전에는, 사이즈가 큰 파일을 패키징, 지속적인 배포를 위한 많은 작업이 필요

-> CLI로 문제의 복잡도를 낮춤!!

#### 문제 2. Online Serving의 Monitoring 및 Error Handling

BentoML 사용 전에는, Error 처리, Logging을 추가로 구현했어야 함!

-> BentoML로 Python Logging Module을 사용해서, Access Log, Prediction Log를 기본으로 제공!

#### 문제 3. Online Serving 퍼포먼스 튜닝의 어려움

Traffic 많은 양의 처리 문제 -> BentoML의 Adaptive Micro Batch 방식으로 많은 요청 처리

### BentoML 특징

> * 쉬운 사용성
> * Online / Offline Serving 지원
> * Docker, 쿠버네티스, AWS 등의 배포 환경 지원 및 가이드 제공

## 2. BentoML 시작하기

### BentoML 사용 Flow - 모델 학습 코드

### BentoML 사용 Flow - Prediction Service Class 생성

BentoService를 활용해 Prediction Service Class 생성!

> * @env: 파이썬 패키지 등 서비스에 필요한 의존성을 정의
> * @api: API 생성 -> Input, Output을 원하는 형태로 선택할 수 있음, 문서 작업 간편화를 위해 Swagger에 들어갈 내용 추가 가능!!
> * @artifacts

Prediction Service에 pack으로 학습한 모델을 저장!!

### BentoML 사용 Flow - Docker Image Build

docker 명령어나 FastAPI를 사용하지 않고 웹 서버를 띄우고, 이미지 빌드!!

## 3. BentoML Component

### BentoService

예측 서비스를 만들기 위한 베이스 클래스!

* @bentoml.artifacts: 여러 머신러닝 모델 포함할 수 있음
* @bentoml.api: Input/Output 정의

### Service Environment

파이썬 관련 환경, Docker 등을 설정할 수 있음!

* @bentoml.ver
* @bentoml.artifacts

### Model Artifact

* @bentoml.artifacts: 사용자가 만든 모델을 저장해서 pretrain model을 읽어 Serialization, Deserialization

여러 모델을 같이 저장할 수 있음

### Model Artifact Metadata

해당 모델의 Metadata

* CLI
* REST API
* Python

### Model Management & Yatai

모델 리스트 확인

### API Function, Adapters

데이터 처리하는 함수도 작성 가능!!

세밀하게 Response를 정의할 수 있음

BentoService가 여러 API를 포함할 수 있음

### Model Serving

* Online Serving: 실시간으로 REST API Endpoint로 예측 요청
* Offline Batch Serving: 예측을 계산한 후, Storage에 저장
* Edge Serving: 모바일, IoT Device에 배포

### Retrieving BentoService

학습한 모델을 저장한 후, Artifact bundle을 찾을 수 있음!

### WEB UI



