# Airflow

## Apache Airflow 소개

### Batch Process

예약된 시간에 실행되는 프로세스를 정의!! -> 일회성, 주기적인 실행 가능!

=> 모델을 주기적으로 학습시키는 경우에 사용!!

=> Batch Serving을 하는 경우와도 이어진다!!

* 크론 표현식

: Batch Process의 스케줄링을 정의한 표현식!!

### Airflow 등장 전 - Batch Process

> * Linux Crontab 문제
>> * 재실행 및 알람 -> 오류 떠도 그거에 대해서 별도 처리를 하지 않는다. => **그래서 실패하면, 실패했다는 알람을 받으면 좋음!**
>> * 과거 실행 이력 및 실행 로그 보기 어려움!
>> * 여러 파일 실행이 어려움
>> * 복잡한 파이프라인을 만들기 힘듦!

### Airflow

현재 스케줄링, 워크플로우 도구의 표준!

-> 스케줄링 도구로 무거울 수 있지만, 거의 모든 기능을 제공!!

### Airflow 제공 기능

> * 파이썬을 사용해 스케줄링 및 파이프라인 작성 가능!
> * 스케줄링 및 파이프라인 목록을 볼 수 있는 웹 UI 제공!
> * 실패 시 알람
> * 실패 시 재실행 시도
> * 동시 실행 워커 수

## 2. Airflow 실습

* 설치

```
pip install 'apache-airflow==2.2.0'
```

* DB 초기화

```
airflow db init
```

* Airflow에 사용할 admin 계정 생성

```
airflow users create --username 유저네임 --password 패스워드 --firstname 이름 --lastname 이름 --role Admin --email 이메일
```

* Airflow Webserver 실행

```
airflow webserver --port 8080
```

* Airflow Scheduler 실행하여 Scheduler 관련 error 제거

```
airflow scheduler
```

### DAG 작성하기 - DAG과 Task

DAG: Airflow에서 스케줄링할 작업 => 순환하지 않는 방향이 존재하는 그래프!

**Airflow는 Crontab처럼 단순히 하나의 파일을 실행하는 것이 아닌, 여러 작업 조합 가능!!!**

* DAG 1개: 1개의 파이프라인
* Task: DAG 내에서 실행할 작업

하나의 DAG - 여러 Task 조합!

### 코드에서의 DAG 작성

> * **DAG 정의(이름, 태그)**
>> * **언제부터 스케줄링 시작**
>> * **스케줄링 간격은 어떻게**
> * **DAG 내 Task 정의**
>> * **Airflow의 Operator class를 가지고 Task 정의**
>> * 다양한 Operator class가 존재!!
> * **첫 번째 Task: bash command 실행**
>> * **BashOperator 사용**
>> * **bash_command 파라미터에 bash로 실행할 command 전달**
> * **두 번째 Task: Python 함수 실행**
>> * **PythonOperator를 사용**
>> * **python_callable v파라미터에 실행할 파이썬 함수 전달**
> * **DAG 내 Task 간 순서 정하기**
>> * **순서는 >>로 표현**

### PythonOperator

파이썬 함수 실행하고 그 뿐만 아니라, Callable한 객체를 파라미터로 넘겨 실행!!

Python 로직을 함수로 생성 -> PythonOperator로 실행!

### BashOperator

Bash command 실행 -> 프로세스가 Python이 아니어도 가능!

### DummyOperator

아무것도 실행 X

DAG 내에서 Task 구성 시, 여러 개의 Task의 SUCCESS를 기다려야 하는 복잡한 Task 구성에 사용!!

## 3. Apache Airflow 아키텍처와 활용방안

### 기본 아키텍처

![rrr](https://user-images.githubusercontent.com/59636424/149260654-b3323d37-96fa-43b8-a9b1-4805b8520bc2.PNG)

* DAG Directory

DAG 파일들을 저장

> * Scheduler: 각종 메타 정보의 기록을 담당
>> * Scheduler - Executor
>>> * Local Executor: DAG Run을 프로세스 단위로 실행, 최대로 생성할 프로세스 수를 정해야 함
>>> * Sequential Executor: 하나의 프로세스에서 DAG Run들을 처리, Airflow 기본 Executor로, 별도 설정이 없으면 이것 사용
>> * Scheduler - Remote Executor
>>> * Celery Executor: DAG Run을 Celery Worker Process로 실행
>>> * Kubernetes Executor: 쿠버네티스 상에서 Airflow를 운영할 때 사용, DAG Run 하나가 하나의 Pod
