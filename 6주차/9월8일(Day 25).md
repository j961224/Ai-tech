# 1. 9월 8일 배운 것!

## 1. Seq2Seq with attention Encoder-decoder architecture Attention mechanism

### Seq2Seq Model

many to many에 해당한다!

### Seq2Seq with attention

many to many 중에, **맨 앞에 넣었던 단어가 응답할 때, 유실될 가능성이 높다!**

차선책으로, 입력 문장의 순서를 뒤집어서 마지막 timestep에 첫 단어가 나오게 한 적이 있다.

![diqdiq](https://user-images.githubusercontent.com/59636424/132435175-b2fae01a-9c24-4ada-8c31-644002584b64.PNG)

    1. Encoder의 마지막 hidden state가 Decoder로 들어간다.

    2. Decoder RNN에 start token(x_1)의 word embedding vector가 주어진다.
    
    3. 그렇게 넣은 것이 Decoder hidden state vector 나온다. (h_1^d)
    
    4. h_1^d는 encoder hidden state들과 내적을 해서 Attention scores가 된다.
    
    5. softmax를 통해 Attention distribution(합이 1인 형태의 상대적인 가중치를 attention vector라고 한다.)이 나온다.
    
    6. 이러한 Attention distribution은 encoder hidden state에 부여되는 가중치로 사용!
    
    7. 이렇게 구한 것은 가중평균을 구할 수 있고 그것으로 하나의 encoding vector(attention output)를 구할 수 있다.
    
**encoder hidden state들과 decoder hidden state 1개가 attention module에 들어가서 가중평균 vector 1개가 output으로 나온다.**

**Decoder hidden state vector와 attention output vector가 concat이 되어서 다음 입력으로 들어간다.**

![wow](https://user-images.githubusercontent.com/59636424/132436923-00dc3f61-63b9-4d36-9449-33a345fd00fd.PNG)

    1. 2번째 timestep에서는 그 당시에 들어오는 것은 입력단어와 decoder hidden state vector가 들어온다.
    
    2. 그러면 decoder hidden state vector(h_2^d)가 들어온다.
    
    3. 앞과 동일한 모듈을 사용하되, h_2^d로 내적하여 Attention score -> distribution을 구한다
    
    4. 이러한 Attention distribution은 encoder hidden state에 부여되는 가중치로 사용!
    
    5. 이렇게 구한 것은 가중평균을 구할 수 있고 그것으로 하나의 encoding vector(attention output)를 구할 수 있다.

**이렇게 또, Decoder hidden state vector와 attention output vector가 concat이 되어서 다음 입력으로 들어간다.**

이렇게 계속 흘러간다!

**핵심인, Attention distribution으로 어떤 단어를 가져올지 가중치를 가져오고 이것을 Attention output이 어떤 단어를 예측할지에 영향을 준다.**

---

### Backpropagation

![wwwww](https://user-images.githubusercontent.com/59636424/132437964-cc6299e3-6673-4320-b6d0-258b4df48d6c.PNG)

    1. 예측했던 output을 decoder RNN과 Encoder RNN으로 돌아간다.
    
    2. 만일, 단어 예측할 때, encoder 정보를 잘 못 가져온 경우 Attention distribution을 원하는 정보가 선택될 수 있도록 업데이트한다.
    
    3. hidden state vector가 다시 Decoder RNN을 갱신한다.

**보통 초반에 output을 잘 못 예측하더라도 다음 timestep에 Ground Truth를 입력으로 넣어주게 된다.**

=> 이 방식은 Teacher Forcing이라고 한다!

### Different Attention Mechanisms

![weee](https://user-images.githubusercontent.com/59636424/132441559-dc8ebaa7-c27b-4c87-a4ce-5f2d6142394c.PNG)

1. 내적하는 방법

2. general 내적 방법

![ee](https://user-images.githubusercontent.com/59636424/132440800-48394f85-6588-467b-ab78-f0d6fbdea4bf.PNG)

    1번 내적 방법에서 가운데에 학습가능한 행렬을 둔다!
    
    보다 일반화된 dot product! -> 이러한 방법으로 attention module의 유사도를 결정해주는 학습 가능한 파라미터
    
3. concat 기반 attention

![qqqq](https://user-images.githubusercontent.com/59636424/132441456-7b9ec9db-01dc-44fa-ad82-c84ac8efeec4.PNG)

    decoder hidden state vector와 encoder 특정 word hidden state vector를 concat한다.
    
    그림의 W1(=Wa)을 거친다.
    
    tanh로 non linear를 만든다!
    
    두번째 layer에서는 W2(v_a^T)(왜 백터로 표현되었냐면 만약에 3차원 벡터가 있다면 W2는 scalar 값을 만들어야하므로 row벡터가 된다.)로 두 벡터의 유사도를 구해준다.
    
    결론으로 two layer의 neural net을 구할 수 있다!

**이러한 방식들로 attention score 부분에서 그전에는 학습가능한 파라미터가 포함되지 않았던 simple 내적을 학습이 필요로하는 파라미터가 포함된 모듈로 변경되었다.**

=> 이것은 Backpropagation을 통해서 유사도 구하는 모듈을 갱신한다. (Wa와 같은 가중치들)

## Attention is Great!

기계번역 분야에 성능을 올릴 수 있었다!

-> decoder의 매 timestep마다 어떤 부분을 집중하고 사용할지가 가능해짐!

bottleneck 문제를 해결!

-> 긴 문장에 대해서 번역이 어려웠는데 그것을 해결

학습 관점에서 기울기 소실 해결

-> LongTerm Dependency를 해결!

해석가능성을 제공해준다!

-> decoder가 각 단어를 예측할 때, 어떤 단어를 집중했는지 알 수 있다!

-> 어떤 단어를 봐야할지에 대한 aligment를 스스로 배울 수 있다!

## Attention Examples in Machine Translation

![rr](https://user-images.githubusercontent.com/59636424/132442508-06c7bb09-42fc-4410-8112-bbb600773624.PNG)

옆쪽이 encoder로 번역할 문장이고 위는 decoder로 번역된 문장이다.

그림을 보면 처음에서 4번째까지 attention이 순차적으로 번역을 했는데 그 다음에는 encoder 해당 단어 순서를 반대로 보아 align을 수행하여 단어를 예측했다.

적절한 attention pattern을 볼 수 있다!

---

## 시각화

### 5-1. Polor Coordinate

#### Polor Plot

* 극 좌표계를 사용하는 시각!

* 회전, 주기성 등을 표현하기에 적합!

#### Data Converting

![rrrrrr](https://user-images.githubusercontent.com/59636424/132447883-f03135ee-71f0-426f-ace7-7b77eb43259f.PNG)

* 이미 앞서 사용한 방식!(Grid 등)

#### Radar Plot

* 극좌표계를 사용하는 대표적 차트

* 별모양으로 생겨 Star Plot으로 불리기도 함

![radar](https://user-images.githubusercontent.com/59636424/132448018-78debe8b-299e-4eec-ada4-be57092486f7.PNG)

#### Radar Chart 주의점

* 각 feature는 독립ㅂ적이며, 척도가 같아야 함

(scale이 같아야 하는 것이 중요하다.)

* feature의 순서에 따라 많이 달라진다!

![ttt](https://user-images.githubusercontent.com/59636424/132448202-6639d632-4e38-461e-9bc9-d969a7bc3d1c.PNG)

* feature가 많아질수록 가독성이 떨어짐

#### 실습

~~~
ax.set_rticks([1, 1.5, 2]) # 좌표점 찍기!
~~~

~~~
fig = plt.figure()
ax = fig.add_subplot(111, polar=True)
ax.set_rmax(2) # 이거 없어도 그릴 수 있다!
ax.set_rticks([1, 1.5, 2])
plt.show()
~~~

![wwww](https://user-images.githubusercontent.com/59636424/132448581-f67204f0-b180-4a11-965a-881cd0b2d15a.png)

~~~
ax.set_rlabel_position(-90) # label 적히는 위치 조정
~~~

~~~
# 45~135도 만큼만 출력(부채꼴) -> 관찰하고자 하는 객체가 특정 부분에만 보인다면 이렇게도 가능
ax.set_thetamin(45)
ax.set_thetamax(135)
~~~

![qnco](https://user-images.githubusercontent.com/59636424/132448849-172946ef-3c31-4ed9-89e8-40121f90a092.png)

### 5-2. Pie Charts

#### 1.1 Pie Chart

* 원을 부채꼴로 분할하여 표현하는 통계 차트

#### 2-1. Donut Chart

* 중간이 비어있는 Pie chart

#### 2-2. Sunburst Chart

* 햇살을 닮은 차트

* 계층적 데이터를 시각화하는데 사용

### 5-3. 다양한 시각화 라이브러리

#### 1-1. Missingno

* 결측치를 체크하는 시각화 리이브러리

* 빠르게 결측치의 분포를 확인하고 싶을 때 사용 가능!

#### 1-2. Treemap

* 계층적 데이터를 직사각형을 사용하여 포함 관계를 표현한 시각화 방법

#### 1-3. Waffle Chart

* 와플 형태로 discrete하게 값을 나타내는 차트

#### 1-4. Venn

* 집합 등에서 사용하는 익숙한 벤 다이어그램


# 3. 과제

## Subword-level Language Model with LSTM

* Subword란?

하나의 단어를 여러개의 단위로 분리했을 때 하나의 단위를 나타낸다.

* tokenization이란?

주어진 입력 데이터를 자연어처리 모델이 인식할 수 있는 단위로 변환해주는 방법

* subword tokenization

말 그대로 subword 단위로 tokenization을 한다는 말로 단어를 매우 잘게 쪼갠다.

---

과제에서 train.txt를 word tokenization을 하게 되면 word embedding parameter 수가 33278 x 200 = 6655600개이다.

그에 비해 RNN parameter 개수는 890880개로 2개를 비교해보면 월등히 word embedding parameter 개수가 많음을 알 수 있다!!

**이런 parameter 비중의 비대칭성을 해결하기 위해서 한 글자씩 자르는 character-level tokenization 방법이 주목 받았지만 sequence길이가 길고 성능이 저하되었다.**

**그래서 subword 단위로 tokenization을 하려고 한다!!**

**Bert 모델의 subword tokenization 알고리즘으로 잘라 embedding parameter 개수를 확인하니 6655600 -> 4619000개로 줄어듬을 알 수 있다.**

### subword-level Language Model with LSTM 과제 목표!

subword tokenization을 쓰는 이유는 그냥 word tokenization의 단점인 word embedding parameter 수가 많음과 Character-level tokenization의 단점인 긴 sequence 길이를 보완하고자 subword tokenization이 필요합니다.
