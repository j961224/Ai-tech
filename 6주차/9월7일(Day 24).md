# 1. 9월 7일 배운 것!

## RNN

![diq](https://user-images.githubusercontent.com/59636424/132228393-5af476a8-cf0f-49e2-beb9-bd2ea6289f44.PNG)

    sequence 데이터가 입출력으로 주어진 상황에 각 timestep sequence x_t와 
    
    전 timestep rnn 모듈에서 계산한 hidden state vector h_(t-1)을 입력으로 받아
    
    현재 timestep에서의 h_t를 출력으로 낸다.

    (A는 RNN 모듈이다.) -> 매 timestep마다 재귀적으로 호출!(A 모듈의 출력이 다음 timestep의 입력으로 들어가므로)
    
    task에 맞는 출력값 y를 계산해줘야한다!

![sssss](https://user-images.githubusercontent.com/59636424/132248617-6e0a52c8-dbab-4470-b92d-40e82e2439a9.png)

    x_t: timestep에 input vector(입력 벡터)
    
    h_(t-1): 전 timestep RNN에서 계산된 hidden state vector
    
    f_W: x_t와 h_(t-1)를 입력으로 받는 parameter W(RNN에 필요한 linear transformation matrix를 정의하는 parameter)를 가지는 RNN 함수
    
    h_t: 출력으로는 현재 timestep t에 대한 hidden state vector
    
    y_t: timestep t에서 최종 예측값에 대한 output을 계산해야하는 경우(h_t를 바탕으로 y_t를 계산)

y_t는 매 timestep마다 계산할 수도 있고 마지막 timestep에만 계산할 수 있다. 

-> 단어별로 품사 예측 시, 매 timestep마다 각 단어의 품사 예측 값이 나와야한다.

-> 문장 긍부정 시, 마지막 단어까지 읽은 뒤에 마지막 timestep에 대해서만 긍/부정 예측

**모든 timestep에서 RNN 모듈 정의하는 W는 동일한 값을 공유한다.**

---

* f_W를 정의

![wqwqwqwqwqwqw](https://user-images.githubusercontent.com/59636424/132230639-e3b306a3-ade2-43ca-a605-b302df7c9827.PNG)

timestep x_t가 3차원 벡터라고 생각하고 h_(t-1)이 2차원 벡터인 경우, h_t는 h_(t-1)과 동일한 dimension을 공유하므로 2차원이어야 한다!

-> x_t와 h_(t-1)로 h_t를 계산한다. -> **tanh**를 거쳐 최종적인 h_t를 계산할 수 있다.

![hh_tt](https://user-images.githubusercontent.com/59636424/132232447-6bdafdef-5ef7-4d1c-8545-c797fd15b4d5.PNG)

x_t와 h_(t-1) 차원 3차원, 2차원이라면 linear transformation을 통해 h_t가 나오면 W는 2 x 5로 정의된다!

    그러면 3열까지 W_xh고 4~5열은 W_hh로 표현가능하다!

W_hy x h_t = y_t -> binary classification이라면 y_t는 1차원이고 multi면 class 갯수만큼 차원을 가진다!

## 2. Types of RNNs

![onone](https://user-images.githubusercontent.com/59636424/132234749-e30cc2d8-a4a5-4607-9832-de7901d26a30.PNG)

> * one-to-one
> > 일반적인 모델 구조
> * one to may
> > 이미지 캡션에 사용 (입력으로 하나의 이미지 -> 이미지 설명 예측 or 생성에 필요한 단어를 timestep별로 생성)
> * many to one
> > 마지막 timestep으로 출력을 내준다. (감정분석)
> * many to many
> > 입출력이 sequence (machine translation -> 번역기)
> * 또 다른 many to many
> > 단어별로 문장이나 품사를 예측하는 경우(PoS tagging) / video classification(비디오가 timestep 마다의 이미지 frame이라고 하면 해당 frame이 어떤 신(전쟁 등)에 해당하는지 분류)

## 3. Character-level Language Model

![ewewewew](https://user-images.githubusercontent.com/59636424/132236377-95dae19b-58be-4fde-bfb9-38ca29765478.PNG)

hello로 우선 Character level Vocabulary 구축! -> 이러한 단어를 one-hot vector로 표현 가능!

![ss](https://user-images.githubusercontent.com/59636424/132236549-2df73f5d-8814-4b76-95bf-342fd986e1b9.PNG)

이렇게 표현한 one-hot vector를 다음과 같이 다음 character를 예측할 수 있어야 한다.

![gh_t](https://user-images.githubusercontent.com/59636424/132237268-c182fd83-5645-471f-89ea-2782e1291090.PNG)

실제로 hidden state vector를 계산한 것이다!

![output vector](https://user-images.githubusercontent.com/59636424/132237734-0b046068-a6ab-418d-a399-3a80af9434ca.PNG)

output vector는 input vector 차원과 같아야한다! -> output layer는 softmax를 통과한 결과로 가장 큰 값을 가진 단어를 선택한다.

---

![wewewewe](https://user-images.githubusercontent.com/59636424/132238324-2a34579a-e061-4413-a066-c0c8645c2b21.PNG)

이러한 방식으로 다음날 주식값을 예측가능하다! (output을 다음 input으로 넣어준다 -> 재사용!)

---

* 긴 문단에 대해서도 학습하기!

![kkkk](https://user-images.githubusercontent.com/59636424/132239076-a65e6157-321e-4a4f-90ae-791e8a18c1a3.PNG)

예시는 셰익스피어 작품으로 여기서 쉼표, 줄바꿈도 다 vocabulary에 넣는다!

![wewewewewewe](https://user-images.githubusercontent.com/59636424/132239320-6a0e0e7e-d0b9-468c-817d-dbb4a747df15.PNG)

초반 iteration에서는 첫 문자로 차례차례 문자 예측 시에는 좋은 성능이 보이지 않지만 계속 학습할수록 점점 더 학습이 잘 되는 것이 확인 가능하다.

---

* 추가로 논문이나 C code를 가지고 RNN을 통해 학습이 가능하다!

## BPTT(Backpropagation through time): 시간을 거슬러 올라간다는 표현!

![qqqqq](https://user-images.githubusercontent.com/59636424/132240553-46828f95-5cee-4e2b-a628-946731175657.PNG)

timestep t에 대한 입력 벡터의 linear transformation matrix(W_xh), 이전 hidden vector의 linear transformation matrix(W_hh), 출력값을 도출하는데 쓰이는 linear transformation matrix(W_hy)는 backpropagation에 의해서 학습이 진행된다.

* **여기서 왜 Backpropagation이 쓰이는가요??**

    W(linear transformation matrix)가 출력값의 오차에 따라 업데이트를 해야하는데 이게 Backpropagation이 수행해준다.
    
    거슬러 올라가면서 그 자리의 계수들을 업데이트 해준다!

* **Truncated-Backpropagation Through Time(생략된 BPTT)**

![eeee](https://user-images.githubusercontent.com/59636424/132241167-0fed2e2e-4828-449f-9903-697afdefa8b7.PNG)

**sequence가 길어지면 한꺼번에 처리하기에는 힘들다! -> 잘라서 제한된 길이에 대한 sequence만으로 학습하려한다!**

: 제한된 sequence를 7개로 1번에 학습하는 것을 한정한다면 7개씩 backpropagation으로 RNN parameter를 학습한다!

-> 이것이 **Truncated-Backpropagation Through Time(생략된-BPTT)**

**한 번더 말하자면 생략된 BPTT는 일정단위씩 끊어 역전파를 계산하며 그러므로 일정단위마다 오차를 다시 계산해줘야한다**

![wewewe](https://user-images.githubusercontent.com/59636424/132247046-e66008f9-a20f-41c5-8d94-74247354b446.png)

위의 사진은 10개씩 끊어 역전파를 해준 모습이다!

    
**RNN의 필요한 정보를 저장 공간은 매 timestep마다 update하는 hidden state vector이다.**

![erretet](https://user-images.githubusercontent.com/59636424/132241755-7fc03d4f-af39-47ad-81ee-ae77bc94137c.PNG)

특정한 hidden state dimension을 고정하고 해당 dimension 값이 어떻게 변화하는지를 크기가 마이너스로 크면 파란색이고 플러스로 크면 빨간색이다.

## Vanishing/Exploding Gradient Problem in RNN

![rtrtrt](https://user-images.githubusercontent.com/59636424/132244175-465a4d4d-0d47-4732-9ad5-50ce80b60d67.PNG)

original RNN은 동일한 matrix를 매 timestep에 곱한다.

-> W_hh가 반복적으로 곱함으로써(같은 숫자 계속 곱함) 기하급수적으로 커지거나 작아지는(공비가 1보다 작을 때) 패턴을 보인다.


![erererer](https://user-images.githubusercontent.com/59636424/132244530-b4965a92-5ad3-477b-b318-45cabeff8656.PNG)

y에서 gradient가 발생되었을때, backpropagation되서 h1까지 전달되면 h3에 대한 h1의 편미분을 구해준다!

-> 그렇게 되면, 미분과정에서 timestep을 거슬러 올라갈수록 3이 반복적으로 곱해지며 **gradient 계산 시에도 timestep 갯수만큼 3이 거듭제곱이 되어 증폭된다!**

* 만약 w_hh가 0.2라면??

    timestep의 차이만큼 곱해지며 0.2의 거듭제곱이 되면서 0으로 줄어든다.
    
**따라서 모델을 적절히 학습하는데 필요로하는데 gradient를 2 timestep을 넘어서 전달을 하게 되면 gradient가 기하급수적으로 커지거나 작아져 학습이 잘 안 된다.**

---

* **RNN에서는 tanh activation function을 사용했는데 왜 사용했을까?**

핵심만 먼저 말하자면 RNN의 기울기 소실 문제를 예방하고자 **gradient가 최대한 오래 유지될 수 있도록하기 위해 tanh가 적합**하다!

우선 **RELU를 쓰게 되면 0이상이라면 y=x이니 발산한 확률이 높아서 사용하지 않게 된다.** -> normalize가 되지 않는다!

* **그러면 값을 0~1로 normalize가 되는 sigmoid는??!**

우선 sigmoid와 tanh 미분한 그래프를 보자

![tanh](https://user-images.githubusercontent.com/59636424/132249100-7d7eefa7-417f-4979-a434-09f07ce24dd8.jpg)

그래프를 보면 tanh의 미분 최대값이 sigmoid에 비해 크다는 것을 알 수 있으니 **tanh에 비해 sigmoid가 기울기 소실 문제가 생길 가능성이 높다!**

그래서 **그나마 tanh를 이용해 gradient를 최대한 유지하려고 사용한다!**

---

## 4. Long Short-Term Memory(LSTM) / Gated Recurrent Unit (GRU)

### LSTM

**original RNN의 문제인 기울기 소실 문제와 Timestep이 먼 경우에 필요로 하는 정보를 보다 효과적으로 처리(Long Term Dependency 개선)**

![qwqwqwqwqwqwqwqw](https://user-images.githubusercontent.com/59636424/132274065-d2931a0d-0996-4fc4-b7d2-40d5b69a9db5.PNG)

        LSTM은 전 timestep에서 넘어오는 정보가 2가지(C_(t-1)(Cell state vector)와 h_(t-1))가 들어온다!
       
        핵심정보는 무엇이냐 하면 cell state vector가 필요로하는 정보를 담고 있다.
        
        hidden state vector는 cell state vector를 1번 더 가공해서 노출할 필요가 있는 정보만을 가진다.

![hey](https://user-images.githubusercontent.com/59636424/132274930-4b3da713-0d9d-43eb-9fcd-c1ab0ee419c4.PNG)

우선, x_t와 h_(t-1)을 입력으로 받아 선형변환시킨다! (여기서는 x_t와 h_(t-1)의 차원은 h라고 한다!)

-> 이 결과를 4개의 vector로 쪼개고 각각의 vector는 sigmoid와 마지막에 tanh로 output vector값들을 구한다!

-> sigmoid로 나오는 3개의 vector는 0~1사이 값으로 가지며 이 output vector들은 원래 값의 얼마만큼을 가져갈지를 정해준다! 

-> tanh로 나오는 1개의 vector는 -1~1사이 값을 가지며 이는 현재 timestep에서 유의미한 정보를 정해준다!


### 앞에서 말한 i,f,o,g 역할은?

전 timestep에서 넘어온 cell state vector(C_(t-1))를 적절히 변환하는데 사용!

### Forget gate

![wow](https://user-images.githubusercontent.com/59636424/132276058-fe5acdd6-fb56-427f-ad43-ad72fbd5fa28.PNG)

전 timestep의 hidden state인 h_(t-1)과 현 timestep인 input vector(x_t)는 sigmoid를 거쳐 얼마나 기억을 할지 남긴다!

-> **sigmoid를 거친 값은 cell state vector와 곱하여 보존할 값을 구해준다!** (전 timestep의 값들을 얼마나 남길지!)

### Gate gate(input gate & update gate)

![얍](https://user-images.githubusercontent.com/59636424/132277656-713b3037-02e7-48df-9b30-53596249da81.PNG)

C_t의 틸다는 tanh를 통해 -1~1값을 가진 vector에 i_t라는 vector를 곱해준다!

-> 이렇게 하는 이유는 더해주고자 하는 값보다 큰 정보를 C_t 틸다로 만들어주고 그 값에서 특정 비율 만큼 정보를 덜어내서 C_(t-1)에 더해주고자 한다.

이렇게 **어떤 값을 업데이트 할 것인지 결정하는 Sigmoid를 거친 값인 input gate 값**과 **후보값들의 벡터인 tanh를 거친 값인 update gate 값**으로 업데이트하기로 한 값을 얼마나 업데이트할 지 정한 만큼 scale한 값이 된다!

**새로운 값 cell state에 추가해주기!!**

### Output gate

![outputlayer](https://user-images.githubusercontent.com/59636424/132279261-fc0a9422-ca81-40f8-a51a-8c11fecc3654.PNG)

C_t는 기억해야할 모든 정보를 담았고 tanh로 -1~1 범위로 만들어주고, o_t는 cell state의 어느 부분을 output으로 내보낼지 결정하며 sigmoid를 씌워준다.

-> **이렇게 C_t와 o_t를 곱해서 output으로 보내고자 하는 부분을 구해준다! (h_t)**

-> **이러한 h_t는 다음 hidden state와 output으로 나온다.**

---

### GRU(Gated Recurrent Unit)

**LSTM모델 구조를 경량화해서 적은 메모리와 빠른 계산을 할 수 있도록 만들었다!**

#### GRU 핵심!

* **LSTM의 forget gate와 input gate를 통합하여 하나의 'update gate'를 만든다.**

* **Cell State와 Hidden State를 통합한다.**

#### GRU 구조

![www](https://user-images.githubusercontent.com/59636424/132281023-d6c4039a-8214-4301-bc76-e3514ec3150b.png)

**GRU의 특징은 hidden state와 cell state를 같이 쓰며 hidden state(GRU는 LSTM의 cell state와 비슷한 역할을 함)로 쓴다.**


> * Reset gate(r_t)
> > 이전의 정보를 얼마나 잊을지 결정
> * Update gate(z_t)
> > 이전의 정보를 얼마나 유지할지 결정
> * Candidate hidden state(h_t 틸다)(updat gate를 아직 반영하지 않았기 때문에 Candidate hidden state이다.)
> > reset gate인 r_t와 h_(t-1)을 곱하여, 이전 timestep에서 무엇을 지워야할지를 결정
> > 
> > 현재 입력값 x_t를 linear transformation matrix로 곱한 것을 더해 이전 timestep에서 무엇을 지워야할지와 현재 timestep 입력값을 가지고 tanh로 -1에서 1사이 값으로 변형시킨다.
> * hidden state(h_t)
> > update gate를 할 차례로 새로운 hidden state(h_t 틸다)와 이전 hidden state(h_(t-1))를 각각 얼마나 사용(유지)할 것인지를 z_t로 비중을 주어 계산한다.

### Backpropagation in LSTM? GRU

**RNN은 동일한 W_hh를 계속 곱했다!**

하지만!!!!, **LSTM은 전 timestep의 cell state vector에서 그때그때 forget gate를 곱하고 필요로 하는 정보를 덧셈을 통해 원하는 정보를 만들어 기울기 소실 문제를 해결**했다.

-> 덧셈연산으로 멀리 있는 timestep에까지 **gradient를 큰 변형없이 전달이 가능하므로 Longterm Dependency를 해결!**

### Summary on RNN/LSTM/GRU

* RNN은 다양한 길이 sequence 데이터에 특화되어 있다!

* original RNN은 구조가 간단하지만 기울기 소실 문제가 있다!

* LSTM과 GRU를 이용해서 기울기 소실 문제와 Longterm Dependency를 해결했다.

---

## Further Question

### BPTT 이외에 RNN/LSTM/GRU의 구조를 유지하면서 gradient vanishing/exploding 문제를 완화할 수 있는 방법이 있을까요?

#### Gradient Clipping(그래디언트 클리핑)

**기울기 값을 자르는 것을 의미**하고 기울기 폭주를 막기 위해 **임계값을 넘지 않도록 값을 자른다!**

=> 임계치만큼 크기를 감소시킨다!

=> RNN에서 매우 유용!

~~~
from tensorflow.keras import optimizers
Adam = optimizers.Adam(lr=0.0001, clipnorm=1.)
~~~

#### Weight initialization(가중치 초기화)

훈련 초기에 가중치 초기화로 기울기 소실 문제를 완화시킬 수 있다.

> * Xavier Initialization(세이비어 초기화)
> > 여러 층의 기울기 분산 사이에 균형을 맞춰서 특정 층이 너무 주목을 받거나 다른 층이 뒤쳐지는 것을 막습니다.
> * He initialization(He 초기화)
> > Xavier Initialziation에서 ReLU나 ReLU 변형 함수에는 성능이 좋지 않아 이에 좋은 초기화 방법이다.

* Xaiver Initialization을 efficientnet_b3에 적용한 예시

~~~
torch.nn.init.xavier_uniform_(model.classifier.weight)
stdv = 1. / math.sqrt(model.classifier.weight.size(1))
model.classifier.bias.data.uniform_(-stdv, stdv)
model.classifier = MyModel().to(device=device) # Multi Sample Dropout 사용
~~~

#### skip-connection으로 이전 정보를 넘겨주는 방법?!

참고한 링크: https://wikidocs.net/61375

### RNN/LSTM/GRU 기반의 Language Model에서 초반 time step의 정보를 전달하기 어려운 점을 완화할 수 있는 방법이 있을까요?

#### Teacher forcing

**Teacher forcing이란?**

입력을 넣어 출력이 나오면 그 출력을 쓰지 않고 **Ground Truth를 입력**으로 넣는다!!


* 여기서 Ground Truth란?

        우리가 정한 정답이자 모델이 우리가 원하는 답으로 예측해주길 바라는 답이다!


**그러므로 여기서 적용하면 teacher forcing으로 초반에는 출력 단어를 쓰지 않고 원래 정답인 단어를 입력으로 넣을 수 있다.**


### 실습 코드 중 몰랐던 내용 정리

* **pack_padded_sequence**: 타임스텝(배치내에서 문장의 최대 길이) 마다 일련의 단어를 처리(padding하면서 짧은 부분은 0으로 합쳤는데 그 곳은 연산하지 않는다!)

~~~
packed_batch = pack_padded_sequence(sorted_batch_emb.transpose(0,1), sorted_lens)
~~~

~~~
packed_batch = pack_padded_sequence(sorted_batch_emb, sorted_lens, batch_first=True)
~~~
