# 1. 9월 9일 배운 것!

## 2. Beam search

### Greedy decoding

매 timestep마다 높은 확률을 가지는 단어를 출력하는데 이것을 Greedy decoding이다.

**현재 timestep에서 가장 좋아보이는 단어를 그때그때 선택하는 것이다!**

![tttttttt](https://user-images.githubusercontent.com/59636424/132607916-19ee4287-3110-4200-bac2-962b3e988f51.PNG)

a가 생성된 것을 바탕으로 뒤에 단어를 생성해야하는데 이 때는 뒤로 돌아갈 수가 없다! -> 최적의 예측값을 도출할 수 없다.

### Exhaustive search

![kkkk](https://user-images.githubusercontent.com/59636424/132608169-165d56da-771b-476e-b1b2-291436a8a7bd.PNG)

P(y_2 | y_1, x) -> x라는 입력 문장과 y_1까지 생성 후, y_2를 생성할 확률

* timestep t까지의 모든 경우를 따지면, 매 timestep마다 고를 수 있는 단어의 수가 vocabulary 사이즈가 된다! (V^t으로 모든 가지수!)

=> 많은 시간이 걸린다!

### Beam search

차선책으로 Greedy decoding과 Exhaustive search의 중간 위치이다!

**decoder의 매 timestep마다 k개 가능한 가지수를 고려하려 한다!**

=> k개 경우의 수를 유지하고 마지막까지 decoding을 진행한 후, 최종 k개 candidate 중에서 가장 확률이 높은 것을 선택!

k(Beam size: 5~10개)개 경우의 수에 해당하는 decoding의 output을 하나의 가설이라고 한다!!!!!!

![xxxxxxxxxxx](https://user-images.githubusercontent.com/59636424/132609877-53198dab-4112-4380-88f3-436bfa628792.PNG)

log를 사용하면 더한 값이 된다!

=> log는 단조증가함수이므로 확률값이 가장 큰 경우에는 log를 취한 값도 다른 경우와 비교할 때 가장 큰 값으로 유지된다!!!

![rrrrr](https://user-images.githubusercontent.com/59636424/132611294-3ab726dd-0ef9-430d-8715-5ad2ef89358d.PNG)

이는 단조증가하는 log 함수 그래프 형태이다.

**모든 경우의 수를 다 따지는 것보다는 효율이 좋다.**

### Beam search: Example

* Beam size: k=2

![wwwewewewewewe](https://user-images.githubusercontent.com/59636424/132610781-5f17dfb0-23b5-4fd8-9719-2672c381598c.PNG)

=> Greedy decoding으로는 하나의 가장 큰 확률값을 뽑겠지만 여기서는 확률값이 높은 2개 단어를 뽑는다. (여기서는 he와 I)

![qwqwqwqw](https://user-images.githubusercontent.com/59636424/132611583-1cde56f8-6ebd-4567-ba99-633cf1e3526f.PNG)

### Beam search: Stopping criterion

* greedy decoding

<END> token을 해당 timestep에 예측하면 끝!
  
* Beam search decoding
  
서로 다른 경로 및 가설이 있으므로 다른 시점에서 <END> token 생성
  
=> 어떤 가설이 <END>를 생성하면 그 경로는 생성을 멈추고 완료! => 저장 공간에 임시로 저장!

---

beam search는 T라는 timestep의 최대값이 있다면, 거기까지 decoding함으로써 beam seaarch 과정을 중단하거나 임시 저장 공간에 저장해둔 완료된 가설이 저장된 것이 n개 만큼 저장하면 중단!
  
### Beam search: Finishing up
  
가장 큰 score를 뽑아야 한다! -> k개 중 joint 확률이 가장 높은 것을 뽑아낸다!

**문제는 짧은 길이를 가진 것이 joint 확률이 높을 것이고 긴 길이는 상대적을 확률이 낮게 나올 것이다. (단어 생성 시 동시 사건 확률을 고려하므로 기존 log 확률값에 항상 마이너스를 더하는 값이 된다.)**
  
-> **이유는 log 값이 0~1사이는 무조건 마이너스 값이므로!**
  
-> **좀 더 정규화를 위해 평균 확률값을 구하고자 한다!**

---
  
### 3. BLEU score
  
생성 모델 품질 및 정확도 평가 척도!

![xxxx](https://user-images.githubusercontent.com/59636424/132613689-5b203176-5f7a-4a8c-b27d-6cf72a6e90ba.PNG)

: 이러한 경우에는 단어 하나씩 밀리게 되어 정답을 맞추지 못한 경우가 될 수 있다.
  
=> **생성된 문장 전체를 보고 두 문장을 비교하는 것이 아니라 고정된 위치에서 정해진 단어 하나가 나와야하는 단순화 평가 방식 때문이다.**
  
**생성 sequence 문장과 Groud Truth를 전체 차원에서 평가할 필요가 있다.**

![ㅌ](https://user-images.githubusercontent.com/59636424/132614185-eec77dc9-c4a4-42b6-9117-c99ff470e5ea.PNG)

#### precision(정밀도)

![정밀도!](https://user-images.githubusercontent.com/59636424/132614063-5586d4ec-ee25-4312-832f-361515922180.PNG)

예측 문장 길이 기준으로 계산 -> 실직적으로 느끼는 정확도
  
#### recall(재현률)
  
![ㅊㅊ](https://user-images.githubusercontent.com/59636424/132614069-f65cfa6e-c443-48d5-9910-a924ba627f97.PNG)

Ground Truth 문장 길이 기준으로 계산
  
    ex) 검색 시스템에서 키워드를 가지고 검색 시, 실제 관련 키워드 문서들이 10개가 있다면 7개가 나오면 나머지 3개의 실제로 검색 키워드 부합해서 나와야하는 문서들은 실제로 우리에게는 노출되지 않는다.
  
    이것을 해결해주는 방법이다!

#### F-measure
  
* 산술 평균, 기하 평균, 조화 평균
  
![ㄱㅎ](https://user-images.githubusercontent.com/59636424/132614842-5867c9f8-738f-45fd-a409-e2ed832c1b21.PNG)

조화, 기하 평균은 작은 값에 더 가까운 형태를 보여준다.(작은 곳에 더 많은 가중치를 준다.)
  
**F-measure는 조화 평균을 이용한다!**
  
#### Precision and Recall
  
![ㅊ](https://user-images.githubusercontent.com/59636424/132615278-ffaef084-59cc-492e-9c81-c223bd097774.PNG)

이 때, 모델 2에서 예측값은 전혀 문법적으로 말이 되지 않는다. (지표 상의 성능은 좋아보이지만)

### BLEU score
  
앞의 단점을 완화하기 위한 BLEU score

**개별 단어 레벨에서 얼마나 공통적으로 Ground Truth문장과 겹치는 단어가 나왔냐는 것 뿐만 아니라 n-gram으로 연속된 n개 단어들이 Ground Truth와 얼마나 겹치는지 계산해서 평가 지표에 적용!**

![ㄷㄷㄷㄷㄷ](https://user-images.githubusercontent.com/59636424/132616145-3c543752-e980-4350-8cd9-fdb7d8f6d138.PNG)

영어를 한국어로 번역 시, '정말'이라는 단어를 예측 못 했지만 영향이 적은 단어이므로 잘 됐다고 할 수 있다.

-> 번역에서는 precision 만 고려!!
  
  이유 1.  번역 결과만을 보고 직접 느낄 수 있는 precision만 고려
  
  이유 2.  '영화'를 '노래'로 번역하면 이는 오역이다.
  
* **n-gram으로 precision을 고려하기!**
  
![ㅊㅊ](https://user-images.githubusercontent.com/59636424/132617064-34510151-bdf7-451e-bd12-0a0c152e5db6.PNG)
  
**n-gram을 1~4개까지 고려하면 각 경우의 preicsion을 계산하여 곱한 후에 1/4승을 한다! (기하평균)**
  
-> 조화평균인 경우, 작은 값에 가중치가 세므로!
  
추가적으로 **Brevity Penalty**라는 것을 사용! 

* Brevity Penalty란?
  
    짧은 문장을 생성 시(reference 길이 보다), 1보다 작은 경우가 나온다!
  
    reference 길이보다 큰 값이 나온다면 1보다 커지므로 min을 커지면 1이 나온다!
  
    길이만을 고려했을 시, Groud Truth 보다 짧은 문장은 그 비율만큼 기하 평균 계산한 precision 값을 낮춰주려고 한다!
  
    이 penalty는 recall의 최대값을 의미한다! (10개 단어의 Ground truth와 생성한 문장이 10개 단어라면 실제로 recall 계산 전에 100% recall을 기대할 수 있다.)
  
    만약에, 예측 문장 단어가 더 많다면, Ground Truth 단어들을 모두 다 소환했다고 이상적인 경우를 생각할 수 있으므로 1을 출력

**Brevity Penalty는 recall를 조금 고려한 Penalty라고 할 수 있다.**
  
---
  
#### BLEU score 예시

![ㅂㅂ](https://user-images.githubusercontent.com/59636424/132618696-032b9ecd-3d4d-400d-8992-3fd1dae13dc4.PNG)

앞서, Precision과 Recall을 이용한 F1-measure를 이용했을 때, Model 2의 지표가 높았지만 BLEU score를 거치므로 결과가 안 좋음을 알 수 있다.

  
# 3. 실습 및 과제
  
## 실습 (Seq2seq + Attention)
  
* Encoder

~~~
class Encoder(nn.Module):
  def __init__(self):
    super(Encoder, self).__init__()

    self.embedding = nn.Embedding(vocab_size, embedding_size)
    self.gru = nn.GRU(
        input_size=embedding_size, 
        hidden_size=hidden_size,
        num_layers=num_layers,
        bidirectional=True if num_dirs > 1 else False,
        dropout=dropout
    )
    self.linear = nn.Linear(num_dirs * hidden_size, hidden_size)

  def forward(self, batch, batch_lens):  # batch: (B, S_L), batch_lens: (B)
    # d_w: word embedding size
    batch_emb = self.embedding(batch)  # (B, S_L, d_w)
    batch_emb = batch_emb.transpose(0, 1)  # (S_L, B, d_w)

    packed_input = pack_padded_sequence(batch_emb, batch_lens)

    h_0 = torch.zeros((num_layers * num_dirs, batch.shape[0], hidden_size))  # (num_layers*num_dirs, B, d_h) = (4, B, d_h)
    packed_outputs, h_n = self.gru(packed_input, h_0)  # h_n: (4, B, d_h)
    outputs = pad_packed_sequence(packed_outputs)[0]  # outputs: (S_L, B, 2d_h)
    outputs = torch.tanh(self.linear(outputs))  # (S_L, B, d_h)

    # 정상적인 sequence Input과 sequence의 순서를 뒤집은 Input이 존재 
    # 마지막 layer의 hidden state를 각각 구하고 마지막에 합쳐준다. (마지막 순/역전파)
    forward_hidden = h_n[-2, :, :]
    backward_hidden = h_n[-1, :, :]
    hidden = torch.tanh(self.linear(torch.cat((forward_hidden, backward_hidden), dim=-1))).unsqueeze(0)  # (1, B, d_h)

    return outputs, hidden
~~~
  
* Decoder

~~~
class Decoder(nn.Module):
  def __init__(self, attention):
    super().__init__()

    self.embedding = nn.Embedding(vocab_size, embedding_size)
    self.attention = attention
    self.rnn = nn.GRU(
        embedding_size,
        hidden_size
    )
    self.output_linear = nn.Linear(2*hidden_size, vocab_size)

  def forward(self, batch, encoder_outputs, hidden):  # batch: (B), encoder_outputs: (L, B, d_h), hidden: (1, B, d_h)  
    batch_emb = self.embedding(batch)  # (B, d_w)
    batch_emb = batch_emb.unsqueeze(0)  # (1, B, d_w)

    outputs, hidden = self.rnn(batch_emb, hidden)  # (1, B, d_h), (1, B, d_h)

    attn_values, attn_scores = self.attention(hidden, encoder_outputs)  # (B, d_h), (B, S_L)
    concat_outputs = torch.cat((outputs, attn_values.unsqueeze(0)), dim=-1)  # (1, B, 2d_h)
    #concat_outputs = (1,10,512) size와 (1,10,512) size를 합치니 -> (1,10,1024)

    return self.output_linear(concat_outputs).squeeze(0), hidden  # (B, V) = (10, 512), (1, B, d_h) = (1,10,512)
~~~
  
    attention으로 구한 attention values와 decoder hidden state vector와 concat하여 linear를 통해 최종 결과값을 도출한다.

* Attention
  
~~~
#위의 그림에서 Scale만 빼면 된다!
class DotAttention(nn.Module):
  def __init__(self):
    super().__init__()

  #Query vector는 현재 단어(영향을 받는 단어 변수/질문), (지금 decoder에서 이런 값이 나왔는데 무엇이 output이 돼야 할까)
  #Key vector는 점수를 매기려는 다른 위치에 있는 단어(영향을 주는 변수/물어보는 단어)
  #Value vector는 입력의 각 단어(그 영향에 대한 가중치/질문에 대한 답)
  def forward(self, decoder_hidden, encoder_outputs):  # (1, B, d_h), (S_L, B, d_h)
    # decoder_hidden 사이즈: torch.Size([1, 10, 512])
    # encoder_outputs 사이즈: torch.Size([15, 10, 512]) / S_L=15

    # Query: 다른 단어와의 관계를 알아보려는 특정 단어
    # decoder_hidden이 token으로 들어와 다음 예상 단어를 예측하니 역할 부합
    # decoder_hidden은 decoder class에서 batch(decoder start token과 같은 것)로 들어오는 것
    query = decoder_hidden.squeeze(0)  # (B, d_h) 

    # key: 위에서 말한 Query가 다른 단어와의 관계를 알아보는데 거기서 말한 다른 단어
    key = encoder_outputs.transpose(0, 1)  # (B, S_L, d_h)

    # query:  torch.Size([10, 512])
    # key:  torch.Size([10, 15, 512])
    # query.unsqueeze(1): torch.Size([10, 1, 512])

    energy = torch.sum(torch.mul(key, query.unsqueeze(1)), dim=-1)  # (B, S_L)

    # energy:  torch.Size([10, 15])
    
    attn_scores = F.softmax(energy, dim=-1)  # (B, S_L)
    attn_values = torch.sum(torch.mul(encoder_outputs.transpose(0, 1), attn_scores.unsqueeze(2)), dim=1)  # (B, d_h)
    # attn_scores: torch.Size([10,15])
    # attn_scores.unsqueeze(2): torch.Size([10,15,1])
    # encoder_outputs.transpose(0, 1) 사이즈: torch.Size([10, 15, 512])

    # attn_values: torch.Size([10, 512])

    return attn_values, attn_scores
~~~

    우선 decoder에서 구한 decoder hidden state vector와 각 encoder들의 hidden state vector들과 곱한다.(내적)
  
    그 값을 softamx를 취하면 확률값이 나타나는데 그 값을 encoder들의 hidden state vector과 곱하여 최종 attention values(attention output)를 구할 수 있다.
  
  
* Seq2seq

~~~
class Seq2seq(nn.Module):
  def __init__(self, encoder, decoder):
    super(Seq2seq, self).__init__()

    self.encoder = encoder
    self.decoder = decoder

  def forward(self, src_batch, src_batch_lens, trg_batch, teacher_forcing_prob=0.5):
    # src_batch: (B, S_L), src_batch_lens: (B), trg_batch: (B, T_L)

    # encoder를 다 수행!
    encoder_outputs, hidden = self.encoder(src_batch, src_batch_lens)  # encoder_outputs: (S_L, B, d_h), hidden: (1, B, d_h)

    input_ids = trg_batch[:, 0]  # (B) # [1,1,1,1...]로 모두 1인 배열로 start_token인거 같다. (한 timestep이 10)
    batch_size = src_batch.shape[0] # 10개 sequence
    outputs = torch.zeros(trg_max_len, batch_size, vocab_size)  # (T_L, B, V) / T_L=22, B=10, V=100

    for t in range(1, trg_max_len):
      #decoder_outputs은 강의노트의 attention output과 h_1^(d)를 concat한 값
      decoder_outputs, hidden = self.decoder(input_ids, encoder_outputs, hidden)  # decoder_outputs: (B, V), hidden: (1, B, d_h)

      outputs[t] = decoder_outputs
      _, top_ids = torch.max(decoder_outputs, dim=-1)  # top_ids: (B)

      # teacher forcing을 할 시, target 값 넣어주기 -> teacher forcing을 랜덤으로 하네...
      input_ids = trg_batch[:, t] if random.random() > teacher_forcing_prob else top_ids

    return outputs # (22,10,100)
~~~

    우선, encoder_outputs(encoder의 hidden state vector들)을 구했다.
  
    input_ids는 target_batch 첫 열을 뽑는데 이것은 start token 인 것 같다. ([1,1,1...])
  
    그리고 outputs에 최종값들을 저장할 변수를 설정한다.
  
    for문으로 0이 start token이니 1부터 target length까지 돌리면서 decoder로 그 timestep의 output을 구해 max를 통해 가장 큰 값을 고른다.
  
    그 값이 예측한 단어와 매칭되는 index이고 teacher forcing을 랜덤으로 하는데 하게 되면 해당 timestep의 정답을 넣어 다음 decoder에 쓰인다.
