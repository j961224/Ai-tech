# 1. 9월 9일 배운 것!

## 2. Beam search

### Greedy decoding

매 timestep마다 높은 확률을 가지는 단어를 출력하는데 이것을 Greedy decoding이다.

**현재 timestep에서 가장 좋아보이는 단어를 그때그때 선택하는 것이다!**

![tttttttt](https://user-images.githubusercontent.com/59636424/132607916-19ee4287-3110-4200-bac2-962b3e988f51.PNG)

a가 생성된 것을 바탕으로 뒤에 단어를 생성해야하는데 이 때는 뒤로 돌아갈 수가 없다! -> 최적의 예측값을 도출할 수 없다.

### Exhaustive search

![kkkk](https://user-images.githubusercontent.com/59636424/132608169-165d56da-771b-476e-b1b2-291436a8a7bd.PNG)

P(y_2 | y_1, x) -> x라는 입력 문장과 y_1까지 생성 후, y_2를 생성할 확률

* timestep t까지의 모든 경우를 따지면, 매 timestep마다 고를 수 있는 단어의 수가 vocabulary 사이즈가 된다! (V^t으로 모든 가지수!)

=> 많은 시간이 걸린다!

### Beam search

차선책으로 Greedy decoding과 Exhaustive search의 중간 위치이다!

**decoder의 매 timestep마다 k개 가능한 가지수를 고려하려 한다!**

=> k개 경우의 수를 유지하고 마지막까지 decoding을 진행한 후, 최종 k개 candidate 중에서 가장 확률이 높은 것을 선택!

k(Beam size: 5~10개)개 경우의 수에 해당하는 decoding의 output을 하나의 가설이라고 한다!!!!!!

![xxxxxxxxxxx](https://user-images.githubusercontent.com/59636424/132609877-53198dab-4112-4380-88f3-436bfa628792.PNG)

log를 사용하면 더한 값이 된다!

=> log는 단조증가함수이므로 확률값이 가장 큰 경우에는 log를 취한 값도 다른 경우와 비교할 때 가장 큰 값으로 유지된다!!!

![rrrrr](https://user-images.githubusercontent.com/59636424/132611294-3ab726dd-0ef9-430d-8715-5ad2ef89358d.PNG)

이는 단조증가하는 log 함수 그래프 형태이다.

**모든 경우의 수를 다 따지는 것보다는 효율이 좋다.**

### Beam search: Example

* Beam size: k=2

![wwwewewewewewe](https://user-images.githubusercontent.com/59636424/132610781-5f17dfb0-23b5-4fd8-9719-2672c381598c.PNG)

=> Greedy decoding으로는 하나의 가장 큰 확률값을 뽑겠지만 여기서는 확률값이 높은 2개 단어를 뽑는다. (여기서는 he와 I)

![qwqwqwqw](https://user-images.githubusercontent.com/59636424/132611583-1cde56f8-6ebd-4567-ba99-633cf1e3526f.PNG)

### Beam search: Stopping criterion

* greedy decoding

<END> token을 해당 timestep에 예측하면 끝!
  
* Beam search decoding
  
서로 다른 경로 및 가설이 있으므로 다른 시점에서 <END> token 생성
  
=> 어떤 가설이 <END>를 생성하면 그 경로는 생성을 멈추고 완료! => 저장 공간에 임시로 저장!

---

beam search는 T라는 timestep의 최대값이 있다면, 거기까지 decoding함으로써 beam seaarch 과정을 중단하거나 임시 저장 공간에 저장해둔 완료된 가설이 저장된 것이 n개 만큼 저장하면 중단!
  
### Beam search: Finishing up
  
가장 큰 score를 뽑아야 한다! -> k개 중 joint 확률이 가장 높은 것을 뽑아낸다!

**문제는 짧은 길이를 가진 것이 joint 확률이 높을 것이고 긴 길이는 상대적을 확률이 낮게 나올 것이다. (단어 생성 시 동시 사건 확률을 고려하므로 기존 log 확률값에 항상 마이너스를 더하는 값이 된다.)**
  
-> **이유는 log 값이 0~1사이는 무조건 마이너스 값이므로!**
  
-> **좀 더 정규화를 위해 평균 확률값을 구하고자 한다!**

---
  
### 3. BLEU score
  
생성 모델 품질 및 정확도 평가 척도!

![xxxx](https://user-images.githubusercontent.com/59636424/132613689-5b203176-5f7a-4a8c-b27d-6cf72a6e90ba.PNG)

: 이러한 경우에는 단어 하나씩 밀리게 되어 정답을 맞추지 못한 경우가 될 수 있다.
  
=> **생성된 문장 전체를 보고 두 문장을 비교하는 것이 아니라 고정된 위치에서 정해진 단어 하나가 나와야하는 단순화 평가 방식 때문이다.**
  
**생성 sequence 문장과 Groud Truth를 전체 차원에서 평가할 필요가 있다.**

![ㅌ](https://user-images.githubusercontent.com/59636424/132614185-eec77dc9-c4a4-42b6-9117-c99ff470e5ea.PNG)

#### precision(정밀도)

![정밀도!](https://user-images.githubusercontent.com/59636424/132614063-5586d4ec-ee25-4312-832f-361515922180.PNG)

예측 문장 길이 기준으로 계산 -> 실직적으로 느끼는 정확도
  
#### recall(재현률)
  
![ㅊㅊ](https://user-images.githubusercontent.com/59636424/132614069-f65cfa6e-c443-48d5-9910-a924ba627f97.PNG)

Ground Truth 문장 길이 기준으로 계산
  
    ex) 검색 시스템에서 키워드를 가지고 검색 시, 실제 관련 키워드 문서들이 10개가 있다면 7개가 나오면 나머지 3개의 실제로 검색 키워드 부합해서 나와야하는 문서들은 실제로 우리에게는 노출되지 않는다.
  
    이것을 해결해주는 방법이다!

#### F-measure
  
* 산술 평균, 기하 평균, 조화 평균
  
![ㄱㅎ](https://user-images.githubusercontent.com/59636424/132614842-5867c9f8-738f-45fd-a409-e2ed832c1b21.PNG)

조화, 기하 평균은 작은 값에 더 가까운 형태를 보여준다.(작은 곳에 더 많은 가중치를 준다.)
  
**F-measure는 조화 평균을 이용한다!**
  
#### Precision and Recall
  
![ㅊ](https://user-images.githubusercontent.com/59636424/132615278-ffaef084-59cc-492e-9c81-c223bd097774.PNG)

이 때, 모델 2에서 예측값은 전혀 문법적으로 말이 되지 않는다. (지표 상의 성능은 좋아보이지만)

### BLEU score
  
앞의 단점을 완화하기 위한 BLEU score

**개별 단어 레벨에서 얼마나 공통적으로 Ground Truth문장과 겹치는 단어가 나왔냐는 것 뿐만 아니라 n-gram으로 연속된 n개 단어들이 Ground Truth와 얼마나 겹치는지 계산해서 평가 지표에 적용!**

![ㄷㄷㄷㄷㄷ](https://user-images.githubusercontent.com/59636424/132616145-3c543752-e980-4350-8cd9-fdb7d8f6d138.PNG)

영어를 한국어로 번역 시, '정말'이라는 단어를 예측 못 했지만 영향이 적은 단어이므로 잘 됐다고 할 수 있다.

-> 번역에서는 precision 만 고려!!
  
  이유 1.  번역 결과만을 보고 직접 느낄 수 있는 precision만 고려
  
  이유 2.  '영화'를 '노래'로 번역하면 이는 오역이다.
  
* **n-gram으로 precision을 고려하기!**
  
![ㅊㅊ](https://user-images.githubusercontent.com/59636424/132617064-34510151-bdf7-451e-bd12-0a0c152e5db6.PNG)
  
**n-gram을 1~4개까지 고려하면 각 경우의 preicsion을 계산하여 곱한 후에 1/4승을 한다! (기하평균)**
  
-> 조화평균인 경우, 작은 값에 가중치가 세므로!
  
추가적으로 **Brevity Penalty**라는 것을 사용! 

* Brevity Penalty란?
  
    짧은 문장을 생성 시(reference 길이 보다), 1보다 작은 경우가 나온다!
  
    reference 길이보다 큰 값이 나온다면 1보다 커지므로 min을 커지면 1이 나온다!
  
    길이만을 고려했을 시, Groud Truth 보다 짧은 문장은 그 비율만큼 기하 평균 계산한 precision 값을 낮춰주려고 한다!
  
    이 penalty는 recall의 최대값을 의미한다! (10개 단어의 Ground truth와 생성한 문장이 10개 단어라면 실제로 recall 계산 전에 100% recall을 기대할 수 있다.)
  
    만약에, 예측 문장 단어가 더 많다면, Ground Truth 단어들을 모두 다 소환했다고 이상적인 경우를 생각할 수 있으므로 1을 출력

**Brevity Penalty는 recall를 조금 고려한 Penalty라고 할 수 있다.**
  
---
  
#### BLEU score 예시

![ㅂㅂ](https://user-images.githubusercontent.com/59636424/132618696-032b9ecd-3d4d-400d-8992-3fd1dae13dc4.PNG)

앞서, Precision과 Recall을 이용한 F1-measure를 이용했을 때, Model 2의 지표가 높았지만 BLEU score를 거치므로 결과가 안 좋음을 알 수 있다.

  



