# 9월 6일 배운 것!

## 1. Intro to Natural Language Processing(NLP)

### Natural language processing(자연어 처리 분야 논문 나오는 곳: ACL, EMNLP, NAACL)

* Low-level parsing

    토큰(단어 단위로 쪼갬)과 토큰나이징(문장을 단어 단위로 쪼개나가는 것)

    Stemming: 단어 어근 추출하는 것!

* Word and phrase level

    POS tagging(Word들이 문장 내에 품사나 성능이 무엇인지 판별)

* Sentence level 

    감정 분석(문장이나 단어들로 긍/부정 판별!)

    machine translation(영어 문장을 한글 문장을 번역하는 등의 경우)

* Multi-sentence and apragraph level

    Entailment prediction(두 문장간의 내포, 모순 관계를 나타낸다.)

    question answering(질문 의도를 이해하고 답에 해당하는 정보를 정확히 나타낸다.)

    dialog systems(chatbot과 같은 대화 자연어 처리 기법)

    summarization(뉴스 문장을 한 줄로 요약하는 기법)

### Text mining(자연어 처리 분야 논문 나오는 곳: KDD, The WebConf, WSDM, CIKM, ICWSM)

* test와 document data로부터 유용한 정보와 인사이트들을 추출!

    -> 빅데이터 분석과 관련되는 경우가 많다.(뉴스 기사들을 모두 모아 특정 키워드 빈도수를 시간순으로 트랜드 분석을 하여 특정 사람의 이미지가 바뀌는 것을 볼 수 있다.)

    (특정 상품을 출시하면 상품과 함께 등장한 내용 수집하고 키워드 빈도수를 분석함으로써 소비자 반응을 얻어내는데 사용!)

* Document clustering(topic modeling)

    -> 상품에 대해 사람들이 주로 가성비, 내구성 등의 세부 내용들을 주로 얘기하고 그것이 어떤지에 대한 정보 얻는게 유용

* computational 사회 과학에도 깊은 연관이 있다.

    -> 트위터 등 social media를 분석해 사람들이 어떤 신조어가 많이 쓰이고 이는 현대 어떤 사회 현상과 관련이 있다를 말할 수 있다.

    -> 혼밥 등의 키워드로 사람들의 생활이 어떻게 변화하는 것 같다고 말할 수 있음

### Information retrieval(자연어 처리 분야 논문 나오는 곳: SIGIR, CIKM, WSDM 등)

    -> 검색 기능을 주로 연구하는 분야

    (검색 성능이 고도화에 이르면서 어느 정도 성숙화가 된 분야이다.)

    -> 이는 추천 시스템으로 이어짐(노래 비슷한 것 자동으로 추천 등의 관심있을 법한 것을 추천!)

    -> 개인화된 광고나 상품 추천까지 가고 있다!


## 2. Bag of Words

### Bag-of-Words Representation

* Step 1. 유일한 단어를 포함한 어휘 만들기 

* Step 2. 유일한 단어를 one-hot vector들로 encoding하기

* Step 3. word들로 구성된 문장과 문서를 one-hot vector들을 더하는 것으로 표현할 수 있다. -> 이를 **Bag of Words라고 말한다!**

### NavieBayes Classifier for Document Classification

-> 이러한 Bag of words vector를 정해진 class로 분류하는 것이 NavieBayes classifier다!

![qqqqqqqqq](https://user-images.githubusercontent.com/59636424/132160949-1865ebc9-dc26-44a5-a345-5dfb10951563.PNG)

#### class와 document를 적용한 Bayes' Rule

![wewewe](https://user-images.githubusercontent.com/59636424/132161007-683bffef-a466-4471-b482-c9de73741e8c.PNG)

    문서 분류 카테고리 c개가 있다고 가정! (document는 d)

    -> 가장 높은 확률을 가지는 class c를 선택! (maximum a posteriori)

    -> P(d): 특정 문서 d가 뽑힐 확률 -> 고정된 문서라고 볼 수 있으니 상수값으로 볼 수 있다! (그러니 무시 가능)

![qq](https://user-images.githubusercontent.com/59636424/132161303-cab99749-d20a-4b3e-bd45-df1f44c71569.PNG)

    -> P(d|c): 특정 카테고리 c가 고정되었을 때, 문서 d가 나올 확률

    -> w1 ~ wn은 동시 사건으로 볼 수 있다. (이것은 각 단어를 나타내는 것) -> 이것들을 곱한다.

* 예시

![wwwwww](https://user-images.githubusercontent.com/59636424/132161407-3fb681a3-633a-4871-8474-fae9fb332862.PNG)

    각 클래스가 등장할 확률이 P(C_cv), P(C_nlp)
    
![nnllpp](https://user-images.githubusercontent.com/59636424/132161506-9640803c-f855-4480-a123-009e090d9201.PNG)

    class가 고정되었을 때, 단어가 나오는 확률!
    
![ww](https://user-images.githubusercontent.com/59636424/132161749-d0c59700-fa99-4256-865d-b7a3cdf5ec03.PNG)

    특정 문장을 각 class에 발견된 확률과 클래스별로 각각의 단어가 가지는 조건부 확률 값과 곱해 최종 확률 값을 구할 수 있다.


## 3. Word Embedding: Word2Vec, GloVe

### What is Word Embedding?

word를 vector로 표현하는 것이다! (유사한 단어일수록 가깝다.)

### Word2Vec

유사한 단어와 가까운 위치로 매핑시키기 위한 방법(의미가 유사한 단어!)

![zzzz](https://user-images.githubusercontent.com/59636424/132162743-de5cc506-f694-456e-be9a-7bab138e693c.PNG)

        cat이라면 단어를 중심으로 잡는다면 앞과 뒤쪽의 단어들이 cat과 의미적으로 관련성이 높은 것으로 생각!
        
* Idea of Word2Vec

**Word2vec에서는 한 단어가 주변 단어를 통해 의미를 알 수 있다.**

![ewewewewewe](https://user-images.githubusercontent.com/59636424/132162822-2d3677e3-2c39-418a-af00-72b4d68cbcec.PNG)

        cat 주변에 나타나는 확률분포를 보여준다!
        
### How Word2Vec, Algorithm Works

Word2vec은 주어진 문장을 word별로 분리하고 유일한 단어들을 모아 사전을 만든다. 그리고 사전 사이즈만큼의 one-hot vector를 만든다. 그 다음으로는 sliding window 기법으로 한 단어를 중심으로 앞 뒤로 나타난 word 각각과 입출력 쌍을 구성한다. (sliding window로 중심단어를 옮겨다닌다!)

![wewewewewe](https://user-images.githubusercontent.com/59636424/132163456-697bff4a-b48c-46e3-87f6-5d6c8f15aaa3.PNG)

        W1이 3차원 벡터를 받아 2차원으로 내어준다. (2 x 3) => W1은 총 vocabulary만큼의 column vector를 가지고 있다면 입력 word에 대한 2 dimension column vector를 뽑아왔다.
        
        W2는 2차원을 3차원 벡터로 내어준다. (3 x 2)
        
        input이 study라면 x가 study가 된다.
        
        softmax를 통과시킴으로써 확률분포값으로 내어준다!
 
 
### Property of Word2Vec

![kk](https://user-images.githubusercontent.com/59636424/132165189-b9a3e123-ed46-4ee9-a804-b4e936b3fef4.PNG)

        유사하게 보여지는 vector가 있는데 이것은 남성에서 여성으로의 벡터와 유사


### Property of Word2Vec - intrusion Detection

여러 단어가 주어져 있을때, 나머지 단어와 가장 상이한 단어를 찾는 것이다.

-> Wor2Vec의 embedding vector를 사용한다!

### Word2Vec의 적용

기계 번역, PoS tagging, 감정 분석 등에 사용된다!

### Glove: Another Word Embedding Model

Word2Vec과 차이는 새로운 loss 값을 사용했따!

![ww](https://user-images.githubusercontent.com/59636424/132165884-16228833-84d7-4199-989f-9dbd5d402ce5.PNG)

        각 입력과 출력쌍에 대해서 두 단어가 한 window에서 몇번 등장했는지 사전에 계산 -> P_ij
        
        u_i: 입력 word의 임베딩 vector
        
        v_j: 출력 word의 임베딩 vector
        
        이 두개의 내적값이 log P_ij와 fitting될 수 있도록하는 loss값을 사용했다.
       
       
**Word2Vec의 경우, 특정 입출력 단어 쌍이 자주 등장하는 경우에 그러한 데이터가 여러 번 학습됨으로써 내적값이 비례해 더 커진다.**
  
**Glove에서는 단어쌍이 동시 등장한 횟수를 미리 계산하고 이에 대한 log값을 취해 직접적은 두 단어간의 내적값에 사용해 중복된 계산을 줄였다.**

### Property of Glove

![qqqq](https://user-images.githubusercontent.com/59636424/132166204-affc81f9-dd7e-4a40-bf0d-95b176b2b4b3.PNG)


        PCA로 차원 축소한 모습

        성별의 차이가 일정한 방향과 크기를 가짐을 알 수 있다.

        

## 2. 추가적인 공부!

### Laplace smoothing


* NaiveBayesClassifier의 코드 중

~~~
for token, dist in tqdm(token_dists.items()):
      if token not in self.likelihoods:
        self.likelihoods[token] = {
            0:(token_dists[token][0] + self.k) / (class_counts[0] + len(self.w2i)*self.k),
            1:(token_dists[token][1] + self.k) / (class_counts[1] + len(self.w2i)*self.k),
        }
~~~

여기서, self.k x len(self.w2i) 부분을 더한 부분이 Laplace smoothing이라고 한다.

* **Laplace smoothing이란?**

        어떠한 단어가 vocabulary에 없는 경우에는 확률이 0이 될 수 있다. 그래서 이것을 방지하기 위해 사용하는 것이 Laplace smoothing이라고 한다.

* **왜 Laplace smoothing을 쓰는가?**

        train 과정에서 사용했던 단어들이 test 과정에서 단어들이 train에 없는 단어라면 확률이 0이 될 수 있으므로(이를 overcast-no) 이를 막기 위한 직관적인 방법이므로 사용된다!

참고 사이트: https://operatingsystems.tistory.com/entry/Data-Mining-Smoothing-Techniques

