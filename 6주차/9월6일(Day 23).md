# 9월 6일 배운 것!

## 1. Intro to Natural Language Processing(NLP)

### Natural language processing(자연어 처리 분야 논문 나오는 곳: ACL, EMNLP, NAACL)

* Low-level parsing

    토큰(단어 단위로 쪼갬)과 토큰나이징(문장을 단어 단위로 쪼개나가는 것)

    Stemming: 단어 어근 추출하는 것!

* Word and phrase level

    POS tagging(Word들이 문장 내에 품사나 성능이 무엇인지 판별)

* Sentence level 

    감정 분석(문장이나 단어들로 긍/부정 판별!)

    machine translation(영어 문장을 한글 문장을 번역하는 등의 경우)

* Multi-sentence and apragraph level

    Entailment prediction(두 문장간의 내포, 모순 관계를 나타낸다.)

    question answering(질문 의도를 이해하고 답에 해당하는 정보를 정확히 나타낸다.)

    dialog systems(chatbot과 같은 대화 자연어 처리 기법)

    summarization(뉴스 문장을 한 줄로 요약하는 기법)

### Text mining(자연어 처리 분야 논문 나오는 곳: KDD, The WebConf, WSDM, CIKM, ICWSM)

* test와 document data로부터 유용한 정보와 인사이트들을 추출!

    -> 빅데이터 분석과 관련되는 경우가 많다.(뉴스 기사들을 모두 모아 특정 키워드 빈도수를 시간순으로 트랜드 분석을 하여 특정 사람의 이미지가 바뀌는 것을 볼 수 있다.)

    (특정 상품을 출시하면 상품과 함께 등장한 내용 수집하고 키워드 빈도수를 분석함으로써 소비자 반응을 얻어내는데 사용!)

* Document clustering(topic modeling)

    -> 상품에 대해 사람들이 주로 가성비, 내구성 등의 세부 내용들을 주로 얘기하고 그것이 어떤지에 대한 정보 얻는게 유용

* computational 사회 과학에도 깊은 연관이 있다.

    -> 트위터 등 social media를 분석해 사람들이 어떤 신조어가 많이 쓰이고 이는 현대 어떤 사회 현상과 관련이 있다를 말할 수 있다.

    -> 혼밥 등의 키워드로 사람들의 생활이 어떻게 변화하는 것 같다고 말할 수 있음

### Information retrieval(자연어 처리 분야 논문 나오는 곳: SIGIR, CIKM, WSDM 등)

    -> 검색 기능을 주로 연구하는 분야

    (검색 성능이 고도화에 이르면서 어느 정도 성숙화가 된 분야이다.)

    -> 이는 추천 시스템으로 이어짐(노래 비슷한 것 자동으로 추천 등의 관심있을 법한 것을 추천!)

    -> 개인화된 광고나 상품 추천까지 가고 있다!


## 2. Bag of Words

### Bag-of-Words Representation

* Step 1. 유일한 단어를 포함한 어휘 만들기 

* Step 2. 유일한 단어를 one-hot vector들로 encoding하기

* Step 3. word들로 구성된 문장과 문서를 one-hot vector들을 더하는 것으로 표현할 수 있다. -> 이를 **Bag of Words라고 말한다!**

### NavieBayes Classifier for Document Classification

-> 이러한 Bag of words vector를 정해진 class로 분류하는 것이 NavieBayes classifier다!

![qqqqqqqqq](https://user-images.githubusercontent.com/59636424/132160949-1865ebc9-dc26-44a5-a345-5dfb10951563.PNG)

#### class와 document를 적용한 Bayes' Rule

![wewewe](https://user-images.githubusercontent.com/59636424/132161007-683bffef-a466-4471-b482-c9de73741e8c.PNG)

    문서 분류 카테고리 c개가 있다고 가정! (document는 d)

    -> 가장 높은 확률을 가지는 class c를 선택! (maximum a posteriori)

    -> P(d): 특정 문서 d가 뽑힐 확률 -> 고정된 문서라고 볼 수 있으니 상수값으로 볼 수 있다! (그러니 무시 가능)

![qq](https://user-images.githubusercontent.com/59636424/132161303-cab99749-d20a-4b3e-bd45-df1f44c71569.PNG)

    -> P(d|c): 특정 카테고리 c가 고정되었을 때, 문서 d가 나올 확률

    -> w1 ~ wn은 동시 사건으로 볼 수 있다. (이것은 각 단어를 나타내는 것) -> 이것들을 곱한다.

* 예시

![wwwwww](https://user-images.githubusercontent.com/59636424/132161407-3fb681a3-633a-4871-8474-fae9fb332862.PNG)

    각 클래스가 등장할 확률이 P(C_cv), P(C_nlp)
    
![nnllpp](https://user-images.githubusercontent.com/59636424/132161506-9640803c-f855-4480-a123-009e090d9201.PNG)

    class가 고정되었을 때, 단어가 나오는 확률!
    
![ww](https://user-images.githubusercontent.com/59636424/132161749-d0c59700-fa99-4256-865d-b7a3cdf5ec03.PNG)

    특정 문장을 각 class에 발견된 확률과 클래스별로 각각의 단어가 가지는 조건부 확률 값과 곱해 최종 확률 값을 구할 수 있다.

    
