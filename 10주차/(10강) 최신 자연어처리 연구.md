# 1. Transformer와 multi-modal 연구 정리!

## BERT 이후의 LM

### 1. XLNet

BERT는 mask token 예측 뿐이니, token 관계를 학습이 불가능!

GPT-2는 단일 방향성으로만 학습하는 단점 존재!

---

#### Relative positional encoding

![ㄱㄱㄱ](https://user-images.githubusercontent.com/59636424/136741384-12095dc8-1846-4dcb-b10f-e578d09dbca7.PNG)

**현재 토큰 위치 대비, 상대적 거리 표현법으로 encoding한다!!**

=> Sequence 길이에 대한 제한이 없어진다!!

---

#### Permutation language modeling

원래는 단어를 순차적으로 예측

=> **순열, 조합을 이용해서 문장을 학습**

모든 token들을 순열과 조합으로 섞어버림!

=> **섞음으로써, 한 방향으로만 학습하는 것을 방지!**

### 2. RoBERTa

BERT와 동일한 구조로 Model 학습 시간 증가 + Batch size 증가 + Train data 증가

**NSP 제거하여 너무 쉬운 문제를 삭제하여 성능 하락 방지**

긴 sequence 추가!

**Dynamic masking 사용!** => 다른 masking을 똑같은 텍스트 데이터에 적용!(10번 다르게!)

### 3. BART

Transformer Encoder와 Decoder 통합 LM

![열 task](https://user-images.githubusercontent.com/59636424/136744530-ee98a2f3-2d93-48de-bd5b-0995e5e7a859.PNG)

여러 task를 예측할 수 있도록 만들었음!

### 4. T-5

Transformer Encoder와 Decoder 통합 LM -> **현재 가장 SOTA 모델!!**

![ee](https://user-images.githubusercontent.com/59636424/136744770-9e5ef72f-c5a0-4545-9b48-319dee59616a.PNG)

여러 task로 학습을 할 수 있다.

=> **학습 시, masking을 하는데 여러 의미 token을 동시 masking한다!** => multi mask를 복원하는 과정을 거친다!

![ttt](https://user-images.githubusercontent.com/59636424/136744978-dd2f8044-55c1-47f7-a39b-4a9dc8dfdf08.PNG)

### 5. Meena

대화 모델을 위한 LM

![qq](https://user-images.githubusercontent.com/59636424/136745363-fcf3cba2-a552-4904-b455-9d581c81e93a.PNG)

**Encoder Block 1 layer + 나머지 Decoder Block layer를 합침!**

**챗봇의 평가를 위한 새로운 Metric인 SSA를 제시!**

---

#### SSA(Sensibleness and Specificity Average)란?

Sensibleness: 현재까지 진행 중인 대화에 적절한 답변을 했는가를 평가

Specificity: 얼마나 구체적으로 답변했는가 평가

### 6. Controllable LM

#### Plug and Play Language Model(PPLM)

원래는 다음 단어를 확률 분포를 통해 선택한다!

**PPLM은 다음에 나올 적절한 단어를 Bag of Word에 저장한다!(예시들을 미리 저장함!)**

=> **원하는 Bag of Word의 단어들이 최대 확률이 되도록 이전 상태의 벡터 수정 방법!**

![tt](https://user-images.githubusercontent.com/59636424/136746594-6814106a-88fc-443e-8abf-63528c2a03cb.PNG)

예시로, The Chicken tastes 다음 단어가 ok로 확률값이 더 높지만 delicious가 더 높은 확률 분포가 나오길 바란다!

    1. bag of word의 모든 확률 데이터를 현재 상태에 맞춰 확인해본다.
    
    2. ok가 더 높지만 delicious 확률분포를 최대 확률로 유지를 위해 backpropo를 통해 chicken에서 만들어진 vector 정보를 수정한다.
    
    => gradient가 update가 아닌, 이전 벡터값 수정!
    
    3. the chicken tastes delicious가 나오도록 한다!
    
**내가 원하는 단어를 생성하도록 유도!(gradient update X)**

bag of word의 중첩도 가능!!(기쁨의 bow와 놀람의 bow 요소를 한꺼번에 합쳐 동일한 bow로 간주할 수 있다)

특정 카테고리에 대한 감정 컨트롤 가능!! (정치적, 종교적 키워드를 중성적인 단어 선택하여 생성!)

확률 분포 조절 가능(분노 bow를 단계적 조절 가능 -> 분노1, 분노2, .... 등등)


### LM모델이 자연어 to 자연어로 충분할까?

우리는 spoken language는 multi modal(모든 감각)으로 학습한다! -> 따라서, 충분치 않다!!

## Multi-modal Language Model

### 2-1. 할머니세포

1개의 단일 세포가 어떤 개념에 대해 반응하는 세포이다!!

**우리의 뇌는 하나의 객체 인지할 때 다양한 Multi-modal 정보로 인지한다!**

### 1. LXMERT

이미지와 자연어를 동시에 학습하는 모델!! -> 학습된 정보를 하나의 모델로 합친다!!

![trtrtrtrt](https://user-images.githubusercontent.com/59636424/136748993-92e13b14-9e17-483d-b70c-af66788f5281.PNG)

**Cross-Modality Output이라는 첫번째 token에서 가져옴! -> 이 token 생성을 위해 자연어와 이미지 임베딩 정보로 만들어진다!** -> 이것에 CLS token이 된다!

이러한 분류 task시 성능이 좋았다!

#### 예시

![ttt](https://user-images.githubusercontent.com/59636424/136749307-ddd7b8e5-8366-4d6c-8ade-98650d65b326.PNG)

이미지 관련 질문을 자연어로 주었다!

### 2. ViLBERT

![ttttt](https://user-images.githubusercontent.com/59636424/136749710-70f16b4f-3025-4e01-b6f3-0eacc01c2f1a.PNG)

BERT와 구조가 똑같지만, **입력을 처음에 이미지 token에 대한 embedding vector를 넣고 SEP넣고 자연어에 대한 vector를 넣는다.**

=> 하나로 합쳐 CLS token을 만든다! -> 거기에 classification layer를 붙이면 분류가 가능하다.

### 3. Dall-e

#### 3-1. VQ-VAE를 통해 이미지의 차원 축소 학습!

**자연어로부터 이미지 생성이 가능한 모델!!**

이미지는 256 x 256 x 3사이즈로 되어있으므로 기존 LM은 수용하기 힘들다. -> 그래서 차원 축소를 한다!!!

![zdzdzdz](https://user-images.githubusercontent.com/59636424/136750315-b595070b-3014-4af5-91bc-82701b3d5439.PNG)

**차원 축소 방법으로 Encoder Network(conv) -> latent vector(이미지 벡터로 환산) -> Decoder Network(deconv)**

=> 이것을 **VQ-VAE**라는 알고리즘이다!

**이렇게 큰 이미지를 차원 축소를 하면 그 다음은 GPT-2와 같다!!**

---

#### 3-2. Autoregressive 형태로 다음 토큰 예측 학습

![zsas](https://user-images.githubusercontent.com/59636424/136751021-fbd6b880-916e-4a00-a1a4-fde5782aa6d2.PNG)

우선, Text token이 앞에 들어간다.(256 token 할당) -> 못 채우면 padding!

=> 그 다음은, 이미지 벡터를 생성!!(이미지 token 생성!)

---

#### 3-3. 한국어 Dall-e 모델 실험

MSCOCO-2014 Dataset: 이미지에 대해서 서술형으로 이미지 설명이 되어있다.

모델 사이즈: Dall-e 대비 1/120으로 줄임








# 2. 추가 공부!

## XLNet의 Relative positional encoding



# 3. 학습회고!
