# 1. Transformer와 multi-modal 연구 정리!

## BERT 이후의 LM

### 1. XLNet

BERT는 mask token 예측 뿐이니, token 관계를 학습이 불가능!

GPT-2는 단일 방향성으로만 학습하는 단점 존재!

---

#### Relative positional encoding

![ㄱㄱㄱ](https://user-images.githubusercontent.com/59636424/136741384-12095dc8-1846-4dcb-b10f-e578d09dbca7.PNG)

**현재 토큰 위치 대비, 상대적 거리 표현법으로 encoding한다!!**

=> Sequence 길이에 대한 제한이 없어진다!!

---

#### Permutation language modeling

원래는 단어를 순차적으로 예측

=> **순열, 조합을 이용해서 문장을 학습**

모든 token들을 순열과 조합으로 섞어버림!

=> **섞음으로써, 한 방향으로만 학습하는 것을 방지!**

### 2. RoBERTa

BERT와 동일한 구조로 Model 학습 시간 증가 + Batch size 증가 + Train data 증가

**NSP 제거하여 너무 쉬운 문제를 삭제하여 성능 하락 방지**

긴 sequence 추가!

**Dynamic masking 사용!** => 다른 masking을 똑같은 텍스트 데이터에 적용!(10번 다르게!)

### 3. BART

Transformer Encoder와 Decoder 통합 LM

![열 task](https://user-images.githubusercontent.com/59636424/136744530-ee98a2f3-2d93-48de-bd5b-0995e5e7a859.PNG)

여러 task를 예측할 수 있도록 만들었음!

### 4. T-5

Transformer Encoder와 Decoder 통합 LM -> **현재 가장 SOTA 모델!!**

![ee](https://user-images.githubusercontent.com/59636424/136744770-9e5ef72f-c5a0-4545-9b48-319dee59616a.PNG)

여러 task로 학습을 할 수 있다.

=> **학습 시, masking을 하는데 여러 의미 token을 동시 masking한다!** => multi mask를 복원하는 과정을 거친다!

![ttt](https://user-images.githubusercontent.com/59636424/136744978-dd2f8044-55c1-47f7-a39b-4a9dc8dfdf08.PNG)

### 5. Meena

대화 모델을 위한 LM

![qq](https://user-images.githubusercontent.com/59636424/136745363-fcf3cba2-a552-4904-b455-9d581c81e93a.PNG)

**Encoder Block 1 layer + 나머지 Decoder Block layer를 합침!**

**챗봇의 평가를 위한 새로운 Metric인 SSA를 제시!**

---

#### SSA(Sensibleness and Specificity Average)란?

Sensibleness: 현재까지 진행 중인 대화에 적절한 답변을 했는가를 평가

Specificity: 얼마나 구체적으로 답변했는가 평가

### 6. Controllable LM

#### Plug and Play Language Model(PPLM)

원래는 다음 단어를 확률 분포를 통해 선택한다!

**PPLM은 다음에 나올 적절한 단어를 Bag of Word에 저장한다!(예시들을 미리 저장함!)**

=> **원하는 Bag of Word의 단어들이 최대 확률이 되도록 이전 상태의 벡터 수정 방법!**

![tt](https://user-images.githubusercontent.com/59636424/136746594-6814106a-88fc-443e-8abf-63528c2a03cb.PNG)

예시로, The Chicken tastes 다음 단어가 ok로 확률값이 더 높지만 delicious가 더 높은 확률 분포가 나오길 바란다!

    1. bag of word의 모든 확률 데이터를 현재 상태에 맞춰 확인해본다.
    
    2. ok가 더 높지만 delicious 확률분포를 최대 확률로 유지를 위해 backpropo를 통해 chicken에서 만들어진 vector 정보를 수정한다.
    
    => gradient가 update가 아닌, 이전 벡터값 수정!
    
    3. the chicken tastes delicious가 나오도록 한다!
    
**내가 원하는 단어를 생성하도록 유도!(gradient update X)**

bag of word의 중첩도 가능!!(기쁨의 bow와 놀람의 bow 요소를 한꺼번에 합쳐 동일한 bow로 간주할 수 있다)

특정 카테고리에 대한 감정 컨트롤 가능!! (정치적, 종교적 키워드를 중성적인 단어 선택하여 생성!)

확률 분포 조절 가능(분노 bow를 단계적 조절 가능 -> 분노1, 분노2, .... 등등)


### LM모델이 자연어 to 자연어로 충분할까?

우리는 spoken language는 multi modal(모든 감각)으로 학습한다! -> 따라서, 충분치 않다!!

## Multi-modal Language Model

### 2-1. 할머니세포

1개의 단일 세포가 어떤 개념에 대해 반응하는 세포이다!!

**우리의 뇌는 하나의 객체 인지할 때 다양한 Multi-modal 정보로 인지한다!**

### 1. LXMERT

이미지와 자연어를 동시에 학습하는 모델!! -> 학습된 정보를 하나의 모델로 합친다!!

![trtrtrtrt](https://user-images.githubusercontent.com/59636424/136748993-92e13b14-9e17-483d-b70c-af66788f5281.PNG)

**Cross-Modality Output이라는 첫번째 token에서 가져옴! -> 이 token 생성을 위해 자연어와 이미지 임베딩 정보로 만들어진다!** -> 이것에 CLS token이 된다!

이러한 분류 task시 성능이 좋았다!

#### 예시

![ttt](https://user-images.githubusercontent.com/59636424/136749307-ddd7b8e5-8366-4d6c-8ade-98650d65b326.PNG)

이미지 관련 질문을 자연어로 주었다!

### 2. ViLBERT

![ttttt](https://user-images.githubusercontent.com/59636424/136749710-70f16b4f-3025-4e01-b6f3-0eacc01c2f1a.PNG)

BERT와 구조가 똑같지만, **입력을 처음에 이미지 token에 대한 embedding vector를 넣고 SEP넣고 자연어에 대한 vector를 넣는다.**

=> 하나로 합쳐 CLS token을 만든다! -> 거기에 classification layer를 붙이면 분류가 가능하다.

### 3. Dall-e

#### 3-1. VQ-VAE를 통해 이미지의 차원 축소 학습!

**자연어로부터 이미지 생성이 가능한 모델!!**

이미지는 256 x 256 x 3사이즈로 되어있으므로 기존 LM은 수용하기 힘들다. -> 그래서 차원 축소를 한다!!!

![zdzdzdz](https://user-images.githubusercontent.com/59636424/136750315-b595070b-3014-4af5-91bc-82701b3d5439.PNG)

**차원 축소 방법으로 Encoder Network(conv) -> latent vector(이미지 벡터로 환산) -> Decoder Network(deconv)**

=> 이것을 **VQ-VAE**라는 알고리즘이다!

**이렇게 큰 이미지를 차원 축소를 하면 그 다음은 GPT-2와 같다!!**

---

#### 3-2. Autoregressive 형태로 다음 토큰 예측 학습

![zsas](https://user-images.githubusercontent.com/59636424/136751021-fbd6b880-916e-4a00-a1a4-fde5782aa6d2.PNG)

우선, Text token이 앞에 들어간다.(256 token 할당) -> 못 채우면 padding!

=> 그 다음은, 이미지 벡터를 생성!!(이미지 token 생성!)

---

#### 3-3. 한국어 Dall-e 모델 실험

MSCOCO-2014 Dataset: 이미지에 대해서 서술형으로 이미지 설명이 되어있다.

모델 사이즈: Dall-e 대비 1/120으로 줄임



### 참조

[Reformer 구조 및 실습](https://colab.research.google.com/github/patrickvonplaten/blog/blob/master/notebooks/03_reformer.ipynb#scrollTo=c2ilA44YdOHj)

[MLM 적용한 Reformer](https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb)

[Summarization with blurr](https://github.com/ohmeow/ohmeow_website/blob/master/_notebooks/2020-05-23-text-generation-with-blurr.ipynb)

[Longformer for QA](https://github.com/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb)

# 2. 추가 공부!

## XLNet의 Relative positional encoding

* 원래 transformer attention 계산식

![원래 abs](https://user-images.githubusercontent.com/59636424/136779453-5332d162-d3b7-42e8-b2e4-76c07f0c2e0a.PNG)

=> (Q^T x K) 식

![xx](https://user-images.githubusercontent.com/59636424/136779806-9153e2b0-de3f-4359-b5cd-658c6f253476.PNG)

E는 임베딩 토큰을 뜻하고, U는 position 정보를 뜻한다.

U에 인코딩된 i번째, j번째 absolute 포지션 정보을 통해 두 단어 간의 위치 차이를 반영

**absolute는 두 수 차이 계산 시, a=1, b=2 이렇게 값을 정하고 차이를 보는 반면, relative는 두 수의 값과 상관없이 차만 본다.(a=1,b=2든 a=5,b=6 상관없이 위치 차이가 1이라는 것만 알려준다.)**

---

* Relative Position Embedding을 적용한 attention 계산식

![rere](https://user-images.githubusercontent.com/59636424/136779653-25b17856-f8d7-4f32-83e4-5a40845d8b9b.PNG)

* R(relative encoding matrix)(R_(i-j))이 U(position encoding matrix)(U_i, U_j) 대신 대체한다!

* (U_i)^T x (W_q)^T -> u, v vector로 변경! (query 단어 위치와 상관없이 같은 값을 가진다. => bias)

        절대 위치인 (U_i)^T x (Q_q)^T 계산은 절대 위치 정보 U가 없으므로 계산을 못 한다!
        
* W_k -> W_(k,E)와 W_(k,R)로 분리시킨다! -> W_(k,E)는 token의 임베딩을 이용한 attention(컨텐츠 기반 key vector), W_(k,R)은 상대 위치 정보를 반영한 attention(위치 기반 key vector)로 사용!!

![rtr](https://user-images.githubusercontent.com/59636424/136781608-afac06dc-5e09-411e-b80f-adf171aa8c18.PNG)

![tt](https://user-images.githubusercontent.com/59636424/136781851-cbac67f3-8a59-43d7-874f-483f7fb86da8.PNG)

## XLNet의 Relative positional encoding huggingface code


~~~
    def relative_positional_encoding(self, qlen, klen, bsz=None):
        # create relative positional encoding.
        freq_seq = torch.arange(0, self.d_model, 2.0, dtype=torch.float)
        inv_freq = 1 / torch.pow(10000, (freq_seq / self.d_model)) # sinusoid encoding matrix를 사용하기 위해서!

        if self.attn_type == "bi": 
            # beg, end = klen - 1, -qlen
            beg, end = klen, -qlen
        elif self.attn_type == "uni":
            # beg, end = klen - 1, -1
            beg, end = klen, -1
        else:
            raise ValueError(f"Unknown `attn_type` {self.attn_type}.")

        if self.bi_data:
            fwd_pos_seq = torch.arange(beg, end, -1.0, dtype=torch.float)
            bwd_pos_seq = torch.arange(-beg, -end, 1.0, dtype=torch.float)

            if self.clamp_len > 0:
                fwd_pos_seq = fwd_pos_seq.clamp(-self.clamp_len, self.clamp_len)
                bwd_pos_seq = bwd_pos_seq.clamp(-self.clamp_len, self.clamp_len)

            if bsz is not None:
                fwd_pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq, bsz // 2)
                bwd_pos_emb = self.positional_embedding(bwd_pos_seq, inv_freq, bsz // 2)
            else:
                fwd_pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq)
                bwd_pos_emb = self.positional_embedding(bwd_pos_seq, inv_freq)

            pos_emb = torch.cat([fwd_pos_emb, bwd_pos_emb], dim=1)
        else:
            fwd_pos_seq = torch.arange(beg, end, -1.0)
            if self.clamp_len > 0:
                fwd_pos_seq = fwd_pos_seq.clamp(-self.clamp_len, self.clamp_len)
            pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq, bsz)

        pos_emb = pos_emb.to(self.device)
        return pos_emb
 ~~~
 
 * qlen: sequence length
 * klen: mask length(mlen)+sequence length(qlen)

![rrrrr](https://user-images.githubusercontent.com/59636424/136782510-4d77109e-7600-4eb0-bb84-7d13db9d24a2.PNG)


 * bsz: batch size
 * clamp_len: clamp_len보다 큰 수는 clamp_len으로 고정!

**if klen=8, qlen=3, d_model=13, attn_type=bi이라면?**

* sinusoid encoding matrix를 사용하기 위한 inv_freq 만들기

~~~
freq_seq = torch.arange(0, d_model, 2.0, dtype=torch.float)
inv_freq = 1 / torch.pow(10000, (freq_seq / d_model)

tensor([ 0.,  2.,  4.,  6.,  8., 10., 12.])
tensor([1.0000e+00, 2.4245e-01, 5.8780e-02, 1.4251e-02, 3.4551e-03, 8.3768e-04,
        2.0309e-04])
~~~

* fwd_pos_seq과 bwd_pos_seq 만들기

~~~
beg = 8
end = -5
fwd_pos_seq = torch.arange(beg, end, -1.0, dtype=torch.float) # 13 length(len(beg)+len(end))
bwd_pos_seq = torch.arange(-beg, -end, 1.0, dtype=torch.float) # 13 length(len(beg)+len(end))

tensor([ 8.,  7.,  6.,  5.,  4.,  3.,  2.,  1.,  0., -1., -2., -3., -4.]) # 0~-4는 앞으로 구할 값!
tensor([-8., -7., -6., -5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.]) 
~~~

특정 index(beg 다음 index)기준으로 relative position 정하기


* clamp_len이 3이라면?

~~~
clamp_len=3
fwd_pos_seq = fwd_pos_seq.clamp(-clamp_len, clamp_len)
bwd_pos_seq = bwd_pos_seq.clamp(-clamp_len, clamp_len)

tensor([ 3.,  3.,  3.,  3.,  3.,  3.,  2.,  1.,  0., -1., -2., -3., -3.])
tensor([-3., -3., -3., -3., -3., -3., -2., -1.,  0.,  1.,  2.,  3.,  3.])
~~~

특정 index와 거리 차이가 clamp_len 이상 차이 날 시, clamp_len or -clamp_len으로 만든다!

---

이 과정을 거친 fwd_pos_emb와 bwd_pos_emb는 각각 positional_embedding(sinusiod를 거쳐 sin과 cos값 concat)을 거쳐 fwd_pos_emb, bwd_pos_emb 순으로 concat한다!


### 참조

[Transformer-XL: Attentive Language Models
Beyond a Fixed-Length Context 논문](https://arxiv.org/pdf/1901.02860.pdf)



# 3. 학습회고!
